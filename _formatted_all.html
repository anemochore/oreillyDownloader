<html><head>
  <meta charset="utf-8"><meta name="generator" content="safari-dl">
  <meta name="publisher" content="Packt Publishing">
  <link type="text/css" rel="stylesheet" href="css/Packt Publishing.css">
  <link type="text/css" rel="stylesheet" href="css/@fy_toc.css">
  <title>Building Data-Driven Applications with LlamaIndex</title></head>
  <body>
<div id="f_0__idContainer001" data-type="front-mat" class="front-mat" file="B21861_FM_xhtml" title2="Building Data-Driven Applications with LlamaIndex" no2="">
			<p class="hidden">Building Data-Driven Applications with LlamaIndex</p>
			<p class="hidden">A practical guide to retrieval-augmented generation (RAG) to enhance LLM applications</p>
			<p class="hidden">Andrei Gheorghiu</p>
			<p><img src="image/Packt_Logo_New.png" alt="" role="presentation" width="480" height="122"></p>
			<h1 class="H1---Chapter" id="f_0__idParaDest-1"><a id="_idTextAnchor000"></a>Building Data-Driven Applications with LlamaIndex</h1>
<p style="font-style: italic;">As this ebook edition doesn't have fixed pagination, the page numbers below are hyperlinked for reference only, based on the printed edition of this book.</p>
			<p>Copyright © 2024 <span class="No-Break">Packt Publishing</span></p>
			<p><em class="italic">All rights reserved</em>. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles <span class="No-Break">or reviews.</span></p>
			<p>Every effort has been made in the preparation of this book to ensure the accuracy of the information presented. However, the information contained in this book is sold without warranty, either express or implied. Neither the author, nor Packt Publishing or its dealers and distributors, will be held liable for any damages caused or alleged to have been caused directly or indirectly by <span class="No-Break">this book.</span></p>
			<p>Packt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee the accuracy of <span class="No-Break">this information.</span></p>
			<p><strong class="bold">Group Product Manager</strong>: <span class="No-Break">Niranjan Naikwadi</span></p>
			<p><strong class="bold">Publishing Product Manager</strong>: <span class="No-Break">Nitin Nainani</span></p>
			<p><strong class="bold">Book Project Manager</strong>: Aparna <span class="No-Break">Ravikumar Nair</span></p>
			<p><strong class="bold">Content Development Editor</strong>: <span class="No-Break">Priyanka Soam</span></p>
			<p><strong class="bold">Technical Editor</strong>: <span class="No-Break">Rahul Limbachiya</span></p>
			<p><strong class="bold">Copy Editor</strong>: <span class="No-Break">Safis Editing</span></p>
			<p><strong class="bold">Indexer</strong>: <span class="No-Break">Pratik Shirodkar</span></p>
			<p><strong class="bold">Production Designer</strong>: <span class="No-Break">Shankar Kalbhor</span></p>
			<p><strong class="bold">DevRel Marketing Coordinator</strong>: <span class="No-Break">Vinishka Kalra</span></p>
			<p>First published: <span class="No-Break">May 2024</span></p>
			<p>Production <span class="No-Break">reference: 1150424</span></p>
			<p><span class="No-Break">Published by</span></p>
			<p>Packt <span class="No-Break">Publishing Ltd.</span></p>
			<p><span class="No-Break">Grosvenor House</span></p>
			<p>11 St <span class="No-Break">Paul’s Square</span></p>
			<p><span class="No-Break">Birmingham</span></p>
			<p>B3 <span class="No-Break">1RB, UK</span></p>
			<p><span class="No-Break">ISBN 978-1-83508-950-7</span></p>
			<p><a href="http://www.packtpub.com" target="_blank" rel="noopener noreferrer"><span class="No-Break">www.packtpub.com</span></a></p>
			<p class="author-quote">For the past six months, the focus required to create this book has sadly kept me away from the people I love. To my family and friends, your understanding and support have been my harbor in the storm of long hours and endless revisions.</p>
			<p class="author-quote">Andreea, your love has been the gentle beacon guiding me through this journey. To my daughter Carla and every young reader out there: never stop learning! Life is a journey with so many possible destinations. Make sure you are the one choosing yours. My dear friends at ITAcademy, you guys rock! Thanks for supporting me along the way. Also, finalizing this book would not have been possible without the dedicated efforts and unwavering commitment of the Packt team. I extend my heartfelt gratitude to everyone involved in this project.</p>
			<p class="author-quote"> – Andrei Gheorghiu</p>
			<h2 id="f_0__idParaDest-2" data-type="sect1" class="sect1" title2="Contributors" no2=""><a id="_idTextAnchor001"></a>Contributors</h2>
			<h1 id="f_0__idParaDest-3"><a id="_idTextAnchor002"></a>About the author</h1>
			<p><strong class="bold">Andrei Gheorghiu</strong> is a seasoned IT professional and accomplished trainer at ITAcademy with over two decades of experience as a trainer, consultant, and auditor. With an impressive array of certifications, including ITIL Master, CISA, ISO 27001 Lead Auditor, and CISSP, Andrei has trained thousands of students in IT service management, information security, IT governance, and audit. His consulting experience spans the implementation of ERP and CRM systems, as well as conducting security assessments and audits for various organizations. Andrei’s passion for groundbreaking innovations drives him to share his vast knowledge and offer practical advice on leveraging technology to solve real-world challenges, particularly in the wake of recent advancements in the field. As a forward-thinking educator, his main goal is to help people upskill and reskill in order to increase their productivity and remain relevant in the age <span class="No-Break">of AI.</span></p>
			<h2 id="f_0__idParaDest-4" data-type="sect1" class="sect1" title2="About the reviewers" no2=""><a id="_idTextAnchor003"></a>About the reviewers</h2>
			<p><strong class="bold">Rajesh Chettiar</strong>, holding a specialization in AI and ML, brings over 13 years of experience in machine learning, Generative AI, automation, and ERP solutions. He is passionate about keeping up with cutting-edge advancements in AI and is committed to improving his skills to <span class="No-Break">foster innovation.</span></p>
			<p>Rajesh resides in Pune with his parents, his wife, Pushpa, and his son, Nishith. In his free time, he likes to play with his son, watch movies with his family, and go on road trips. He also has a fondness for listening to <span class="No-Break">Bollywood music.</span></p>
			<p><strong class="bold">Elliot</strong> helped write some of the LlamaIndexTS (Typescript version of LlamaIndex) codebase. He is actively looking to take on new generative AI projects (as of early 2024), he is available on GitHub <span class="No-Break">and Linkedin.</span></p>
			<p class="author-quote">I thank the Lord for everything. Thank you, Dad, Mom, and twin sister for your amazing support. Thank you to my friends who gave me their honest opinions and helped me grow. Thank you Yi Ding at LlamaIndex for helping me start this GenAI journey, and Yujian Tang for introducing me to Yi and always being supportive of open-source. Finally, thank you to everyone who has reached out to talk about generative AI; I learn new things every day from each of you</p>
			<p><strong class="bold">Srikannan Balakrishnan</strong> is an experienced AI/ML professional and a technical writer with a passion for translating complex information into simpler forms. He has a background in data science, including AI/ML, which fuels his ability to understand the intricacies of the subject matter and present it in a way that is accessible to both technical and non-technical audiences. He also has experience in Generative AI and has worked with different clients to solve their business problems with the power of data and AI. Beyond his technical expertise, he is a skilled communicator with a keen eye for detail. He is dedicated to crafting user-friendly documentation that empowers readers to grasp new concepts and navigate complex systems <span class="No-Break">with confidence.</span></p>
			<p><strong class="bold">Arijit Das</strong> is an experienced Data Scientist with over 5 years of commercial experience, providing data-driven solutions to Fortune 500 clients across the US, UK, and EU. With expertise in Finance, Banking, Logistics, and HR management, Arijit excels in the Data Science lifecycle, from data extraction to model deployment and MLOps. Proficient in Supervised and Unsupervised ML techniques, including NLP, Arijit is currently focused on implementing cutting-edge ML practices at <span class="No-Break">Citi globally.</span></p>
		</div>
<div id="f_1__idContainer005" data-type="front-mat" class="front-mat" file="B21861_Preface_xhtml" title2="Preface" no2="">
			<h1 id="f_1__idParaDest-5"><a id="_idTextAnchor004"></a>Preface</h1>
			<p>Beyond the initial hype that the fast advance of Generative AI and <strong class="bold">Large Language Models</strong> (<strong class="bold">LLMs</strong>) has produced, we have been able to observe both the abilities and shortcomings of this technology. LLMs are versatile and powerful tools driving innovation across various fields, serving as the foundation for natural language generation technology. Despite their potential, though, LLMs have limitations such as lacking access to real-time data, struggling to distinguish truth from falsehoods, maintaining context over long documents, and exhibiting unpredictable failures in reasoning and fact retention. <strong class="bold">Retrieval-Augmented Generation</strong> (<strong class="bold">RAG</strong>) attempts to solve many of these shortcomings and LlamaIndex is perhaps the simplest and most user-friendly way to begin your journey into this new <span class="No-Break">development paradigm.</span></p>
			<p>Driven by a flourishing and expanding community, this open source framework provides a huge number of tools for different RAG scenarios. Perhaps, that’s also why this book is needed. When I first encountered the LlamaIndex framework, I was impressed by its comprehensive official documentation. However, I soon realized that the sheer amount of options can be overwhelming for someone who’s just starting out. Therefore, my goal was to provide a beginner-friendly guide that helps you navigate the framework’s capabilities and use them in your projects. The more you explore the inner mechanics of LlamaIndex, the more you’ll appreciate its effectiveness. By breaking down complex concepts and offering practical examples, this book aims to bridge the gap between the official documentation and your understanding, ensuring that you can confidently build RAG applications while avoiding <span class="No-Break">common pitfalls.</span></p>
			<p>So, join me on a journey through the LlamaIndex ecosystem. From understanding fundamental RAG concepts to mastering advanced techniques, you’ll learn how to ingest, index, and query data from various sources, create optimized indexes tailored to your use cases, and build chatbots and interactive web applications that showcase the true potential of Generative AI. The book contains a lot of practical code examples, several best practices in prompt engineering, and troubleshooting techniques that will help you navigate the challenges of building LLM-based applications augmented with <span class="No-Break">your data.</span></p>
			<p>By the end of this book, you’ll have the skills and expertise to create powerful, interactive, AI-driven applications using LlamaIndex and Python. Moreover, you’ll be able to predict costs, deal with potential privacy issues, and deploy your applications, helping you become a sought-after professional in the rapidly growing field of <span class="No-Break">Generative AI.</span></p>
			<h2 id="f_1__idParaDest-6" data-type="sect1" class="sect1" title2="Who this book is for" no2=""><a id="_idTextAnchor005"></a>Who this book is for</h2>
			<p>This book has been specifically designed for developers at varying stages of their careers who are eager to understand and exploit the capabilities of Generative AI, particularly through the use of RAG. It aims to serve as a foundational guide for those with a basic understanding of Python development and a general familiarity with Generative <span class="No-Break">AI concepts.</span></p>
			<p>Here are the key audiences who will find this <span class="No-Break">book invaluable:</span></p>
			<ul>
				<li><strong class="bold">Entry-level developers</strong><em class="italic">: </em>Individuals who have a foundational understanding of Python and are beginning their journey into the world of generative AI will find this book an excellent starting point. It will guide you through the initial steps of using the LlamaIndex framework to create robust and innovative applications. You’ll learn the core components, basic workflows, and best practices to kickstart your RAG application development journey.</li>
				<li><strong class="bold">Experienced developers</strong>: For those who are already familiar with the landscape of generative AI and are looking to deepen their expertise, this book offers insight into advanced topics within the LlamaIndex framework. You’ll discover how to leverage your existing skills to develop and deploy more complex RAG applications, enhancing the capabilities of your projects and pushing the boundaries of what’s possible with AI.</li>
				<li><strong class="bold">Professionals seeking to harness the full power of LLMs</strong><em class="italic">:</em> If you’re looking to improve your productivity by building quick solutions for data-driven problems, this book will teach you the basic concepts and provide you with powerful abilities. If you’re a natural learner and want to experiment with this wonderful technology, this book will provide you with the tools to solve complex problems with greater efficiency and creativity.</li>
			</ul>
			<h2 id="f_1__idParaDest-7" data-type="sect1" class="sect1" title2="What this book covers" no2=""><a id="_idTextAnchor006"></a>What this book covers</h2>
			<p><a href="#_idTextAnchor014"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, <em class="italic">Understanding Large Language Models,</em> serves as an introduction to generative AI and LLMs. It explains what LLMs are, their role in modern technology, and their strengths and weaknesses. The chapter aims to provide you with a foundational understanding of the capabilities of LLMs that LlamaIndex <span class="No-Break">builds upon.</span></p>
			<p><a href="#_idTextAnchor023"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, <em class="italic">LlamaIndex: The Hidden Jewel - An Introduction to the LlamaIndex Ecosystem, </em>introduces the LlamaIndex ecosystem and how it can augment LLMs. It explains the general structure of the book – starting with basic concepts and gradually introducing more complex elements of the LlamaIndex framework. The chapter also introduces the<em class="italic"> </em><strong class="bold">PITS</strong><em class="italic"> – </em><strong class="bold">Personalized Intelligent Tutoring System</strong><em class="italic"> </em>project, which will be used to apply the concepts studied in the book and covers the preparation of the <span class="No-Break">development environment.</span></p>
			<p><a href="#_idTextAnchor045"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Kickstarting Your Journey with LlamaIndex</em>, covers the basics of starting your first LlamaIndex project. It explains the essential components of a RAG application in LlamaIndex, such as documents, nodes, indexes, and query engines. The chapter provides a typical workflow model and a simple hands-on example, where readers will begin building the <span class="No-Break">PITS project.</span></p>
			<p><a href="#_idTextAnchor068"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Ingesting Data into Our RAG Workflow,</em> focuses on importing our proprietary data into LlamaIndex, emphasizing the usage of the LlamaHub connectors. We learn how to break down and organize documents by parsing them into coherent, indexable chunks of information. The chapter also covers ingestion pipelines, important data privacy considerations, metadata extraction, and simple cost <span class="No-Break">estimation methods.</span></p>
			<p><a href="#_idTextAnchor104"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Indexing with LlamaIndex, </em>explores the topic of data indexing. It provides an overview of how indexing works, comparing different indexing techniques to help readers choose the most suitable one for their use cases. The chapter also explains the concept of layered indexing and covers persistent index storage and retrieval, cost estimation, embeddings, vector stores, similarity search, and <span class="No-Break">storage contexts.</span></p>
			<p><a href="#_idTextAnchor131"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Querying Our Data, Part 1 – Context Retrieval, </em>explains the mechanics of querying data and various querying strategies and architectures within LlamaIndex, with a deep focus on retrievers. It covers advanced concepts such as asynchronous retrieval, metadata filters, tools, selectors, retriever routers, and query transformations. The chapter also discusses fundamental paradigms such as dense retrieval and sparse retrieval, along with their strengths <span class="No-Break">and weaknesses.</span></p>
			<p><a href="#_idTextAnchor155"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Querying Our Data, Part 2 – Postprocessing and Response Synthesis, </em>continues the query mechanics topic, explaining the role of node post-processing and response synthesizers in the RAG workflow. It presents the overall query engine construct and its usage, as well as output parsing. The hands-on part of this chapter focuses on using LlamaIndex to generate personalized content in the <span class="No-Break">PITS application.</span></p>
			<p><a href="#_idTextAnchor179"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Building Chatbots and Agents with LlamaIndex, </em>introduces the essentials of chatbots, agents, and conversation tracking with LlamaIndex, applying this knowledge to the hands-on project. You will learn how LlamaIndex facilitates fluid interaction, retains context, and manages custom retrieval/response strategies, which are essential aspects for building effective <span class="No-Break">conversational interfaces.</span></p>
			<p><a href="#_idTextAnchor197"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Customizing and Deploying Our LlamaIndex Project,</em> provides a comprehensive guide to personalizing and launching LlamaIndex projects. It covers tailoring different components of the RAG pipeline, a beginner-friendly tutorial on deploying with Streamlit, advanced tracing methods for debugging, and techniques for evaluating and fine-tuning a <span class="No-Break">LlamaIndex application.</span></p>
			<p><a href="#_idTextAnchor214"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Prompt Engineering Guidelines and Best Practices,</em> explains the essential role of prompt engineering in enhancing the effectiveness of a RAG pipeline, highlighting how prompts are used “under the hood” of the LlamaIndex framework. It guides readers on the nuances of customizing and optimizing prompts to harness the full power of LlamaIndex and ensure more reliable and tailored <span class="No-Break">AI outputs.</span></p>
			<p><a href="#_idTextAnchor232"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, <em class="italic">Conclusion and Additional Resources,</em> serves as a comprehensive conclusion, highlighting other projects and pathways for extended learning and summarizing the core insights from the book. It offers an overview of the main features of the framework, provides a curated list of additional resources for further exploration, and includes an index for quick <span class="No-Break">terminology reference.</span></p>
			<h2 id="f_1__idParaDest-8" data-type="sect1" class="sect1" title2="To get the most out of this book" no2=""><a id="_idTextAnchor007"></a>To get the most out of this book</h2>
			<p>You will need to have a basic understanding of Python development. General experience in using Generative AI models is also recommended. All the examples provided in the book have been specifically designed to run in a local Python environment, and because several libraries will be required along the way, it is recommended that you have a minimum of 20 GB of storage space available on <span class="No-Break">your computer.</span></p>
			<table id="table001" class="No-Table-Style" data-type="table" title2="(no caption)" no2=""><colgroup><col><col></colgroup><tbody><tr class="No-Table-Style"><th class="No-Table-Style">
							<p><strong class="bold">Software/hardware covered in </strong><span class="No-Break"><strong class="bold">the book</strong></span></p>
						</th><th class="No-Table-Style">
							<p><strong class="bold">Operating </strong><span class="No-Break"><strong class="bold">system requirements</strong></span></p>
						</th></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p>Python &gt;= <span class="No-Break">3.11</span></p>
						</td><td class="No-Table-Style">
							<p>Windows <span class="No-Break">or Linux</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p>LlamaIndex &gt;= <span class="No-Break">0.10</span></p>
						</td><td class="No-Table-Style"></td></tr></tbody></table>
			<p>Because most of the examples presented in the book rely on the OpenAI API, you’ll also need to obtain an OpenAI <span class="No-Break">API key.</span></p>
			<p><strong class="bold">If you are using the digital version of this book, we advise you to type the code yourself or access the code from the book’s GitHub repository (a link is available in the next section). Doing so will help you avoid any potential errors related to the copying and pasting </strong><span class="No-Break"><strong class="bold">of code.</strong></span></p>
			<p>As many of the code examples rely on the OpenAI API, keep in mind that running them will incur costs. Everything has been optimized for minimum cost but neither the author nor the publisher are responsible for these costs. You should also be advised of the security implications when using a public API such as the one provided by OpenAI. If you choose to use your own proprietary data to experiment with different examples, make sure you consult OpenAI’s privacy policy <span class="No-Break">in advance.</span></p>
			<h2 id="f_1__idParaDest-9" data-type="sect1" class="sect1" title2="Download the example code files" no2=""><a id="_idTextAnchor008"></a>Download the example code files</h2>
			<p>You can download the example code files for this book from GitHub at <a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex" target="_blank" rel="noopener noreferrer">https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex</a>. The repository is organized in different folders. There is one corresponding folder for each chapter titled <em class="italic">ch&lt;x&gt;</em>, where <em class="italic">&lt;x&gt;</em> represents the chapter number. The folder called <em class="italic">PITS_APP</em> contains the source code of the main project presented throughout the book. If there’s an update to the code, it will be updated in the <span class="No-Break">GitHub repository.</span></p>
			<p>We also have other code bundles from our rich catalog of books and videos available at <a href="https://github.com/PacktPublishing/" target="_blank" rel="noopener noreferrer">https://github.com/PacktPublishing/</a>. Check <span class="No-Break">them out!</span></p>
			<h2 id="f_1__idParaDest-10" data-type="sect1" class="sect1" title2="Conventions used" no2=""><a id="_idTextAnchor009"></a>Conventions used</h2>
			<p>There are a number of text conventions used throughout <span class="No-Break">this book.</span></p>
			<p><strong class="source-inline">Code in text</strong>: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. Here is an example: “[…] using the <strong class="source-inline">download_llama_pack()</strong> method and specifying a download location such <span class="No-Break">as […]”</span></p>
			<p>A block of code is set <span class="No-Break">as follows:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_1" title2="(no caption)" no2="">from llama_index.llms.openai import OpenAI
llm = OpenAI(
&nbsp;&nbsp;&nbsp;&nbsp;api_base='http://localhost:1234/v1',
&nbsp;&nbsp;&nbsp;&nbsp;temperature=0.7
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set <span class="No-Break">in bold:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_2" title2="(no caption)" no2="">from llama_index.llms.openai import OpenAI
llm = OpenAI(
&nbsp;&nbsp;&nbsp;&nbsp;api_base=<strong class="bold">'http://localhost:1234/v1'</strong>,
&nbsp;&nbsp;&nbsp;&nbsp;temperature=0.7
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Any command-line input or output is written <span class="No-Break">as follows:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_3" title2="(no caption)" no2="">$ pip install llama-index-llms-neutrino</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p><strong class="bold">Bold</strong>: Indicates a new term, an important word, or words that you see onscreen. For instance, words in menus or dialog boxes appear in <strong class="bold">bold</strong>. Here is an example: “Select <strong class="bold">System info</strong> from the <span class="No-Break"><strong class="bold">Administration</strong></span><span class="No-Break"> panel.”</span></p>
			<p class="callout-heading">Tips or important notes</p>
			<p class="callout">Appear like this.</p>
			<h2 id="f_1__idParaDest-11" data-type="sect1" class="sect1" title2="Get in touch" no2=""><a id="_idTextAnchor010"></a>Get in touch</h2>
			<p>Feedback from our readers is <span class="No-Break">always welcome.</span></p>
			<p><strong class="bold">General feedback</strong>: If you have questions about any aspect of this book, email us at <a href="mailto:customercare@packtpub.com">customercare@packtpub.com</a> and mention the book title in the subject of <span class="No-Break">your message.</span></p>
			<p><strong class="bold">Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit <a href="http://www.packtpub.com/support/errata" target="_blank" rel="noopener noreferrer">www.packtpub.com/support/errata</a> and fill in <span class="No-Break">the form.</span></p>
			<p><strong class="bold">Piracy</strong>: If you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <a href="mailto:copyright@packt.com">copyright@packt.com</a> with a link to <span class="No-Break">the material.</span></p>
			<p><strong class="bold">If you are interested in becoming an author</strong>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please <span class="No-Break">visit </span><a href="http://authors.packtpub.com" target="_blank" rel="noopener noreferrer"><span class="No-Break">authors.packtpub.com</span></a><span class="No-Break">.</span></p>
			<h2 id="f_1__idParaDest-12" data-type="sect1" class="sect1" title2="Share Your Thoughts" no2=""><a id="_idTextAnchor011"></a>Share Your Thoughts</h2>
			<p>Once you’ve read <em class="italic">Building Data-Driven Applications with LlamaIndex</em>, we’d love to hear your thoughts! Please <a href="https://packt.link/r/1-835-08950-X" target="_blank" rel="noopener noreferrer">click here to go straight to the Amazon review page</a> for this book and share <span class="No-Break">your feedback.</span></p>
			<p>Your review is important to us and the tech community and will help us make sure we’re delivering excellent <span class="No-Break">quality content.</span></p>
			<h2 id="f_1__idParaDest-13" data-type="sect1" class="sect1" title2="Download a free PDF copy of this book" no2=""><a id="_idTextAnchor012"></a>Download a free PDF copy of this book</h2>
			<p>Thanks for purchasing <span class="No-Break">this book!</span></p>
			<p>Do you like to read on the go but are unable to carry your print <span class="No-Break">books everywhere?</span></p>
			<p>Is your eBook purchase not compatible with the device of <span class="No-Break">your choice?</span></p>
			<p>Don’t worry, now with every Packt book you get a DRM-free PDF version of that book at <span class="No-Break">no cost.</span></p>
			<p>Read anywhere, any place, on any device. Search, copy, and paste code from your favorite technical books directly into <span class="No-Break">your application.</span></p>
			<p>The perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free content in your <span class="No-Break">inbox daily</span></p>
			<p>Follow these simple steps to get <span class="No-Break">the benefits:</span></p>
			<ol>
				<li>Scan the QR code or visit the <span class="No-Break">link below</span></li>
			</ol>
			<div>
				<div id="_idContainer004" class="IMG---Figure">
					<img src="image/B21861_QR_Free_PDF.jpg" alt="Download a free PDF copy of this book
" width="200" height="200" data-type="figure" id="untitled_figure_1" title2="https://packt.link/free-ebook/9781835089507" no2="">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a href="https://packt.link/free-ebook/9781835089507" target="_blank" rel="noopener noreferrer">https://packt.link/free-ebook/9781835089507</a></p>
			<ol>
				<li value="2">Submit your proof <span class="No-Break">of purchase</span></li>
				<li>That’s it! We’ll send your free PDF and other benefits to your <span class="No-Break">email directly</span></li>
			</ol>
		</div>
<div id="f_2__idContainer006" class="part" data-type="part" file="B21861_Part_1_xhtml" title2="Introduction to  Generative AI and LlamaIndex" no2="1">
			<h1 class="H1---Chapter" id="f_2__idParaDest-14" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor013"></a>Part 1:Introduction to  Generative AI and LlamaIndex</h1>
<p style="font-style: italic;">As this ebook edition doesn't have fixed pagination, the page numbers below are hyperlinked for reference only, based on the printed edition of this book.</p>
		</div>
<div id="f_3__idContainer016" data-type="chapter" class="chapter" file="B21861_01_xhtml" title2="Understanding Large Language Models" no2="1">
			<h1 id="f_3__idParaDest-15" class="chapter-number"><a id="_idTextAnchor014"></a>1</h1>
			<h1 id="f_3__idParaDest-16"><a id="_idTextAnchor015"></a>Understanding Large Language Models</h1>
			<p>If you are reading <a id="_idIndexMarker000"></a>this book, you have probably explored the realm of <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>) and already recognize their potential applications as well as their pitfalls. This book aims to address the challenges LLMs face and provides a practical guide to building data-driven LLM applications with LlamaIndex, taking <a id="_idIndexMarker001"></a>developers from foundational concepts to advanced techniques for implementing <strong class="bold">retrieval-augmented generation</strong> (<strong class="bold">RAG</strong>) to create high-performance interactive <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) systems <a id="_idIndexMarker002"></a>augmented by <span class="No-Break">external data.</span></p>
			<p>This chapter introduces <strong class="bold">generative AI</strong> (<strong class="bold">GenAI</strong>) and LLMs. It explains how LLMs generate <a id="_idIndexMarker003"></a>human-like text after training on massive datasets. We’ll also overview LLM capabilities, limitations such as outdated knowledge potential for false information, and lack of reasoning. You’ll be introduced to RAG as a potential solution, combining retrieval models using indexed data with generative models to increase fact accuracy, logical reasoning, and context relevance. Overall, you’ll gain a basic LLM understanding and learn about RAG as a way to overcome some LLM weaknesses, setting the stage for utilizing <span class="No-Break">LLMs practically.</span></p>
			<p>In this chapter, we will cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Introducing GenAI <span class="No-Break">and LLMs</span></li>
				<li>Understanding the role of LLMs in <span class="No-Break">modern technology</span></li>
				<li>Exploring challenges <span class="No-Break">with LLMs</span></li>
				<li>Augmenting LLMs <span class="No-Break">with RAG</span></li>
			</ul>
			<h2 id="f_3__idParaDest-17" data-type="sect1" class="sect1" title2="Introducing GenAI and LLMs" no2="1.1"><a id="_idTextAnchor016"></a>1.1. Introducing GenAI and LLMs</h2>
			<p>Introductions <a id="_idIndexMarker004"></a>are sometimes boring, but here, it is important <a id="_idIndexMarker005"></a>for us to set the context and help you familiarize yourself with GenAI and LLMs before we dive deep into LlamaIndex. I will try to be as concise as possible and, if the reader is already familiar with this information, I apologize for the <span class="No-Break">brief digression.</span></p>
			<h3 id="f_3__idParaDest-18" data-type="sect2" class="sect2" title2="What is GenAI?" no2="1.1.1"><a id="_idTextAnchor017"></a>1.1.1. What is GenAI?</h3>
			<p><strong class="bold">GenAI</strong> refers to <a id="_idIndexMarker006"></a>systems that are capable of generating new content such as text, images, audio, or video. Unlike more specialized AI systems that are designed for specific tasks such as image classification or speech recognition, GenAI models can create completely new assets that are often very difficult – if not impossible – to distinguish from <span class="No-Break">human-created content.</span></p>
			<p>These <a id="_idIndexMarker007"></a>systems use <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) techniques such as <strong class="bold">neural networks</strong> (<strong class="bold">NNs</strong>) that <a id="_idIndexMarker008"></a>are trained on vast amounts of data. By learning patterns and structures within the training data, generative models can model the underlying probability distribution of the data and sample from this distribution to generate new examples. In other words, they act as big <span class="No-Break">prediction machines.</span></p>
			<p>We will now discuss LLMs, which are one of the most popular fields <span class="No-Break">in GenAI.</span></p>
			<h3 id="f_3__idParaDest-19" data-type="sect2" class="sect2" title2="What is an LLM?" no2="1.1.2"><a id="_idTextAnchor018"></a>1.1.2. What is an LLM?</h3>
			<p>One of <a id="_idIndexMarker009"></a>the most prominent and rapidly advancing branches <a id="_idIndexMarker010"></a>of GenAI is <strong class="bold">natural language generation</strong> (<strong class="bold">NLG</strong>) through <strong class="bold">LLMs</strong> (<span class="No-Break"><em class="italic">Figure 1</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">):</span></p>
			<div>
				<div id="_idContainer011" class="IMG---Figure">
					<img src="image/B21861_1_01.jpg" alt="Figure 1.1 – LLMs are a sub-branch of GenAI" width="1100" height="596" data-type="figure" id="untitled_figure_2" title2="– LLMs are a sub-branch of GenAI" no2="1.1">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – LLMs are a sub-branch of GenAI</p>
			<p>LLMs are NNs that are specifically designed and optimized to understand and generate human language. They are <em class="italic">large</em> in the sense that they are trained on massive amounts of text containing <a id="_idIndexMarker011"></a>billions or even trillions of words scraped from the internet and other sources. Larger models show increased performance on benchmarks, better generalization, and new emergent abilities. In contrast with earlier, rule-based generation systems, the main distinguishing feature of an LLM is that it can produce novel, original text that <span class="No-Break">reads naturally.</span></p>
			<p>By learning patterns from many sources, LLMs acquire various language skills found in their training data – from nuanced grammar to topic knowledge and even basic common-sense reasoning. These learned patterns allow LLMs to extend human-written text in contextually relevant ways. As they keep improving, LLMs create new possibilities for automatically <a id="_idIndexMarker012"></a>generating <strong class="bold">natural language</strong> (<strong class="bold">NL</strong>) content <span class="No-Break">at scale.</span></p>
			<p>During the training process, LLMs gradually learn probabilistic relationships between words and rules that govern language structure from their huge dataset of training data. Once trained, they are able to generate remarkably human-like text by predicting the probability of the next word in a sequence, based on the previous words. In many cases, the text they generate is so natural that it makes you wonder: aren’t we humans just a similar but more sophisticated prediction machine? But that’s a topic for <span class="No-Break">another book.</span></p>
			<p>One of the key <a id="_idIndexMarker013"></a>architectural innovations is the <strong class="bold">transformer</strong> (that is the <em class="italic">T</em> in <em class="italic">GPT</em>), which uses an <strong class="bold">attention mechanism</strong> to learn contextual relationships between words. Attention <a id="_idIndexMarker014"></a>allows the model to learn long-range dependencies in text. It’s like if you’re listening carefully in a conversation, you pay <strong class="bold">attention</strong> to the context to understand the full meaning. This means they <em class="italic">understand</em> not just words that are close together but also how words that are far apart in a sentence or paragraph relate to <span class="No-Break">each other.</span></p>
			<p><em class="italic">Attention</em> allows the model to selectively focus on relevant parts of the input sequence when making predictions, thus capturing complex patterns and dependencies within the data. This feature <a id="_idIndexMarker015"></a>makes it possible for particularly large transformer models (with many parameters and trained on massive datasets) to demonstrate surprising new abilities such as in-context learning, where they can perform tasks <a id="_idIndexMarker016"></a>with just a few examples in their prompt. To learn more about transformers and <strong class="bold">Generative Pre-trained Transformer</strong> (<strong class="bold">GPT</strong>), you can refer to <em class="italic">Improving Language Understanding with unsupervised learning</em>– Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya <span class="No-Break">Sutskever (</span><a href="https://openai.com/research/language-unsupervised" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://openai.com/research/language-unsupervised</span></a><span class="No-Break">).</span></p>
			<p>The best-performing LLMs such as GPT-4, Claude 2.1, and Llama 2 contain trillions of parameters <a id="_idIndexMarker017"></a>and have been trained on internet-scale datasets using advanced <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) techniques. The resulting model has an extensive vocabulary and a broad knowledge of language structure such as grammar and syntax, and about the world in general. Thanks to their unique traits, LLMs are able to generate text that is coherent, grammatically correct, and semantically relevant. The outputs they produce may not always be completely logical or factually accurate, but they usually read convincingly like being written by a human. But it’s not all about size. The quality of data and training algorithms – among others – can also play a huge role in the resulting performance of a <span class="No-Break">particular model.</span></p>
			<p>Many models feature a user interface that allows for response generation through prompts. Additionally, some offer an API for developers to access the model programmatically. This method will be our primary focus in the upcoming chapters of <span class="No-Break">our book.</span></p>
			<p>Next up, we’ll talk about how LLMs are making big changes in tech. They’re helping not just big companies but everyone. Curious? Let’s <span class="No-Break">keep reading.</span></p>
			<h2 id="f_3__idParaDest-20" data-type="sect1" class="sect1" title2="Understanding the role of LLMs in modern technology" no2="1.2"><a id="_idTextAnchor019"></a>1.2. Understanding the role of LLMs in modern technology</h2>
			<p>Oh! What good times we are living in. There has never been a more favorable era for small businesses <a id="_idIndexMarker018"></a>and entrepreneurs. Given the enormous potential of this technology, it’s a real miracle that, instead of ending up strictly under the control of large corporations or governments, it is literally within everyone’s reach. Now, it’s truly possible for almost anyone – even a non-technical person – to realize their ideas and solve problems that until now seemed impossible to solve without a huge amount <span class="No-Break">of resources.</span></p>
			<p>The disruptive potential that LLMs have – in almost all industries – <span class="No-Break">is enormous.</span></p>
			<p>It’s true: there are concerns that this technology could replace us. However, technology’s role is to make lives easier, taking over repetitive activities. As before, we’ll likely do the same things, only much more efficiently and better with LLMs’ help. We will do more <span class="No-Break">with less.</span></p>
			<p>I would dare say that LLMs have become the foundation of NLG technology. They can already power chatbots, search engines, coding assistants, text summarization tools, and other applications that synthesize written text interactively or automatically. And their capabilities keep advancing rapidly with bigger datasets <span class="No-Break">and models.</span></p>
			<p>And then, there are also the <strong class="bold">agents</strong>. These automated wonders are capable of perceiving and interpreting <em class="italic">stimuli</em> from the digital environment – and not just digital – to make decisions and act accordingly. Backed by the power of an LLM, intelligent agents can solve complex problems and fundamentally change the way we interact with technology. We’ll cover this topic in more detail throughout <a href="#_idTextAnchor179"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Building Chatbots and Agents </em><span class="No-Break"><em class="italic">with LlamaIndex</em></span><span class="No-Break">.</span></p>
			<p>Despite their relatively short existence, LLMs have already proven to be remarkably versatile and powerful. With the right techniques and prompts, their output can be steered in useful directions at scale. LLMs are driving innovation in numerous fields as their generative powers continue to evolve. Their capabilities keep expanding from nuanced dialog to multimodal intelligence. And, at the moment, the LLM-powered wave of innovation across industries and technologies shows no signs of <span class="No-Break">slowing down.</span></p>
			<p><strong class="bold">The Gartner Hype Cycle model</strong> serves <a id="_idIndexMarker019"></a>as a strategic guide for technology leaders, helping them evaluate new technologies not just on their merits but also in the context of their organization’s specific needs and <span class="No-Break">goals (</span><a href="https://www.gartner.com/en/research/methodologies/gartner-hype-cycle" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://www.gartner.com/en/research/methodologies/gartner-hype-cycle</span></a><span class="No-Break">).</span></p>
			<p>Judging by current adoption levels, LLMs are currently well into the <strong class="bold">Slope of Enlightenment</strong> stage, ready to take off into the <strong class="bold">Plateau of Productivity</strong> – where mainstream adoption really starts to take off (<span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.2</em>). Companies are becoming more pragmatic <a id="_idIndexMarker020"></a>about their application, focusing on specialized use cases where they offer the <span class="No-Break">most value:</span></p>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="image/B21861_1_02.jpg" alt="Figure 1.2 – The Gartner Hype Cycle" width="1100" height="742" data-type="figure" id="untitled_figure_3" title2="– The Gartner Hype Cycle" no2="1.2">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – The Gartner Hype Cycle</p>
			<p>But, unlike other more specific technologies, LLMs are rather a new form of infrastructure – a kind of ecosystem where new concepts will be able to manifest and, undoubtedly, revolutionary applications will <span class="No-Break">be born.</span></p>
			<p>This is their true potential, and this is the ideal time to learn how to take advantage of the opportunities <span class="No-Break">they offer.</span></p>
			<p>Before we jump into innovative solutions that could maximize LLMs’ capabilities, let’s take a step back and look at some challenges <span class="No-Break">and limitations.</span></p>
			<h2 id="f_3__idParaDest-21" data-type="sect1" class="sect1" title2="Exploring challenges with LLMs" no2="1.3"><a id="_idTextAnchor020"></a>1.3. Exploring challenges with LLMs</h2>
			<p>Not all the news is good, however. It’s time to also discuss the <em class="italic">darker</em> side <span class="No-Break">of LLMs.</span></p>
			<p>These <a id="_idIndexMarker021"></a>models do have important limitations and some collateral effects too. Here is a list of the most important ones, but please consider it non-exhaustive. There may be others not included here, and the order is <span class="No-Break">arbitrarily chosen:</span></p>
			<ul>
				<li>They lack access to <span class="No-Break">real-time data.</span><ul><li>LLMs are trained on a static dataset, meaning that the information they have is only as up to date as the data they were trained on, which might not include the latest news, scientific discoveries, or <span class="No-Break">social trends.</span></li><li>This limitation can be critical when users seek real-time or recent information, as the LLMs might provide outdated or irrelevant responses. Furthermore, even if they cite data or statistics, these numbers are likely to have changed or evolved, leading to <span class="No-Break">potential misinformation.</span></li></ul></li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">While recent features introduced by OpenAI, for example, allow the underlying LLM to integrate with Bing to retrieve fresh context from the internet, that’s not an inherent feature of the LLM but rather an augmentation provided by the <span class="No-Break">ChatGPT interface.</span></p>
			<ul>
				<li>This lack of real-time updating also means that LLMs – by themselves – are not suited for tasks such as live customer service queries that may require real-time access to user data, inventory levels, or system statuses, <span class="No-Break">for example.</span></li>
			</ul>
			<ul>
				<li> They have no intrinsic way of distinguishing factual truth <span class="No-Break">from falsehoods.</span><ul><li>Without proper monitoring, they can generate convincing misinformation. And trust me – they don’t do it on purpose. In very simple terms, LLMs are basically just looking for words that <span class="No-Break">fit together.</span></li><li>Check out <em class="italic">Figure 1.3</em> for an example of how one of the previous versions of the GPT-3.5 model would produce <span class="No-Break">false information:</span></li></ul></li>
			</ul>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="image/B21861_1_03.jpg" alt="Figure 1.3 – Screenshot from a GPT 3.5-turbo-instruct playground" width="1102" height="352" data-type="figure" id="untitled_figure_4" title2="– Screenshot from a GPT 3.5-turbo-instruct playground" no2="1.3">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – Screenshot from a GPT 3.5-turbo-instruct playground</p>
			<ul>
				<li>As these models stochastically (randomly) generate text, their outputs are not guaranteed to be completely logical, factual, or harmless. Also, the training <a id="_idIndexMarker022"></a>data inherently biases the model, and LLMs may generate toxic, incorrect, or nonsensical text without warning. Since this data sometimes includes unsavory elements of online discourse, LLMs risk amplifying harmful biases and toxic content present in their <span class="No-Break">training data.</span></li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">While this kind of result may be easily achieved in a playground environment, using an older AI model, OpenAI’s ChatGPT interface uses newer models and employs additional guardrails, thus making these kinds of responses much <span class="No-Break">less probable.</span></p>
			<ul>
				<li>They also cannot maintain context and memory over <span class="No-Break">long documents.</span><ul><li>An interaction with a vanilla-flavor, standard LLM can prove to be a charm for simple topics or a quick question-and-answer session. But go beyond the context window limit of the model, and you’ll soon experience its limitations as it struggles to maintain coherence and may lose important details from earlier parts of the conversation or document. This can result in fragmented or incomplete responses that may not fully address the complexities of a long-form interaction or in-depth analysis, just like a human suffering from <em class="italic">short-term </em><span class="No-Break"><em class="italic">memory loss</em></span><span class="No-Break">.</span></li></ul></li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">Although recently released AI models such as Anthropic’s Claude 2.1 and Google’s Gemini Pro 1.5 have dramatically raised the bar in terms of context window limit, ingesting an entire book and running inference on such a large context may prove to be prohibitive from a <span class="No-Break">cost perspective.</span></p>
			<ul>
				<li>LLMs also <a id="_idIndexMarker023"></a>exhibit unpredictable failures in reasoning and fact retention. Take a look at <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.4</em> for a typical logic reasoning problem that proves to be challenging even for newer models such <span class="No-Break">as GPT-4:</span></li>
			</ul>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="image/B21861_1_04.jpg" alt="Figure 1.4 – Screenshot from a GPT-4 playground" width="1099" height="259" data-type="figure" id="untitled_figure_5" title2="– Screenshot from a GPT-4 playground" no2="1.4">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – Screenshot from a GPT-4 playground</p>
			<ul>
				<li>In this example, the answer is wrong because the only scenario that fits is if Emily is the one telling the truth. The treasure would then be neither in the attic nor in <span class="No-Break">the basement.</span></li>
				<li>Their capabilities beyond fluent text generation remain inconsistent and limited. Blindly trusting their output without skepticism <span class="No-Break">invites errors.</span></li>
			</ul>
			<ul>
				<li>The complexity of massive LLMs also reduces transparency into <span class="No-Break">their functioning.</span><ul><li>The lack of interpretability makes it hard to audit for issues or understand exactly when and why they fail. All you get is the output, but there’s no easy way of knowing the actual decision process that led to that output or the documented fact in which that particular output is grounded. As such, LLMs still require careful governance to mitigate risks from biased, false, or <span class="No-Break">dangerous outputs.</span></li></ul></li>
				<li>As with <a id="_idIndexMarker024"></a>many other things out there, it turns out we cannot really call them sustainable. At least <span class="No-Break">not yet.</span><ul><li>Their massive scale makes them expensive to train and environmentally costly due to huge computing requirements. And it’s not just the training itself but also their usage. According to some estimates, “<em class="italic">the water consumption of ChatGPT has been estimated at 500 milliliters for a session of 20-50 queries</em>” – <em class="italic">AMPLIFY, VOL. 36, NO. 8</em>: <em class="italic">Arthur D. Little’s Greg Smith, Michael Bateman, Remy Gillet, and Eystein Thanisch</em> (<a href="https://www.cutter.com/article/environmental-impact-large-language-models" target="_blank" rel="noopener noreferrer">https://www.cutter.com/article/environmental-impact-large-language-models</a>). This is not negligible by any means. Think about the countless failed attempts to get an answer from an LLM, then multiply that with the countless users exercising their prompt engineering skills <span class="No-Break">every minute.</span></li></ul></li>
				<li>And here’s some more bad news: as models advance in complexity and training techniques, LLMs are rapidly becoming a huge source of <span class="No-Break">machine-generated text.</span><ul><li>So huge, in fact, that according to predictions, it will end up almost entirely replacing human-generated text (<em class="italic">Brown, Tom B. et al. (2020)</em>. <em class="italic">Language Models are Few-Shot Learners</em>. <em class="italic">arXiv:2005.14165 [</em><span class="No-Break"><em class="italic">cs.CL]</em></span><span class="No-Break">. </span><a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://arxiv.org/abs/2005.14165</span></a><span class="No-Break">).</span></li><li>In a way, this means they may become the victims of their own success. As more and more data is generated by AI, it gradually <em class="italic">contaminates</em> the training of new models, decreasing <span class="No-Break">their capabilities.</span></li><li>As in biology, any ecosystem that cannot maintain a healthy diversity in its genetic pool will <span class="No-Break">gradually degrade.</span></li></ul></li>
			</ul>
			<p><em class="italic">I saved the good news </em><span class="No-Break"><em class="italic">for last.</em></span></p>
			<p>What if I told you there is at least one solution that can partially address almost all <span class="No-Break">these problems?</span></p>
			<p>In many ways, a language model is very similar to an operating system. It provides a foundational layer upon which applications can be built. Just as an operating system manages <a id="_idIndexMarker025"></a>hardware resources and provides services for computer programs, LLMs manage linguistic resources and provide services <a id="_idIndexMarker026"></a>for various <strong class="bold">NL processing</strong> (<strong class="bold">NLP</strong>) tasks. Using prompts to interact with them is much like writing code using an Assembly Language. It’s a low-level interaction. But, as you’ll soon find out, there are more sophisticated and practical ways of using LLMs to their <span class="No-Break">full potential.</span></p>
			<p>It’s time to talk <span class="No-Break">about RAG.</span></p>
			<h2 id="f_3__idParaDest-22" data-type="sect1" class="sect1" title2="Augmenting LLMs with RAG" no2="1.4"><a id="_idTextAnchor021"></a>1.4. Augmenting LLMs with RAG</h2>
			<p>Coined for <a id="_idIndexMarker027"></a>the first time in a 2020 <a id="_idIndexMarker028"></a>paper, <em class="italic">Lewis, Patrick et al. (2005). “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks”. arXiv:2005.11401 [cs.CL]</em> (<a href="https://arxiv.org/abs/2005.11401" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2005.11401</a>), published by several researchers from Meta, RAG is a technique that combines the powers of retrieval methods and generative models to answer user questions. The idea is to first retrieve relevant information from an indexed data source containing proprietary knowledge and then use that retrieved information to generate a more informed, context-rich response using a generative model (<span class="No-Break"><em class="italic">Figure 1</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">):</span></p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/B21861_1_05.jpg" alt="Figure 1.5 – A RAG model" width="1101" height="560" data-type="figure" id="untitled_figure_6" title2="– A RAG model" no2="1.5">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.5 – A RAG model</p>
			<p>Let’s <a id="_idIndexMarker029"></a>have a look at what this <a id="_idIndexMarker030"></a>means <span class="No-Break">in practice:</span></p>
			<ul>
				<li><strong class="bold">Much better fact retention</strong>: One of the advantages of using RAG is its ability to pull from specific data sources, which can improve fact retention. Instead of relying solely on the generative model’s own <em class="italic">knowledge</em> – which is mostly generic – it refers to external documents to construct its answers, increasing the chances that the information <span class="No-Break">is accurate.</span></li>
				<li><strong class="bold">Improved reasoning</strong>: The retrieval step allows RAG models to pull in information that is specifically related to the question. In general, this would result in more logical and coherent reasoning. This could help overcome limitations in reasoning that many <span class="No-Break">LLMs face.</span></li>
				<li><strong class="bold">Context relevance</strong>: Because it pulls information from external sources based on the query, RAG can be more contextually accurate than a standalone generative model, which has to rely only on its training data and might not have the most up-to-date or contextually relevant information. Not only that, but you could also get an actual <em class="italic">quote</em> from the model regarding the source of the actual knowledge used in <span class="No-Break">the answer.</span></li>
				<li><strong class="bold">Reduced trust issues</strong>: While not foolproof, the hybrid approach means that RAG could, in principle, be less prone to generating completely false or nonsensical answers. That means an increased probability of receiving a <span class="No-Break">valid output.</span></li>
				<li><strong class="bold">Validation</strong>: It’s often easier to validate the reliability of the retrieved documents in an RAG setup by setting up a mechanism to provide a reference to the original information used for generating a response. This could be a step toward more transparent and trustworthy <span class="No-Break">model behavior.</span></li>
			</ul>
			<p class="callout-heading">A word of caution</p>
			<p class="callout">Even <a id="_idIndexMarker031"></a>if RAG makes LLMs better <a id="_idIndexMarker032"></a>and more reliable, it doesn’t completely fix the issue of them sometimes giving wrong or confusing answers. There is no silver bullet that will completely eliminate all the issues mentioned previously. It’s still a good idea to double-check and evaluate their outputs, and we’ll talk about ways of doing that later in the book. Because, as you may already know or you’ve probably guessed by now, LlamaIndex is one of the many ways of augmenting LLM-based applications using RAG. And a very effective one, I <span class="No-Break">should add.</span></p>
			<p>While some LLM providers have started introducing RAG components into their API, such as OpenAI’s <strong class="bold">Assistants</strong> feature, using a standalone framework such as LlamaIndex provides many more customization options. It also enables the usage of local models, enabling self-hosted solutions and greatly reducing costs and privacy concerns associated with a <span class="No-Break">hosted model.</span></p>
			<h2 id="f_3__idParaDest-23" data-type="sect1" class="sect1" title2="Summary" no2="1.5"><a id="_idTextAnchor022"></a>1.5. Summary</h2>
			<p>In this chapter, we covered a quick introduction to GenAI and LLMs. You learned how LLMs such as GPT work and some of their capabilities and limitations. A key takeaway is that while powerful, LLMs have weaknesses – such as the potential for false information and lack of reasoning – that require mitigation techniques. We discussed RAG as one method to overcome some <span class="No-Break">LLM limitations.</span></p>
			<p>These lessons provide useful background on how to approach LLMs practically while being aware of their risks. At the same time, you learned the importance of techniques such as RAG to address LLMs’ <span class="No-Break">potential downsides.</span></p>
			<p>With this introductory foundation in place, we are now ready to dive into the next chapter where we will explore the LlamaIndex ecosystem. LlamaIndex offers an effective RAG framework to augment LLMs with indexed data for more accurate, logical outputs. Learning to leverage LlamaIndex tools will be the natural next step to harness the power of LLMs in a <span class="No-Break">proficient way.</span></p>
		</div>
<div id="f_4__idContainer023" data-type="chapter" class="chapter" file="B21861_02_xhtml" title2="LlamaIndex: The Hidden Jewel - An Introduction to the LlamaIndex Ecosystem" no2="2">
			<h1 id="f_4__idParaDest-24" class="chapter-number"><a id="_idTextAnchor023"></a>2</h1>
			<h1 id="f_4__idParaDest-25"><a id="_idTextAnchor024"></a>LlamaIndex: The Hidden Jewel - An Introduction to the LlamaIndex Ecosystem</h1>
			<p>Now that <a id="_idIndexMarker033"></a>you’ve got a solid understanding of what <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>) are and what they can (and cannot) do. It’s time to discover how <strong class="bold">LlamaIndex</strong> can take <a id="_idIndexMarker034"></a>your interactive AI applications to the <a id="_idIndexMarker035"></a>next level. We’ll explore how <strong class="bold">retrieval-augmented generation</strong> (<strong class="bold">RAG</strong>) using LlamaIndex can provide the missing link between the vast knowledge of LLMs and your <span class="No-Break">proprietary data.</span></p>
			<p>In this chapter, we will cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Optimizing language models – The symbiosis of fine-tuning, RAG, <span class="No-Break">and LlamaIndex</span></li>
				<li>Discovering <a id="_idIndexMarker036"></a>the advantages of progressively <span class="No-Break">disclosing complexity</span></li>
				<li>Introducing <strong class="bold">personalized intelligent tutoring system</strong> (<strong class="bold">PITS</strong>) – our hands-on <span class="No-Break">LlamaIndex project</span></li>
				<li>Preparing our <span class="No-Break">coding environment</span></li>
				<li>Familiarizing ourselves with the structure of the LlamaIndex <span class="No-Break">code repository</span></li>
			</ul>
			<h2 id="f_4__idParaDest-26" data-type="sect1" class="sect1" title2="Technical requirements" no2="2.1"><a id="_idTextAnchor025"></a>2.1. Technical requirements</h2>
			<p>The following elements will be required for <span class="No-Break">this chapter:</span></p>
			<ul>
				<li><em class="italic">Python </em><span class="No-Break"><em class="italic">3.11</em></span><span class="No-Break"> (</span><a href="https://www.python.org/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://www.python.org/</span></a><span class="No-Break">)</span></li>
				<li><span class="No-Break"><em class="italic">Git</em></span><span class="No-Break"> (</span><a href="https://git-scm.com/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://git-scm.com/</span></a><span class="No-Break">)</span></li>
				<li><span class="No-Break"><em class="italic">LlamaIndex</em></span><span class="No-Break"> (</span><a href="https://github.com/run-llama/llama_index" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://github.com/run-llama/llama_index</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">OpenAI account</em> and an <span class="No-Break"><em class="italic">API key</em></span></li>
				<li><span class="No-Break"><em class="italic">Streamlit</em></span><span class="No-Break"> (</span><a href="https://github.com/streamlit/streamlit" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://github.com/streamlit/streamlit</span></a><span class="No-Break">)</span></li>
				<li><span class="No-Break"><em class="italic">PyPDF</em></span><span class="No-Break"> (</span><a href="https://pypi.org/project/pypdf/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pypi.org/project/pypdf/</span></a><span class="No-Break">)</span></li>
				<li><span class="No-Break"><em class="italic">DOC2Txt</em></span><span class="No-Break"> (</span><a href="https://github.com/ankushshah89/python-docx2txt/blob/master/docx2txt/docx2txt.py" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://github.com/ankushshah89/python-docx2txt/blob/master/docx2txt/docx2txt.py</span></a><span class="No-Break">)</span></li>
			</ul>
			<p>All the sample code snippets presented throughout this book as well as the entire project code base can be found in this GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex</span></a><span class="No-Break">.</span></p>
			<h2 id="f_4__idParaDest-27" data-type="sect1" class="sect1" title2="Optimizing language models – the symbiosis of fine-tuning, RAG, and LlamaIndex" no2="2.2"><a id="_idTextAnchor026"></a>2.2. Optimizing language models – the symbiosis of 
fine-tuning, RAG, and LlamaIndex</h2>
			<p>In the previous chapter, we saw that vanilla LLMs have some limitations right outside of the box. Their <a id="_idIndexMarker037"></a>knowledge is static and they occasionally spit out nonsense. We also learned about RAG as a potential way to mitigate <a id="_idIndexMarker038"></a>these issues. Blending <strong class="bold">prompt engineering</strong> techniques with programmatic methods, RAG can elegantly solve many of the <span class="No-Break">LLM shortcomings.</span></p>
			<p class="callout-heading">What is prompt engineering?</p>
			<p class="callout">Prompt <a id="_idIndexMarker039"></a>engineering involves crafting text inputs designed to be effectively processed by a <strong class="bold">generative AI</strong> (<strong class="bold">GenAI</strong>) model. Composed in natural language, these prompts describe the specific tasks to be carried out by the AI. We’ll have a much deeper conversation on this topic during <a href="#_idTextAnchor214"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Prompt Engineering Guidelines and </em><span class="No-Break"><em class="italic">Best Practices</em></span><span class="No-Break">.</span></p>
			<h3 id="f_4__idParaDest-28" data-type="sect2" class="sect2" title2="Is RAG the only possible solution?" no2="2.2.1"><a id="_idTextAnchor027"></a>2.2.1. Is RAG the only possible solution?</h3>
			<p>Of course not. Another approach is to fine-tune the AI model, which involves additional training on <a id="_idIndexMarker040"></a>proprietary data to adapt the LLM and embed new data. It takes a model that is pre-trained on a general collection of data and continues its training on a more specialized dataset. This specialized dataset can be tailored to a particular domain, language, or set of tasks that you are interested in. The result is a model that maintains its broad knowledge base while gaining expertise in a <span class="No-Break">specific area.</span></p>
			<p>Take a look at <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.1</em> for a graphical explanation of <span class="No-Break">the process.</span></p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B21861_02_1.jpg" alt="Figure 2.1 – An illustration of the LLM fine-tuning process" width="1100" height="555" data-type="figure" id="untitled_figure_7" title2="– An illustration of the LLM fine-tuning process" no2="2.1">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – An illustration of the LLM fine-tuning process</p>
			<p>Fine-tuning can improve performance but has drawbacks, such as being expensive, requiring large datasets, and being difficult to update with fresh information. It also has the disadvantage of permanently altering the original AI model, which makes it inappropriate for personalizing purposes. Think of the original AI model as a classic recipe for a beloved dish. Fine-tuning this model is akin to modifying the traditional recipe to suit specific tastes or requirements. While these changes can make the dish more suitable for some, they also fundamentally alter the <span class="No-Break">original recipe.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Not all <a id="_idIndexMarker041"></a>fine-tuning methods permanently alter the base AI model. Take <strong class="bold">Low-Rank Adaptation</strong> (<strong class="bold">LoRA</strong>) for example. LoRA is a fine-tuning method for LLMs that offers a more efficient approach compared to traditional <strong class="bold">full fine-tuning</strong>. In full fine-tuning, all <a id="_idIndexMarker042"></a>layers of a neural network are optimized, which, while effective, is resource-intensive and time-consuming. LoRA, on the other hand, involves fine-tuning only two smaller matrices that approximate the larger weight matrix of the pre-trained LLM. In the LoRA method, the original weights of the model are <em class="italic">frozen</em>, meaning they are not directly updated during the fine-tuning process. The changes to the model’s behavior are achieved by the addition of these low-rank matrices. This approach allows for the original model to be preserved, while still enabling it to be adapted for new tasks or improved performance. You can find more information on this method <span class="No-Break">here: </span><a href="https://ar5iv.labs.arxiv.org/html/2106.09685" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://ar5iv.labs.arxiv.org/html/2106.09685</span></a><span class="No-Break">.</span></p>
			<p>Even though LoRA is more efficient in terms of memory usage compared to full fine-tuning, it still requires computational resources and expertise to implement and optimize effectively, which might be a barrier for some users. Using fine-tuning to create a more personalized experience for a large number of different users requires re-running the tuning process for every user, which is definitely <span class="No-Break">not cost-effective.</span></p>
			<p>I’m not trying to say that RAG is a better alternative to LLM fine-tuning. In fact, RAG and fine-tuning are complementary techniques that are often used together. However, to rapidly incorporating changing data and personalization, RAG <span class="No-Break">is preferable.</span></p>
			<h3 id="f_4__idParaDest-29" data-type="sect2" class="sect2" title2="What LlamaIndex does" no2="2.2.2"><a id="_idTextAnchor028"></a>2.2.2. What LlamaIndex does</h3>
			<p>With LlamaIndex, you can rapidly create <em class="italic">smart</em> LLMs that can adapt to your specific use case. Instead <a id="_idIndexMarker043"></a>of relying only on their generic pre-trained knowledge, you can inject targeted information so that they give you accurate, relevant answers. It provides an easy way to connect external datasets to LLMs such as GPT-4, Claude, and Llama. LlamaIndex builds a bridge between your custom knowledge and the vast capabilities <span class="No-Break">of LLMs.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Created in 2022 by Princeton University graduate and entrepreneur Jerry Liu, the <em class="italic">LlamaIndex framework</em> has quickly become very popular in the developer community. LlamaIndex allows you to take advantage of the computational power and language understanding capabilities of LLMs while focusing their responses on specific, reliable data. This unique combination enables businesses and individuals to get the most out of their AI investments, as they can use the same underlying technology for a wide array of <span class="No-Break">specialized applications.</span></p>
			<p>For example, you could index a collection of your company’s documents. Then, when you ask questions related to your business, the LLM augmented with LlamaIndex provides responses based on real data rather than just making up <span class="No-Break">vague answers!</span></p>
			<p>The result is that <a id="_idIndexMarker044"></a>you get all the expressive power of LLMs while greatly reducing the amount of incorrect or irrelevant information. LlamaIndex guides the LLM to pull from trusted sources you provide, and these sources could contain both <em class="italic">structured</em> and <em class="italic">unstructured</em> data. In fact, as we will see in the next chapters, the framework can ingest data from pretty much <em class="italic">any</em> data source available. That’s pretty <span class="No-Break">cool, right?</span></p>
			<p>If you are not already thinking about the many possible uses for this framework, let me give you some quick ideas. With LlamaIndex, you could do <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Build a search engine for your document collection</strong>: One of its most powerful applications is the ability to index all your documents – they could be PDFs, Word files, Notion documents, GitHub repos, or other formats. Once indexed, you can query the LLM to search for specific information, making it a powerful search engine tailored specifically for <span class="No-Break">your resources</span></li>
				<li><strong class="bold">Create a company chatbot with customized knowledge</strong>: If your business has specific jargon, policies, or expertise, you can make the LLM <em class="italic">understand</em> these nuances. The chatbot could then handle a range of queries, from basic customer service questions to more specialized interactions that would typically require <span class="No-Break">human expertise</span></li>
				<li><strong class="bold">Generate summaries of large reports or papers</strong>: If your organization deals with lengthy documents or reports, LlamaIndex can be used to feed the LLM with their contents. Then, you can ask the LLM to generate concise summaries, capturing the most <span class="No-Break">important points</span></li>
				<li><strong class="bold">Develop a smart assistant for complex workflows</strong>: By training the LLM on the nuances of multi-step tasks or procedures unique to your organization, you can transform it into a smart assistant data agent that provides valuable insights <span class="No-Break">and guidance</span></li>
			</ul>
			<p>And these are just the tip of <span class="No-Break">the iceberg.</span></p>
			<p>In addition, <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.2</em> shows how implementing smart RAG strategies can offset some of the costs <a id="_idIndexMarker045"></a>associated with fine-tuning the model on a <span class="No-Break">specific domain.</span></p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B21861_02_2.jpg" alt="Figure 2.2 – The relative costs of updating data in a pre-trained LLM" width="1650" height="792" data-type="figure" id="untitled_figure_8" title2="– The relative costs of updating data in a pre-trained LLM" no2="2.2">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – The relative costs of updating data in a pre-trained LLM</p>
			<p>Before we dive deeper into the applications and use cases of the LlamaIndex framework, let’s talk a bit about the architecture and the design principles <span class="No-Break">behind it!</span></p>
			<h2 id="f_4__idParaDest-30" data-type="sect1" class="sect1" title2="Discovering the advantages of progressively disclosing complexity" no2="2.3"><a id="_idTextAnchor029"></a>2.3. Discovering the advantages of progressively disclosing complexity</h2>
			<p>The creator of LlamaIndex wanted to make it accessible to everyone – from beginners just getting <a id="_idIndexMarker046"></a>started with LLMs all the way to expert developers building complex systems. That’s why LlamaIndex uses a design <a id="_idIndexMarker047"></a>principle called <strong class="bold">progressive disclosure of complexity</strong>. Don’t worry about the fancy name – it just means that the framework starts simple and gradually reveals more advanced features when you <span class="No-Break">need them.</span></p>
			<p>When you first use LlamaIndex, it feels like magic! With just a few lines of code, you can connect data and start querying the LLM. Under the hood, LlamaIndex converts the data into an efficient index that the LLM <span class="No-Break">can use.</span></p>
			<p>Have a look at this very simple example that first loads a set of text documents from a local directory. It then builds an index over the documents and queries that index to get a summarized view of the documents based on a natural <span class="No-Break">language query:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_4" title2="(no caption)" no2="">from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
documents = SimpleDirectoryReader('files').load_data()
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine()
response = query_engine.query(
&nbsp;&nbsp;&nbsp;&nbsp;"summarize each document in a few sentences"
)
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>It’s that <a id="_idIndexMarker048"></a>simple. Just six lines <span class="No-Break">of code!</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Don’t try to run the code just yet. It’s more for illustration purposes. There is a bit of environmental preparation we need to handle before that. Don’t worry, we’ll cover that a bit later in this chapter and then you’ll be ready <span class="No-Break">to go.</span></p>
			<p>As you use LlamaIndex more, you will uncover its more powerful capabilities. There are plenty of parameters you can tweak. You can select specialized index structures optimized for different uses, carry out detailed cost analyses for different prompt strategies, customize query algorithms, and <span class="No-Break">much more.</span></p>
			<p>But LlamaIndex always starts you off gently before getting into more detailed workings, and for quick and simple projects, you don’t need to go much deeper than that. This way, both beginners and experts can benefit from its versatility <span class="No-Break">and capabilities.</span></p>
			<p>Now, let’s go on a quick tour of our hands-on project and then start prepping for the fun part: writing <span class="No-Break">the code.</span></p>
			<h3 id="f_4__idParaDest-31" data-type="sect2" class="sect2" title2="An important aspect to consider" no2="2.3.1"><a id="_idTextAnchor030"></a>2.3.1. An important aspect to consider</h3>
			<p>As you go further through this book, and you will most likely want to experiment based on the <a id="_idIndexMarker049"></a>examples it gives, you need to keep one very important point in mind. By default, the LlamaIndex framework is configured to use AI models provided by OpenAI. Although these models are extremely powerful and versatile, they incur costs. Many of the LlamaIndex functionalities presented in this book, be it metadata extraction, indexing, retrieval, or response synthesis, are based on either LLMs or embedding models. I have tried to use as simple examples as possible with small sample datasets in an attempt to limit these costs as much <span class="No-Break">as possible.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">I strongly advise you to keep a close eye on the OpenAI API consumption. In case you don’t already have it, the link where you can monitor the API usage is here: <a href="https://platform.openai.com/usage" target="_blank" rel="noopener noreferrer">https://platform.openai.com/usage</a>. I also advise you to be careful from a privacy perspective. These issues are discussed in more detail in <em class="italic">Chapters 4</em> <span class="No-Break">and </span><span class="No-Break"><em class="italic">5</em></span><span class="No-Break">.</span></p>
			<p>Alternatively, if you want to avoid both the costs of using an external LLM and the potential privacy risks, you can apply the methods described in <a href="#_idTextAnchor197"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Customizing and Deploying Our LlamaIndex Project</em>. It is important to note, however, that all examples provided in the book are written and tested using the default models provided by OpenAI. There is a (quite likely) possibility that some examples may not work as well – or at all – running on locally <span class="No-Break">hosted alternatives.</span></p>
			<h2 id="f_4__idParaDest-32" data-type="sect1" class="sect1" title2="Introducing PITS – our LlamaIndex hands-on project" no2="2.4"><a id="_idTextAnchor031"></a>2.4. Introducing PITS – our LlamaIndex hands-on project</h2>
			<p><em class="italic">Nothing beats learning </em><span class="No-Break"><em class="italic">by doing</em></span><span class="No-Break">.</span></p>
			<p>So, I’ve <a id="_idIndexMarker050"></a>cooked up a fun and useful project for us to start <span class="No-Break">using LlamaIndex!</span></p>
			<p>Here, we will introduce PITS. Wouldn’t it be cool to have an AI tutor that helps you learn new concepts interactively? Well, we’re going to build <span class="No-Break">one together!</span></p>
			<h3 id="f_4__idParaDest-33" data-type="sect2" class="sect2" title2="Here’s how it will work" no2="2.4.1"><a id="_idTextAnchor032"></a>2.4.1. Here’s how it will work</h3>
			<p>First, you will introduce yourself to PITS. You’ll have the chance to describe the topic you want to learn about and specify any personal learning preferences you <span class="No-Break">may have.</span></p>
			<p>Then, you will <a id="_idIndexMarker051"></a>be able to upload any existing study materials you may have on the topic. PITS will accept and ingest any PDFs, Word documents, or text files you <span class="No-Break">may provide.</span></p>
			<p>Based on the documents provided, the tutor will first build a quiz. You’ll have the option to complete the quiz. That way, the tutor will be able to gauge your current knowledge of the topic and adjust the <span class="No-Break">learning experience.</span></p>
			<p>Our nifty tutor will then build learning material for you. This will consist of slides and narration for each slide. The training material will be divided <span class="No-Break">into chapters.</span></p>
			<p>Then, your learning journey begins. During each learning session, PITS you will advance through the chapters, presenting each topic in your preferred style and adapting to your <span class="No-Break">knowledge level.</span></p>
			<p>After each concept is explained, you’ll have a chance to ask for more explanations or examples to learn more about the topic. It will answer your questions, create quizzes, explain concepts, and adapt responses based on <span class="No-Break">your needs.</span></p>
			<p>The best part is that your entire conversation with the agent will be recorded. It will remember both your questions and its own answers so it won’t repeat itself or lose the <span class="No-Break">conversation context.</span></p>
			<p>Too tired to continue in one session? Not a problem. When you’re ready to start another lesson, it will just resume from where you left off and give you a summary of the <span class="No-Break">previous discussion.</span></p>
			<p>But, hey! They say a picture’s worth a thousand <span class="No-Break">words, right?</span></p>
			<p>You’ll find an overview in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B21861_02_3.jpg" alt="Figure 2.3 – An overview of the PITS workflow" width="1650" height="796" data-type="figure" id="untitled_figure_9" title2="– An overview of the PITS workflow" no2="2.3">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – An overview of the PITS workflow</p>
			<p>It doesn’t <a id="_idIndexMarker052"></a>really get more customized than this. This is the ultimate <span class="No-Break">learning experience.</span></p>
			<p>As you can imagine, PITS needs to be smart on several fronts. It needs to be able to do <span class="No-Break">the following:</span></p>
			<ul>
				<li>Understand and index the study materials <span class="No-Break">we provide</span></li>
				<li>Converse fluently with users and retain <span class="No-Break">the context</span></li>
				<li>Teach effectively based on the <span class="No-Break">indexed knowledge</span></li>
			</ul>
			<p>LlamaIndex will help with the first part by ingesting the study material. The user will be able to upload any relevant training material such as manuals, slides or even student notes, and <span class="No-Break">sample questions.</span></p>
			<p>For the second part, we’ll mostly use the capabilities of GPT-4 to power the actual <span class="No-Break">teaching interactions.</span></p>
			<p>However, the foundation will be the knowledge augmentation capabilities of LlamaIndex. Pretty neat, right? We’ll have a personally <span class="No-Break">customized tutor!</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">I’m not sure whether you’ve read my biography, but I work as a trainer. The moment I first learned of the power of GenAI and discovered GPT-3, I knew exactly that a few years <a id="_idIndexMarker053"></a>from now, systems such as PITS would emerge sooner or later. I was thrilled about their potential to provide free, quality education to people around the world, regardless of their location, background, or financial status. Later, when I discovered RAG and tools such as LlamaIndex, I became convinced that they would appear rather sooner <span class="No-Break">than later.</span></p>
			<p>Okay, enough daydreaming – let’s start setting up <span class="No-Break">the pieces.</span></p>
			<h2 id="f_4__idParaDest-34" data-type="sect1" class="sect1" title2="Preparing our coding environment" no2="2.5"><a id="_idTextAnchor033"></a>2.5. Preparing our coding environment</h2>
			<p>Before we embark on the LlamaIndex coding journey, it’s essential to set up our development <a id="_idIndexMarker054"></a>environment properly. This setup is the first step toward ensuring that we can smoothly run through the examples and exercises I’ve prepared <span class="No-Break">for you.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">To maintain simplicity and ensure consistency across all examples, I’ve designed the sample code to be run in a local Python environment. I’m aware that many of you are fond of using web-based coding environments such as Google Colab and Jupyter Notebooks for your coding projects, so I kindly ask for your understanding if these examples do not directly translate to or run in these platforms. My goal was to keep our setup straightforward, allowing us to focus on the learning experience without compatibility concerns. Thank you for your understanding and <span class="No-Break">happy coding!</span></p>
			<p>Let’s quickly get our computer set up for some cool <span class="No-Break">LlamaIndex coding.</span></p>
			<h3 id="f_4__idParaDest-35" data-type="sect2" class="sect2" title2="Installing Python" no2="2.5.1"><a id="_idTextAnchor034"></a>2.5.1. Installing Python</h3>
			<p>You’ll need a Python 3.7+ environment. I recommend Python 3.11 <span class="No-Break">if possible.</span></p>
			<p>If you don’t <a id="_idIndexMarker055"></a>have Python, install it from <a href="https://www.python.org" target="_blank" rel="noopener noreferrer">https://www.python.org</a>. If you already have an older version, you can upgrade or install a newer Python version side <span class="No-Break">by side.</span></p>
			<p>For a coding <a id="_idIndexMarker056"></a>environment, my personal preference is <strong class="bold">NotePad++</strong> (<a href="https://notepad-plus-plus.org/" target="_blank" rel="noopener noreferrer">https://notepad-plus-plus.org/</a>), which is not quite an IDE but is very fast. However, you can <a id="_idIndexMarker057"></a>also use Microsoft’s <strong class="bold">VSCode</strong> (<a href="https://code.visualstudio.com/" target="_blank" rel="noopener noreferrer">https://code.visualstudio.com/</a>), <strong class="bold">PyCharm</strong> (<a href="https://www.jetbrains.com/pycharm/" target="_blank" rel="noopener noreferrer">https://www.jetbrains.com/pycharm/</a>), or <a id="_idIndexMarker058"></a>anything else <span class="No-Break">you prefer.</span></p>
			<h3 id="f_4__idParaDest-36" data-type="sect2" class="sect2" title2="Installing Git" no2="2.5.2"><a id="_idTextAnchor035"></a>2.5.2. Installing Git</h3>
			<p>Before we proceed, it’s important to have Git installed. Git is a version control system that lets you <a id="_idIndexMarker059"></a>manage changes to your code and collaborate with others. It’s <a id="_idIndexMarker060"></a>also essential for cloning code repositories, like the one we’ll be using in <span class="No-Break">this book.</span></p>
			<p>Head over <a id="_idIndexMarker061"></a>to the official Git website (<a href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git" target="_blank" rel="noopener noreferrer">https://git-scm.com/book/en/v2/Getting-Started-Installing-Git</a>) and download the installer for your <span class="No-Break">operating system.</span></p>
			<p>Follow the installation steps, and you should have Git up and running in <span class="No-Break">no time.</span></p>
			<p>All the sample code snippets presented throughout the book as well as the entire project code base can be found in this GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex</span></a><span class="No-Break">.</span></p>
			<p>So, if you want to download the project files locally, once you have finished installing Git, you can simply follow <span class="No-Break">these steps:</span></p>
			<ol>
				<li><strong class="bold">Navigate to the desired directory</strong>: Open a new command prompt or terminal window. Use the <strong class="source-inline">cd</strong> command to navigate to the directory where you’d like to store the project. Here is <span class="No-Break">an example:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="terminal" id="untitled_terminal_1" title2="(no caption)" no2=""><strong class="bold">cd path/to/your/directory</strong></pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>				<li><strong class="bold">Clone the repository</strong>: Run the following command to clone the <span class="No-Break">GitHub repository:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="terminal" id="untitled_terminal_2" title2="(no caption)" no2=""><strong class="bold">git clone https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex</strong></pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div><p class="list-inset">This will download a copy of the project to your <span class="No-Break">local machine.</span></p></li>				<li><strong class="bold">Enter the project directory</strong>: Navigate into the newly created <span class="No-Break">project folder:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="terminal" id="untitled_terminal_3" title2="(no caption)" no2=""><strong class="bold">cd Building-Data-Driven-Applications-with-LlamaIndex</strong></pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div><p class="list-inset">As we move forward with our project, you have <span class="No-Break">two options:</span></p><ul><li>You can either write the code on your own and then compare it with what’s in <span class="No-Break">the repository</span></li><li>Or you can directly explore the code files in the repository to get a better understanding of the <span class="No-Break">code structure</span></li></ul></li>			</ol>
			<p>If you <a id="_idIndexMarker062"></a>correctly performed all of the preceding steps, listing the <a id="_idIndexMarker063"></a>contents of the current folder should return several subfolders called <strong class="source-inline">chX</strong> – where <strong class="source-inline">X</strong> is the chapter number, and a separate subfolder called <strong class="source-inline">PITS_APP</strong>. The chapter folders contain all sample source files corresponding to each chapter. The <strong class="source-inline">PITS_APP</strong> folder contains the source code for our <span class="No-Break">main project.</span></p>
			<h3 id="f_4__idParaDest-37" data-type="sect2" class="sect2" title2="Installing LlamaIndex" no2="2.5.3"><a id="_idTextAnchor036"></a>2.5.3. Installing LlamaIndex</h3>
			<p>Next, let’s <a id="_idIndexMarker064"></a>get the LlamaIndex library installed. At your command <a id="_idIndexMarker065"></a>prompt, run <span class="No-Break">the following:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_5" title2="(no caption)" no2="">pip install llama-index</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This will include a LlamaIndex package that contains the core LlamaIndex components as well as a selection of useful integrations. For the most efficient deployment possible, there is also the option of installing just the minimum core components and only the necessary integrations, but for the purpose of this book, the presented option will do <span class="No-Break">just fine.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">In case you’re already running a version older than v0.10, it is recommended that you start with <a id="_idIndexMarker066"></a>a fresh install in a virtual environment to avoid any <a id="_idIndexMarker067"></a>conflicts with the legacy version. You can find detailed instructions <span class="No-Break">here: </span><a href="https://pretty-sodium-5e0.notion.site/v0-10-0-Migration-Guide-6ede431dcb8841b09ea171e7f133bd77" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pretty-sodium-5e0.notion.site/v0-10-0-Migration-Guide-6ede431dcb8841b09ea171e7f133bd77</span></a><span class="No-Break">.</span></p>
			<p>We’re now ready to import and start <span class="No-Break">using it.</span></p>
			<h3 id="f_4__idParaDest-38" data-type="sect2" class="sect2" title2="Signing up for an OpenAI API key" no2="2.5.4"><a id="_idTextAnchor037"></a>2.5.4. Signing up for an OpenAI API key</h3>
			<p>Since we’ll <a id="_idIndexMarker068"></a>be using OpenAI’s GPT models <a id="_idIndexMarker069"></a>via LlamaIndex, you’ll need an API key to authenticate. Head to <a href="https://platform.openai.com" target="_blank" rel="noopener noreferrer">https://platform.openai.com</a> and sign up. Once logged in, you can create <a id="_idIndexMarker070"></a>a new secret API key. Make sure to keep <span class="No-Break">it safe!</span></p>
			<p>LlamaIndex will use this key every time it interacts with OpenAI’s models. Because it has to be kept secret, it’s a good idea to store it in an environment variable on your <span class="No-Break">local machine.</span></p>
			<h4 data-type="sect3" class="sect3" title2="A short guide for Windows users" no2="2.5.4.1">2.5.4.1. A short guide for Windows users</h4>
			<p>On <a id="_idIndexMarker071"></a>Windows, you can accomplish that by following <span class="No-Break">these steps:</span></p>
			<ol>
				<li>Open <strong class="bold">Environment Variables</strong>: Open the Start menu and search for <strong class="bold">Environment Variables</strong> or right-click on <strong class="bold">This PC</strong> or <strong class="bold">My Computer</strong> and <span class="No-Break">select </span><span class="No-Break"><strong class="bold">Properties</strong></span><span class="No-Break">.</span></li>
				<li>Then, click on <strong class="bold">Advanced system settings</strong> followed by the <strong class="bold">Environment Variables</strong> button in the <strong class="bold">Advanced</strong> tab as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.4:</em></span></li>
			</ol>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B21861_02_4.jpg" alt="Figure 2.4 – Editing Windows environment variables" width="617" height="713" data-type="figure" id="untitled_figure_10" title2="– Editing Windows environment variables" no2="2.4">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – Editing Windows environment variables</p>
			<ol>
				<li value="3"><strong class="bold">Create a new environment variable</strong>: In the <strong class="bold">Environment Variables</strong> window, under <a id="_idIndexMarker072"></a>the <strong class="bold">User variables</strong> section, click the <span class="No-Break"><strong class="bold">New</strong></span><span class="No-Break"> button.</span></li>
				<li><strong class="bold">Enter the variable details</strong>: For the <strong class="bold">Variable name</strong>, enter <strong class="source-inline">OPENAI_API_KEY</strong>. For <strong class="bold">Variable value</strong>, paste the secret API key you received from OpenAI. See <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.5</em> for <span class="No-Break">an illustration.</span></li>
			</ol>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B21861_02_5.jpg" alt="Figure 2.5 – Creating the OPENAI_API_KEY environment variable" width="881" height="946" data-type="figure" id="untitled_figure_11" title2="– Creating the OPENAI_API_KEY environment variable" no2="2.5">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5 – Creating the OPENAI_API_KEY environment variable</p>
			<ol>
				<li value="5"><strong class="bold">Confirm and apply</strong>: Click <strong class="bold">OK</strong> to close all of the dialog boxes. You will need to restart <a id="_idIndexMarker073"></a>your computer for the changes to <span class="No-Break">take effect.</span></li>
				<li><strong class="bold">Verify the environment variable</strong>: To ensure the variable is set correctly, open a new command prompt, and run <span class="No-Break">the following:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="terminal" id="untitled_terminal_4" title2="(no caption)" no2=""><strong class="bold">echo %OPENAI_API_KEY%</strong></pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>			</ol>
			<p>This should display the API key you <span class="No-Break">just stored.</span></p>
			<h4 data-type="sect3" class="sect3" title2="A short guide for Linux/Mac users" no2="2.5.4.2">2.5.4.2. A short guide for Linux/Mac users</h4>
			<p>On Linux/Mac, you can accomplish Signing up for an OpenAI API key by following <span class="No-Break">these steps:</span></p>
			<ol>
				<li>Run the following command in your terminal, replacing <strong class="source-inline">&lt;yourkey&gt;</strong> with your <span class="No-Break">API key:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="terminal" id="untitled_terminal_5" title2="(no caption)" no2=""><strong class="bold">echo "export OPENAI_API_KEY='yourkey'" &gt;&gt; ~/.zshrc</strong></pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>				<li>Update <a id="_idIndexMarker074"></a>the shell with the <span class="No-Break">new variable:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="terminal" id="untitled_terminal_6" title2="(no caption)" no2=""><strong class="bold">source ~/.zshrc</strong></pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>				<li>Make sure <a id="_idIndexMarker075"></a>that you have set your environment variable with the <span class="No-Break">following command:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="terminal" id="untitled_terminal_7" title2="(no caption)" no2=""><strong class="bold">echo $OPENAI_API_KEY</strong></pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div><p class="list-inset">Your OpenAI API key is now securely stored in an environment variable and can be easily accessed by LlamaIndex when needed, without exposing it in your code <span class="No-Break">or system.</span></p></li>			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">While OpenAI provides a free trial option for their GPT models through their API, you’ll only receive a limited number of free credits. Currently, the free credit is limited to $5 and expires after 3 months. That should be more than enough to experiment for the purpose of our project and for reading the book. However, If you wish to get serious about building LLM-based applications, you’ll have to sign up for a paid account on their platform. Alternatively, you can always choose to use another AI model for LlamaIndex. We will discuss customizing the AI model in more detail in <a href="#_idTextAnchor214"><span class="No-Break"><em class="italic">Chapter 10</em></span></a><em class="italic">, Prompt Engineering Guidelines and </em><span class="No-Break"><em class="italic">Best Practices</em></span><span class="No-Break">.</span></p>
			<p>OK. The backend is all set up. Let’s talk about the rest of <span class="No-Break">the stack.</span></p>
			<h3 id="f_4__idParaDest-39" data-type="sect2" class="sect2" title2="Discovering Streamlit – the perfect tool for rapid building and deployment!" no2="2.5.5"><a id="_idTextAnchor038"></a>2.5.5. Discovering Streamlit – the perfect tool for rapid building and deployment!</h3>
			<p>Before we <a id="_idIndexMarker076"></a>can build cool apps such as PITS, we need <a id="_idIndexMarker077"></a>somewhere to … well, build and run them! That’s where Streamlit comes in. Streamlit is an awesome open-source Python library that makes it super easy to create and deploy web apps <span class="No-Break">and dashboards.</span></p>
			<p>With just a few lines of Python code, you can build complete web interfaces and see the results instantly. The best part is that Streamlit apps can be deployed nearly anywhere – on servers, on platforms such as Heroku, or even directly <span class="No-Break">from GitHub!</span></p>
			<p>I love Streamlit because it lets me focus on the fun stuff – such as creating PITS with LlamaIndex – rather than fussing over complex web development. For AI experimentation, <span class="No-Break">it’s perfect!</span></p>
			<p>We’ll primarily <a id="_idIndexMarker078"></a>use it to create the interface for uploading study guides and interacting with our PITS tutor. For the purpose of the next chapters, we’ll be using Streamlit <a id="_idIndexMarker079"></a>for running and testing our app locally. However, in <a href="#_idTextAnchor197"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Customizing and Deploying Our LlamaIndex Project</em>, we will also discover how we can easily deploy our app using <strong class="bold">Streamlit Share</strong> or any other hosting service <span class="No-Break">you prefer.</span></p>
			<p>Streamlit has tons of cool capabilities such as data frames, charts, and widgets – but don’t worry about learning it all now. As we build up features, I’ll explain the relevant parts so you can gain Streamlit skills along <span class="No-Break">the way!</span></p>
			<h3 id="f_4__idParaDest-40" data-type="sect2" class="sect2" title2="Installing Streamlit" no2="2.5.6"><a id="_idTextAnchor039"></a>2.5.6. Installing Streamlit</h3>
			<p>Lastly, we need <a id="_idIndexMarker080"></a>to install the <span class="No-Break">Streamlit library:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_6" title2="(no caption)" no2="">pip install streamlit</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Great! We <a id="_idIndexMarker081"></a>have our backend tool (LlamaIndex), our frontend layer (Streamlit), and our goal (PITS). It’s time for a <span class="No-Break">final touch.</span></p>
			<h3 id="f_4__idParaDest-41" data-type="sect2" class="sect2" title2="Finishing up" no2="2.5.7"><a id="_idTextAnchor040"></a>2.5.7. Finishing up</h3>
			<p>Because our <a id="_idIndexMarker082"></a>project should be able to ingest PDF and DOCX documents, we will also need to install two <span class="No-Break">additional libraries:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_7" title2="(no caption)" no2="">pip install pypdf
pip install docx2txt</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>That’s it! Our environment is <span class="No-Break">LlamaIndex ready.</span></p>
			<p>Let’s recap what <span class="No-Break">we have:</span></p>
			<ul>
				<li><span class="No-Break">Python 3.11</span></li>
				<li><span class="No-Break">Git</span></li>
				<li><span class="No-Break">LlamaIndex package</span></li>
				<li>OpenAI <a id="_idIndexMarker083"></a>account and an <span class="No-Break">API key</span></li>
				<li>Streamlit for <span class="No-Break">app building</span></li>
				<li>PyPDF and <span class="No-Break">DOC2Txt libraries</span></li>
			</ul>
			<h3 id="f_4__idParaDest-42" data-type="sect2" class="sect2" title2="One final check" no2="2.5.8"><a id="_idTextAnchor041"></a>2.5.8. One final check</h3>
			<p>To verify <a id="_idIndexMarker084"></a>that everything was installed correctly, open a new command prompt or terminal window, and run the <span class="No-Break">following commands:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_8" title2="(no caption)" no2="">python --version
git --version
pip show llama-index
echo %OPENAI_API_KEY%
pip show streamlit
pip show pypdf
pip show docx2txt</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>A simple way to check whether your environment is ready is to try navigating into the <strong class="source-inline">ch2</strong> subfolder of your local <strong class="source-inline">git</strong> folder and run the file <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">sample1.py</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_9" title2="(no caption)" no2="">python sample1.py</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>You should get a nice summary of the two sample documents provided in the <strong class="source-inline">ch2/files</strong> subfolder if everything has been <span class="No-Break">properly installed.</span></p>
			<p>If anything is missing, please go back and retake the necessary steps before proceeding further. Trust me, you’ll avoid a lot of pain and frustration further down <span class="No-Break">the line.</span></p>
			<p>We’re all set to start ingesting data, constructing indices with LlamaIndex, and building our PITS tutor app! I don’t know about you, but I’m <em class="italic">kid-in-a-candy-store</em> excited to <span class="No-Break">start experimenting.</span></p>
			<p>In the next chapters, we’ll get hands-on with our first LlamaIndex program. This is where the real fun begins! We’ll explore ingesting data, constructing indexes, executing queries, <span class="No-Break">and more.</span></p>
			<p>I’ll explain <a id="_idIndexMarker085"></a>each concept and line of code in simple terms along the way. In no time, you’ll be implementing the basics like a LlamaIndex pro! Once we’ve got these fundamentals down, we can start expanding the capabilities of our <span class="No-Break">tutor app.</span></p>
			<p>But first, let’s clarify the overall code structure of the framework’s <span class="No-Break">GitHub repository.</span></p>
			<h2 id="f_4__idParaDest-43" data-type="sect1" class="sect1" title2="Familiarizing ourselves with the structure of the LlamaIndex code repository" no2="2.6"><a id="_idTextAnchor042"></a>2.6. Familiarizing ourselves with the structure of the LlamaIndex code repository</h2>
			<p>Because you’ll probably spend a lot of time browsing the official code repository of the LlamaIndex <a id="_idIndexMarker086"></a>framework, it’s good to have an overall image of its general structure. You can always consult the repository <span class="No-Break">here: </span><a href="https://github.com/run-llama/llama_index" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://github.com/run-llama/llama_index</span></a><span class="No-Break">.</span></p>
			<p>Starting with version 0.10, the code has been thoroughly reorganized into a more modular structure. The purpose of this new structure is to improve efficiency, by avoiding loading any unnecessary dependencies, while also improving readability and overall user experience <span class="No-Break">for developers.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.6</em> describes the main components of the <span class="No-Break">code structure:</span></p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B21861_02_6.jpg" alt="Figure 2.6 – The LlamaIndex GitHub repository code structure" width="1344" height="779" data-type="figure" id="untitled_figure_12" title2="– The LlamaIndex GitHub repository code structure" no2="2.6">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6 – The LlamaIndex GitHub repository code structure</p>
			<p>The <strong class="source-inline">llama-index-core</strong> folder serves as the foundational package for LlamaIndex, enabling developers to install the essential framework and then selectively add from over 300 integration packages and different Llama-packs to tailor functionality for their specific <span class="No-Break">application needs.</span></p>
			<p>The <strong class="source-inline">llama-index-integrations</strong> folder of LlamaIndex consists of various add-on packages that extend the functionality of the core framework. These allow developers to customize <a id="_idIndexMarker087"></a>their build with specific elements such as custom LLMs, data loaders, embedding models, and vector store providers to best fit their application’s requirements. We’ll cover some of these integrations later in our book, starting with <a href="#_idTextAnchor068"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Ingesting Data into Our </em><span class="No-Break"><em class="italic">RAG Workflow</em></span><span class="No-Break">.</span></p>
			<p>The <strong class="source-inline">llama-index-packs</strong> folder contains more than 50 Llama packs. Developed and constantly improved by the LlamaIndex developer community, these packs serve as ready-made templates designed to kickstart a user’s application. We’ll talk about them in more detail during <a href="#_idTextAnchor197"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Customizing and Deploying Our </em><span class="No-Break"><em class="italic">LlamaIndex Project</em></span><span class="No-Break">.</span></p>
			<p>The <strong class="source-inline">llama-index-cli</strong> folder is used by the LlamaIndex command-line interface, which we will also cover briefly during <a href="#_idTextAnchor197"><span class="No-Break"><em class="italic">Chapter 9</em></span></a><em class="italic">, Customizing and Deploying Our </em><span class="No-Break"><em class="italic">LlamaIndex Project</em></span><span class="No-Break">.</span></p>
			<p>The last section, called <strong class="bold">OTHERS</strong> in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.6</em>, consists of two folders that currently contain fine-tuning abstractions and some experimental features that we will not cover in <span class="No-Break">this book.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">The subfolders in <strong class="source-inline">llama-index-integrations</strong> and <strong class="source-inline">llama-index-packs</strong> represent individual packages. The folder name corresponds to the package name. For example, the <strong class="source-inline">llama-index-integrations/llms/llama-index-llms-mistralai</strong> folder corresponds to the <strong class="source-inline">llama-index-llms-mistralai</strong> <span class="No-Break">PyPI package.</span></p>
			<p>Following this example, there is something you need to do before you import and use the <strong class="source-inline">mistralai</strong> package in your code <span class="No-Break">like this:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_10" title2="(no caption)" no2="">from llama_index.llms.mistralai import MistralAI</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>You’ll have to first install the corresponding PyPI package by running <span class="No-Break">the following:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_11" title2="(no caption)" no2="">pip install llama-index.llms.mistralai</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Don’t worry <a id="_idIndexMarker088"></a>too much about missing any necessary packages for the examples included in the book, as you will find them nicely listed at the beginning of each chapter under the <em class="italic">Technical </em><span class="No-Break"><em class="italic">requirements</em></span><span class="No-Break"> heading.</span></p>
			<h2 id="f_4__idParaDest-44" data-type="sect1" class="sect1" title2="Summary" no2="2.7"><a id="_idTextAnchor043"></a>2.7. Summary</h2>
			<p>In this chapter, we introduced LlamaIndex, a framework for connecting LLMs to external datasets. We discovered how LlamaIndex allows LLMs to incorporate real-world knowledge into <span class="No-Break">their responses.</span></p>
			<p>The chapter discussed the benefits of LlamaIndex over fine-tuning, such as easier updating and personalization. It introduced the concept of progressive disclosure of complexity, where LlamaIndex starts simple but reveals advanced capabilities <span class="No-Break">when needed.</span></p>
			<p>The chapter then presented an overview of the hands-on project PITS, a personalized intelligent tutoring system. It covered setting up the required tools such as Python, Git, and Streamlit, and getting an OpenAI API key. The chapter finished by verifying that the environment is ready for building <span class="No-Break">LlamaIndex apps.</span></p>
			<p>We’re now ready to continue our journey and proceed with a more technical understanding of the inner workings of the LlamaIndex framework. See you in the <span class="No-Break">next chapter!</span></p>
		</div>
<div id="f_5__idContainer024" class="part" data-type="part" file="B21861_Part_2_xhtml" title2="Starting Your First LlamaIndex Project" no2="2">
			<h1 id="f_5__idParaDest-45" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor044"></a>Part 2: Starting Your First LlamaIndex Project</h1>
		</div>
<div id="f_6__idContainer034" data-type="chapter" class="chapter" file="B21861_03_xhtml" title2="Kickstarting Your Journey with LlamaIndex" no2="3">
			<h1 id="f_6__idParaDest-46" class="chapter-number"><a id="_idTextAnchor045"></a>3</h1>
			<h1 class="H1---Chapter" id="f_6__idParaDest-47"><a id="_idTextAnchor046"></a>Kickstarting Your Journey with LlamaIndex</h1>
<p style="font-style: italic;">As this ebook edition doesn't have fixed pagination, the page numbers below are hyperlinked for reference only, based on the printed edition of this book.</p>
			<p>It’s time to dive deeper and gain a more technical understanding of how LlamaIndex works its magic under the hood. In this chapter, we’ll explore some of the key concepts and components that make up LlamaIndex’s architecture. We’ll learn about the core building blocks used by the framework to ingest, structure, and query our data. Understanding these fundamentals will provide a solid foundation before we start applying them hands-on. We’ll go through the theoretical aspects of each concept and then connect the dots between the theory and <span class="No-Break">practical application.</span></p>
			<p>Here are the main topics covered in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Uncovering the essential building blocks of LlamaIndex – <strong class="bold">Documents</strong>, <strong class="bold">Nodes</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="bold">indexes</strong></span></li>
				<li>Building our first interactive, augmented <strong class="bold">large language model</strong> (<span class="No-Break"><strong class="bold">LLM</strong></span><span class="No-Break">) application</span></li>
				<li>Starting our <strong class="bold">personalized intelligent tutoring system</strong> (<strong class="bold">PITS</strong>) project – a <span class="No-Break">hands-on exercise</span></li>
			</ul>
			<h2 id="f_6__idParaDest-48" data-type="sect1" class="sect1" title2="Technical requirements" no2="3.1"><a id="_idTextAnchor047"></a>3.1. Technical requirements</h2>
			<p>You will need to install the following Python libraries in your environment to be able to run the examples included in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li><span class="No-Break"><em class="italic">PYYAML</em></span><span class="No-Break"> (</span><a href="https://pyyaml.org/wiki/PyYAMLDocumentation" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pyyaml.org/wiki/PyYAMLDocumentation</span></a><span class="No-Break">)</span></li>
				<li><span class="No-Break"><em class="italic">Wikipedia</em></span><span class="No-Break"> (</span><a href="https://wikipedia.readthedocs.io/en/latest/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://wikipedia.readthedocs.io/en/latest/</span></a><span class="No-Break">)</span></li>
			</ul>
			<p>Two LlamaIndex integration packages will also <span class="No-Break">be required:</span></p>
			<ul>
				<li><em class="italic">Wikipedia </em><span class="No-Break"><em class="italic">reader</em></span><span class="No-Break"> (</span><a href="https://pypi.org/project/llama-index-readers-wikipedia/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pypi.org/project/llama-index-readers-wikipedia/</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">OpenAI </em><span class="No-Break"><em class="italic">LLMs</em></span><span class="No-Break"> (</span><a href="https://pypi.org/project/llama-index-llms-openai/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pypi.org/project/llama-index-llms-openai/</span></a><span class="No-Break">)</span></li>
			</ul>
			<p>All code samples from this chapter can be found in the <strong class="source-inline">ch3</strong> subfolder of the book’s GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex</span></a><span class="No-Break">.</span></p>
			<h2 id="f_6__idParaDest-49" data-type="sect1" class="sect1" title2="Uncovering the essential building blocks of LlamaIndex – documents, nodes, and indexes" no2="3.2"><a id="_idTextAnchor048"></a>3.2. Uncovering the essential building blocks of LlamaIndex – documents, nodes, and indexes</h2>
			<p>As we’re getting started with <a id="_idIndexMarker089"></a>LlamaIndex, it’s time to understand some of the key concepts and components that make up its architecture. You may consider this chapter as a quick introduction to the<a id="_idIndexMarker090"></a> typical <strong class="bold">retrieval-augmented generation</strong> (<strong class="bold">RAG</strong>) architecture with LlamaIndex and an overview of the most important tools provided by this framework. It should give you a basic understanding of how to build a simple RAG application. In the next chapters, we’ll take it step by step and explore in detail each one of the components <span class="No-Break">presented here.</span></p>
			<p>At a high level, LlamaIndex helps connect external data sources to LLMs. To do this effectively, it needs to ingest, structure, and organize your data in a way that allows for efficient retrieval and querying. In this first part of our chapter, we’ll explore the core elements that enable LlamaIndex to augment LLMs – Documents, Nodes, <span class="No-Break">and indexes.</span></p>
			<h3 id="f_6__idParaDest-50" data-type="sect2" class="sect2" title2="Documents" no2="3.2.1"><a id="_idTextAnchor049"></a>3.2.1. Documents</h3>
			<p>It all begins with <span class="No-Break">the data.</span></p>
			<p>Now, trying to handle<a id="_idIndexMarker091"></a> raw data directly can be as tricky as holding water in your hands. It’s often all over the place without any set structure. This is where we need to step in and give it some shape. That’s exactly what we do in LlamaIndex with something called Documents. A Document<a id="_idIndexMarker092"></a> is how we capture and contain any kind of data, whether you enter it manually or load it over from an external source. It’s like putting the data in a nice bottle so it’s easier <span class="No-Break">to handle.</span></p>
			<p>Imagine <a id="_idIndexMarker093"></a>you’ve got a bunch of your company’s procedures saved as PDFs and you want to make sense of them using a powerful language model such as GPT-4. In LlamaIndex, each of these procedures would be turned into its <strong class="source-inline">Document</strong> object – and it’s <a id="_idIndexMarker094"></a>not just about files. Say you have data sitting in a database or coming through an API – those can be Documents, too. Check out <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.1</em> for a <span class="No-Break">visual overview:</span></p>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/B21861_03_1.jpg" alt="Figure 3.1 – Documents can come from multiple sources" width="1076" height="451" data-type="figure" id="untitled_figure_13" title2="– Documents can come from multiple sources" no2="3.1">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – Documents can come from multiple sources</p>
			<p>Think of the <strong class="source-inline">Document</strong> class as a container. It holds not just the raw text or data from wherever it originated but also any extra bits of information you decide to tag along. This <a id="_idIndexMarker095"></a>extra info, called <strong class="bold">metadata</strong>, is a game changer when you start searching through your Documents because it lets you get really specific with <span class="No-Break">your queries.</span></p>
			<p>Here is a basic example of how a Document can be <span class="No-Break">created manually:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_12" title2="(no caption)" no2="">from llama_index.core import Document
text = "The quick brown fox jumps over the lazy dog."
doc = Document(
&nbsp;&nbsp;&nbsp;&nbsp;text=text,
&nbsp;&nbsp;&nbsp;&nbsp;metadata={'author': 'John Doe','category': 'others'},
&nbsp;&nbsp;&nbsp;&nbsp;id_='1'
)
print(doc)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In this<a id="_idIndexMarker096"></a> example, after importing the <strong class="source-inline">Document</strong> class, we create a <strong class="source-inline">Document</strong> object called <strong class="source-inline">doc</strong>. The object contains the actual text, a document ID, and <a id="_idIndexMarker097"></a>some additional metadata of our choice that is provided as <span class="No-Break">a dictionary.</span></p>
			<p>Here are some of the most important attributes of a <span class="No-Break"><strong class="source-inline">Document</strong></span><span class="No-Break"> object:</span></p>
			<ul>
				<li><strong class="source-inline">text</strong>: This attribute stores the text content of <span class="No-Break">the document</span></li>
				<li><strong class="source-inline">metadata</strong>: This attribute is a dictionary that can be used to include additional information about the document, such as the file name or categories. The keys in the metadata dictionary must be strings and the values can be strings, floats, <span class="No-Break">or integers</span></li>
				<li><strong class="source-inline">id_</strong>: This is a unique ID for each Document. You can set this manually if you want, but if you don’t specify an ID, LlamaIndex will automatically generate one <span class="No-Break">for you</span></li>
			</ul>
			<p>There are also other attributes that you can find by consulting the GitHub repository of LlamaIndex. However, to keep things simple, at this moment we will only focus on these three. These attributes provide various ways to customize and enhance the functionality of the <strong class="source-inline">Document</strong> class <span class="No-Break">in LlamaIndex.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.2</em> presents the basic structure of a <span class="No-Break">LlamaIndex Document.</span></p>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="image/B21861_03_2.jpg" alt="Figure 3.2 – The basic structure of a document" width="1062" height="343" data-type="figure" id="untitled_figure_14" title2="– The basic structure of a document" no2="3.2">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – The basic structure of a document</p>
			<p>The LlamaIndex Documents contain data in its unprocessed, or <strong class="bold">raw</strong>, form. Although the given example illustrates how we can manually create one, typically, in practical applications, these Documents are generated in bulk by sourcing them from various data sources. This bulk ingestion of data uses <a id="_idIndexMarker098"></a>predefined <strong class="bold">data loaders</strong> – sometimes <a id="_idIndexMarker099"></a>called <strong class="bold">connectors</strong> or <a id="_idIndexMarker100"></a>simply <strong class="bold">readers</strong> – from an extensive library <a id="_idIndexMarker101"></a>known as <span class="No-Break"><strong class="bold">LlamaHub</strong></span><span class="No-Break"> (</span><a href="https://llamahub.ai/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://llamahub.ai/</span></a><span class="No-Break">).</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Developed primarily by the LlamaIndex community, these plug-and-play packages extend the functionality of the core components of the framework. They provide different LLMs, agent tools, embedding models, vector stores, and data loaders. These data ingestion tools offer compatibility with a wide range of data file formats, databases, and API endpoints. There are more than 130 different data readers in LlamaHub already and the list keeps growing. We’ll cover the topic of LlamaHub in much more detail in the next chapter. For now, we’ll focus on the <span class="No-Break">data loaders.</span></p>
			<p>Here is a basic example of automated data ingestion using one of the predefined LlamaHub data loaders. Before you can run the example, make sure you install the libraries mentioned in the <em class="italic">technical requirements </em>section and complete all the necessary environment preparations mentioned in <a href="#_idTextAnchor023"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> if you <span class="No-Break">haven’t already:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_13" title2="(no caption)" no2="">pip install wikipedia
pip install llama-index-readers-wikipedia</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The first<a id="_idIndexMarker102"></a> library<a id="_idIndexMarker103"></a> allows for easy access and parsing of data from Wikipedia while the second one is the LlamaIndex integration for the Wikipedia <span class="No-Break">data loader.</span></p>
			<p>Once you have installed the two libraries, you’ll be able to run the <span class="No-Break">following example:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_14" title2="(no caption)" no2="">from llama_index.readers.wikipedia import WikipediaReader
loader = WikipediaReader()
documents = loader.load_data(
&nbsp;&nbsp;&nbsp;&nbsp;pages=['Pythagorean theorem','General relativity']
)
print(f"loaded {len(documents)} documents")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The <strong class="source-inline">WikpediaReader</strong> loader <a id="_idIndexMarker104"></a>extracts the text from Wikipedia articles using the Wikipedia Python package. Apart from <strong class="source-inline">WikipediaReader</strong>, there are many more specialized data connectors available in <span class="No-Break">the LlamaHub.</span></p>
			<p>So, creating <a id="_idIndexMarker105"></a>Documents is a very straightforward process. But how do <a id="_idIndexMarker106"></a>the raw <strong class="source-inline">Document</strong> objects get converted into a format that LLMs can efficiently process and reason over? This is where Nodes <span class="No-Break">come in.</span></p>
			<h3 id="f_6__idParaDest-51" data-type="sect2" class="sect2" title2="Nodes" no2="3.2.2"><a id="_idTextAnchor050"></a>3.2.2. Nodes</h3>
			<p>While <a id="_idIndexMarker107"></a>Documents represent the raw data and can be used as such, Nodes<a id="_idIndexMarker108"></a> are smaller chunks of content extracted from the Documents. The goal is to break down Documents into smaller, more manageable pieces of text. This serves a <span class="No-Break">few purposes:</span></p>
			<ul>
				<li><strong class="bold">Allows our proprietary knowledge to fit within the model’s prompt limits</strong>: Imagine that if we had an internal procedure that is 50 pages long, we would definitely run into size limit problems when trying to feed that in the context of our prompt. However, most likely, in practice, we wouldn’t need to feed the entire procedure in one prompt. Therefore, selecting just the relevant Nodes can solve <span class="No-Break">this problem.</span></li>
				<li><strong class="bold">Creates semantic units of data centered around specific information</strong>: This can make it easier to work with and analyze the data, as it is organized into smaller, more <span class="No-Break">focused units.</span></li>
				<li><strong class="bold">Allows the creation of relationships between Nodes</strong>: This means that Nodes can be linked together based on their relationships, creating a network of interconnected data. This can be useful for understanding the connections and dependencies between different pieces of information within <span class="No-Break">the Documents.</span></li>
			</ul>
			<p>Take a look at <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.3</em> for a visual representation of <span class="No-Break">this concept:</span></p>
			<div>
				<div id="_idContainer030" class="IMG---Figure">
					<img src="image/B21861_03_3.jpg" alt="Figure 3.3 – Relationships between Nodes extracted from a Document" width="1076" height="249" data-type="figure" id="untitled_figure_15" title2="– Relationships between Nodes extracted from a Document" no2="3.3">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – Relationships between Nodes extracted from a Document</p>
			<p>In LlamaIndex, Nodes<a id="_idIndexMarker109"></a> can also store images but we won’t focus on that functionality<a id="_idIndexMarker110"></a> in this book. Our main protagonist from now on will be the <span class="No-Break"><strong class="source-inline">TextNode</strong></span><span class="No-Break"> class.</span></p>
			<p>Here’s a list <a id="_idIndexMarker111"></a>of some important attributes of the <span class="No-Break"><strong class="source-inline">TextNode</strong></span><span class="No-Break"> class:</span></p>
			<ul>
				<li><strong class="source-inline">text</strong>: The chunk of text derived from an <span class="No-Break">original Document.</span></li>
				<li><strong class="source-inline">start_char_idx</strong> and <strong class="source-inline">end_char_idx</strong> are optional integer values that can store the starting and ending character positions of the text within the Document. This could be helpful when the text is part of a larger Document, and you need to pinpoint the <span class="No-Break">exact location.</span></li>
				<li><strong class="source-inline">text_template</strong> and <strong class="source-inline">metadata_template</strong> are template fields that define how the text and metadata are formatted. They help produce a more structured and readable representation <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">TextNode</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">metadata_seperator</strong>: This is a string field that defines the separator between metadata fields. When multiple metadata items are included, this separator is used to maintain readability <span class="No-Break">and structure.</span></li>
				<li>Any useful <strong class="source-inline">metadata</strong> such as the parent Document ID, relationships to other Nodes, and optional tags. This metadata can be used for storing additional context when necessary. We’ll talk about it in more detail in <a href="#_idTextAnchor068"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Ingesting Data into Our </em><span class="No-Break"><em class="italic">RAG Workflow</em></span><span class="No-Break">.</span></li>
			</ul>
			<p>Just like in the case of Documents, if you want to see a full list of the <strong class="source-inline">TextNode</strong> attributes, you can find them described on the LlamaIndex GitHub <span class="No-Break">repository: </span><a href="https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/schema.py" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/schema.py</span></a><span class="No-Break">.</span></p>
			<p>You should know<a id="_idIndexMarker112"></a> that the Nodes will automatically inherit any <a id="_idIndexMarker113"></a>metadata already present at the Document level but their metadata can also be <span class="No-Break">individually customized.</span></p>
			<p>There are several ways in which Nodes can be created in LlamaIndex, which we will discuss in upcoming subsections. Let’s start with the manual creation <span class="No-Break">of Nodes.</span></p>
			<h3 id="f_6__idParaDest-52" data-type="sect2" class="sect2" title2="Manually creating the Node objects" no2="3.2.3"><a id="_idTextAnchor051"></a>3.2.3. Manually creating the Node objects</h3>
			<p>Here is a simple example of how <a id="_idIndexMarker114"></a>we can manually create <span class="No-Break"><strong class="source-inline">Node</strong></span><span class="No-Break"> objects:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_15" title2="(no caption)" no2="">from llama_index.core import Document
from llama_index.core.schema import TextNode
doc = Document(text="This is a sample document text")
n1 = TextNode(text=doc.text[0:16], doc_id=doc.id_)
n2 = TextNode(text=doc.text[17:30], doc_id=doc.id_)
print(n1)
print(n2)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In this example, we’re using the text-slicing capabilities of Python to manually extract the text for the two Nodes. This manual approach can be very handy when you really want to have full control of both the text of the Nodes and the <span class="No-Break">accompanying metadata.</span></p>
			<p>To understand what’s happening backstage, let’s have a look at the output of <span class="No-Break">this code:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_16" title2="(no caption)" no2="">Node ID: 102b570f-5b22-48b5-b9b6-6378597e920d
Text: This is a sample
Node ID: 0ad81b09-bf12-4063-bfe4-6c5fd3c36cd4
Text: document text</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p class="callout-heading">Note</p>
			<p class="callout">As you can see, the two Nodes contain a randomly generated ID and the segments of text that we have sliced from the original Document. The <strong class="source-inline">TextNode</strong> constructor automatically generated an ID for each node using the Python UUID module. But we can customize that identifier after creating the Nodes if we want to employ a different <span class="No-Break">identification scheme.</span></p>
			<h3 id="f_6__idParaDest-53" data-type="sect2" class="sect2" title2="Automatically extracting Nodes from Documents using splitters" no2="3.2.4"><a id="_idTextAnchor052"></a>3.2.4. Automatically extracting Nodes from Documents using splitters</h3>
			<p>Because <strong class="bold">Document chunking</strong> is <a id="_idIndexMarker115"></a>very important in an RAG workflow, LlamaIndex comes<a id="_idIndexMarker116"></a> with built-in tools for this <a id="_idIndexMarker117"></a>purpose. One such<a id="_idIndexMarker118"></a> tool <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">TokenTextSplitter</strong></span><span class="No-Break">.</span></p>
			<p>As an example of how we can automatically generate Nodes, <strong class="source-inline">TokenTextSplitter</strong> attempts to split the<a id="_idIndexMarker119"></a> Document text into chunks that contain whole sentences. Each chunk will include one or more sentences and there’s also a default overlap between the chunks to maintain <span class="No-Break">more context.</span></p>
			<p>Under the hood, there are a number of parameters that we can customize on <strong class="source-inline">SimpleNodeParser</strong> such as <strong class="source-inline">chunk_size</strong> and <strong class="source-inline">chunk_overlap</strong> but we will talk about them more and how this text splitter works in the next chapter. For now, let’s have a look at a simple example of how to use <strong class="source-inline">TokenTextSplitter</strong> with its default settings on a <span class="No-Break"><strong class="source-inline">Document</strong></span><span class="No-Break"> object:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_17" title2="(no caption)" no2="">from llama_index.core import Document
from llama_index.core.node_parser import TokenTextSplitter
doc = Document(
&nbsp;&nbsp;&nbsp;&nbsp;text=(
&nbsp;&nbsp;&nbsp;&nbsp;"This is sentence 1. This is sentence 2. "
&nbsp;&nbsp;&nbsp;&nbsp;"Sentence 3 here."
&nbsp;&nbsp;&nbsp;&nbsp;),
&nbsp;&nbsp;&nbsp;&nbsp;metadata={"author": "John Smith"}
)
splitter = TokenTextSplitter(
&nbsp;&nbsp;&nbsp;&nbsp;chunk_size=12,
&nbsp;&nbsp;&nbsp;&nbsp;chunk_overlap=0,
&nbsp;&nbsp;&nbsp;&nbsp;separator=" "
)
nodes = splitter.get_nodes_from_documents([doc])
for node in nodes:
&nbsp;&nbsp;&nbsp;&nbsp;print(node.text)
&nbsp;&nbsp;&nbsp;&nbsp;print(node.metadata)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Here<a id="_idIndexMarker120"></a> is the<a id="_idIndexMarker121"></a> code<a id="_idIndexMarker122"></a> output <span class="No-Break">this time:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_18" title2="(no caption)" no2="">Metadata length (6) is close to chunk size (12). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.
This is sentence 1.
{'author': 'John Smith'}
This is sentence 2.
{'author': 'John Smith'}
Sentence 3 here.
{'author': 'John Smith'}</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p class="callout-heading">Note</p>
			<p class="callout">Given that chunk size is how much content can be processed at a time, if the metadata is too large, it will take up most of the space in each chunk, leaving less room for the actual content text. This can lead to chunks that are mostly metadata with very little actual content. In our example, the warning is triggered because the effective chunk size (the chunk size minus the space taken up by the metadata) results in chunks that would be less than 50 tokens. This is considered too small for <span class="No-Break">efficient processing.</span></p>
			<p>This <a id="_idIndexMarker123"></a>was just a basic example meant to <a id="_idIndexMarker124"></a>illustrate an automatic method for <a id="_idIndexMarker125"></a>chunking the data in separated Nodes. If you look at the metadata of each node, you’ll also notice that it was automatically inherited from the <span class="No-Break">originating Document.</span></p>
			<p class="callout-heading">Are there any other ways to create Nodes?</p>
			<p class="callout">Yes, there are a few other methods. In the next chapter, we’ll go more in-depth with the text-splitting and node-parsing techniques available in LlamaIndex. You will also have the opportunity to understand how they work under the hood and what kind of customization options <span class="No-Break">they provide.</span></p>
			<p>But wait, there’s more to understand <span class="No-Break">about Nodes.</span></p>
			<h3 id="f_6__idParaDest-54" data-type="sect2" class="sect2" title2="Nodes don’t like to be alone – they crave relationships" no2="3.2.5"><a id="_idTextAnchor053"></a>3.2.5. Nodes don’t like to be alone – they crave relationships</h3>
			<p>Now that<a id="_idIndexMarker126"></a> we’ve covered some basic examples of how to create<a id="_idIndexMarker127"></a> simple Nodes, how about adding some relationships <span class="No-Break">between them?</span></p>
			<p>Here’s an example that manually creates a simple relationship between <span class="No-Break">two Nodes:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_19" title2="(no caption)" no2="">from llama_index.core import Document
from llama_index.core.schema import (
&nbsp;&nbsp;&nbsp;&nbsp;TextNode,
&nbsp;&nbsp;&nbsp;&nbsp;NodeRelationship,
&nbsp;&nbsp;&nbsp;&nbsp;RelatedNodeInfo
)
doc = Document(text="First sentence. Second Sentence")
n1 = TextNode(text="First sentence", node_id=doc.doc_id)
n2 = TextNode(text="Second sentence", node_id=doc.doc_id)
n1.relationships[NodeRelationship.NEXT] = n2.node_id
n2.relationships[NodeRelationship.PREVIOUS] = n1.node_id
print(n1.relationships)
print(n2.relationships)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In this example, we’ve <a id="_idIndexMarker128"></a>manually created two Nodes and defined a <strong class="bold">previous</strong> or <strong class="bold">next</strong> relationship <a id="_idIndexMarker129"></a>between them. The relationship tracks the order of Nodes within the original Document. This code tells LlamaIndex that the two Nodes belong to the initial Document and they also come in a <span class="No-Break">particular order.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.4</em> shows exactly what LlamaIndex understands now after we ran <span class="No-Break">the code:</span></p>
			<div>
				<div id="_idContainer031" class="IMG---Figure">
					<img src="image/B21861_03_4.jpg" alt="Figure 3.4 – Previous or next relationship between two Nodes" width="1080" height="286" data-type="figure" id="untitled_figure_16" title2="– Previous or next relationship between two Nodes" no2="3.4">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – Previous or next relationship between two Nodes</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You should know that LlamaIndex contains the necessary tools to <em class="italic">automatically</em> create relationships between the Nodes. For example, when using the automated node parsers discussed previously, in their default configuration, LlamaIndex will automatically create previous or next relationships between the Nodes <span class="No-Break">it generates.</span></p>
			<p>There are other types of relationships that we could define. In addition to simple relationships such as previous or next, Nodes can be connected using <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="source-inline">SOURCE</strong>: The <strong class="bold">source relationship</strong> represents<a id="_idIndexMarker130"></a> the original source Document that a node was extracted or parsed from. When you parse a Document into multiple Nodes, you can track which Document each node originated from using the <span class="No-Break">source relationship.</span></li>
				<li><strong class="source-inline">PARENT</strong>: The <strong class="bold">parent relationship</strong> indicates a hierarchical structure where the node with this relationship is one level <a id="_idIndexMarker131"></a>higher than the associated node. In a tree structure, a parent node would have one or more children. This relationship is used to navigate or manage nested data structures where you might have a main node and subordinate Nodes representing sections, paragraphs, or other subdivisions of the <span class="No-Break">main node.</span></li>
				<li><strong class="source-inline">CHILD</strong>: This is the opposite of <strong class="source-inline">PARENT</strong>. A node with the <strong class="bold">child relationship</strong> is a subordinate of another node – the <a id="_idIndexMarker132"></a>parent. Child Nodes can be <a id="_idIndexMarker133"></a>seen as the leaves or branches in <a id="_idIndexMarker134"></a>a tree structure stemming from their <span class="No-Break">parent node.</span></li>
			</ul>
			<p>But why are relationships important? Let’s discuss why they <span class="No-Break">are useful.</span></p>
			<h3 id="f_6__idParaDest-55" data-type="sect2" class="sect2" title2="Why are relationships important?" no2="3.2.6"><a id="_idTextAnchor054"></a>3.2.6. Why are relationships important?</h3>
			<p>Creating <a id="_idIndexMarker135"></a>relationships between Nodes in LlamaIndex can be useful for <span class="No-Break">several reasons:</span></p>
			<ul>
				<li><strong class="bold">Enables more contextual querying</strong>: By linking Nodes together, you can leverage their relationships during querying to retrieve additional relevant context. For example, when querying a node, you could also return the previous or next Nodes to provide <span class="No-Break">more context.</span></li>
				<li><strong class="bold">Allows tracking provenance</strong>: Relationships encode provenance – where source Nodes originated and how they are connected. This is useful when you need to identify the original source of a node <span class="No-Break">for example.</span></li>
				<li><strong class="bold">Enables navigation through nodes</strong>: Traversing Nodes by their relationships enables new types of queries. For example, finding the next node that contains some keyword. Navigation along relationships provides another dimension <span class="No-Break">for searching.</span></li>
				<li><strong class="bold">Supports the construction of knowledge graphs</strong>: Nodes and relationships are the building blocks of knowledge graphs. Linking Nodes into a graph structure allows for constructing knowledge graphs from text using LlamaIndex. We’ll talk more about knowledge graphs during <a href="#_idTextAnchor104"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Indexing </em><span class="No-Break"><em class="italic">with LlamaIndex.</em></span></li>
				<li><strong class="bold">Improves the index structure</strong>: Some LlamaIndex indexes, such as trees and graphs, utilize node relationships to build their internal structure. Relationships allow the construction of more complex and expressive index topologies. We will discuss this more in <a href="#_idTextAnchor104"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Indexing </em><span class="No-Break"><em class="italic">with LlamaIndex.</em></span></li>
			</ul>
			<p>In summary, relationships <a id="_idIndexMarker136"></a>augment the Nodes with additional contextual connections. This supports more expressive querying, source-tracking knowledge graph construction, and complex <span class="No-Break">index structures.</span></p>
			<p>With raw data ingested as Documents and structured into Nodes that can be queried, the last step is to organize Nodes into <span class="No-Break">efficient indexes.</span></p>
			<h3 id="f_6__idParaDest-56" data-type="sect2" class="sect2" title2="Indexes" no2="3.2.7"><a id="_idTextAnchor055"></a>3.2.7. Indexes</h3>
			<p>Our third important concept – the <a id="_idIndexMarker137"></a>index – refers to a specific data structure used to organize a <a id="_idIndexMarker138"></a>collection of Nodes for optimized storage <span class="No-Break">and retrieval.</span></p>
			<p class="callout-heading">A simplified analogy</p>
			<p class="callout">Getting your data into shape for RAG is kind of like getting your clothes ready for a big trip – you have to make sure everything is organized and accessible! Let’s say you’re packing for an important business trip. You could just throw everything into your suitcase, but your shirts, socks, pants, and other stuff would get mixed up! The problem is that when you want to grab what you need quickly, you may pull out the wrong item and end up inventing an entirely new <span class="No-Break">dress code.</span></p>
			<p>That’s exactly why indexing your data is so crucial when prepping for LLM augmentation. Without indexing, your data is a messy pile of disorganized facts and files, and it’s like digging through a bursting suitcase for a matching pair <span class="No-Break">of socks.</span></p>
			<p>Proper indexing neatly sorts information into categories that make sense. For example, our sales records are in one index, and support tickets in another. It’s just like packing related items together. This transforms messy data into neatly organized knowledge that AI can make use of. You go from randomly hunting through a suitcase to grabbing exactly what you need from <span class="No-Break">custom pockets.</span></p>
			<p>So, remember – to avoid frustration and wasted time down the road, put in the work early to index and structure your data. It will make your job much easier down <span class="No-Break">the line.</span></p>
			<p>LlamaIndex supports different types of<a id="_idIndexMarker139"></a> indexes, each with its strengths and trade-offs. Here is a list of some of the <a id="_idIndexMarker140"></a>available <span class="No-Break">index types:</span></p>
			<ul>
				<li><strong class="source-inline">SummaryIndex</strong>: This is <a id="_idIndexMarker141"></a>very similar to a box for recipes – it <a id="_idIndexMarker142"></a>keeps your Nodes in order, so you can access them one by one. It takes in a set of documents, chunks them up into Nodes, and then concatenates them into a list. It’s great for reading through a <span class="No-Break">big Document.</span></li>
				<li><strong class="source-inline">DocumentSummaryIndex</strong>: This constructs a concise summary for each document, mapping these<a id="_idIndexMarker143"></a> summaries back to <a id="_idIndexMarker144"></a>their respective nodes. It facilitates efficient information retrieval by using these summaries to quickly identify <span class="No-Break">relevant documents.</span></li>
				<li><strong class="source-inline">VectorStoreIndex</strong>: This is one of the more sophisticated types of indexes and probably the workhorse <a id="_idIndexMarker145"></a>in most RAG applications. It converts <a id="_idIndexMarker146"></a>text into vector embeddings and uses math to group similar Nodes, helping locate Nodes that <span class="No-Break">are alike.</span></li>
				<li><strong class="source-inline">TreeIndex</strong>: The <a id="_idIndexMarker147"></a>perfect solution for those who love order. This index <a id="_idIndexMarker148"></a>behaves similarly to putting smaller boxes inside bigger ones, organizing Nodes by levels in a tree-like structure. Inside, each parent node stores summaries of the children nodes. These are generated by the LLM, using a general summarization prompt. This particular index can be very useful <span class="No-Break">for summarization.</span></li>
				<li><strong class="source-inline">KeywordTableIndex</strong>: Imagine you need to find a dish by the ingredients you have. The <a id="_idIndexMarker149"></a>keyword index <a id="_idIndexMarker150"></a>connects important words to the Nodes they’re in. It makes finding any node easy by looking <span class="No-Break">up keywords.</span></li>
				<li><strong class="source-inline">KnowledgeGraphIndex</strong>: This is useful when you need to link facts in a big network of data stored as a<a id="_idIndexMarker151"></a> knowledge graph. This <a id="_idIndexMarker152"></a>one is good for answering tricky questions about lots of <span class="No-Break">connected information.</span></li>
				<li><strong class="source-inline">ComposableGraph</strong>: This allows you to<a id="_idIndexMarker153"></a> create complex index <a id="_idIndexMarker154"></a>structures in which Document-level indexes are indexed in higher-level collections. That’s right: you can even build an index of indexes if you want to access the data from multiple Documents in a larger collection <span class="No-Break">of Documents.</span></li>
			</ul>
			<p>We’ll talk more about the inner workings of these<a id="_idIndexMarker155"></a> indexes and other variations in <a href="#_idTextAnchor104"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Indexing with LlamaIndex</em>. This is just an overview of <span class="No-Break">the topic.</span></p>
			<p>All the index types<a id="_idIndexMarker156"></a> in LlamaIndex share some<a id="_idIndexMarker157"></a> common <span class="No-Break">core features:</span></p>
			<ul>
				<li><strong class="bold">Building the index</strong>: Each index type can be constructed by passing in a set of Nodes during initialization. This builds the underlying <span class="No-Break">index structure.</span></li>
				<li><strong class="bold">Inserting new Nodes</strong>: After an index is built, new Nodes can be manually inserted. This adds to the existing <span class="No-Break">index structure.</span></li>
				<li><strong class="bold">Querying the index</strong>: Once built, indexes provide a query interface to retrieve relevant Nodes based on a specific query. The retrieval logic varies by <span class="No-Break">index type.</span></li>
			</ul>
			<p>The specifics of index structure and querying differ across index types. But this building, inserting, and querying pattern is consistent. Understanding the particular features of each index type is really important if you want to exploit their full potential. During <a href="#_idTextAnchor104"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Indexing with LlamaIndex</em>, we will cover this topic in much more detail and I will give you specific examples for each type <span class="No-Break">of index.</span></p>
			<p>For now, let’s consider a simple example to illustrate the creation <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">SummaryIndex</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_20" title2="(no caption)" no2="">from llama_index.core import SummaryIndex, Document
from llama_index.core.schema import TextNode
nodes = [
&nbsp;&nbsp;&nbsp;&nbsp;TextNode(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text="Lionel Messi is a football player from Argentina."
&nbsp;&nbsp;&nbsp;&nbsp;),
&nbsp;&nbsp;&nbsp;&nbsp;TextNode(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text="He has won the Ballon d'Or trophy 7 times."
&nbsp;&nbsp;&nbsp;&nbsp;),
&nbsp;&nbsp;&nbsp;&nbsp;TextNode(text="Lionel Messi's hometown is Rosario."),
&nbsp;&nbsp;&nbsp;&nbsp;TextNode(text="He was born on June 24, 1987.")
]
index = SummaryIndex(nodes)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This is very <a id="_idIndexMarker158"></a>simple to follow. We first defined a set of Nodes containing the data and then created <strong class="source-inline">SummaryIndex</strong> based on these Nodes. This index is a simple list-based <span class="No-Break">data structure.</span></p>
			<p>Think of <strong class="source-inline">SummaryIndex</strong> as a<a id="_idIndexMarker159"></a> little notepad where you jot down points from lots of stories. When it’s getting set up, it takes a big bunch of stories, breaks them into smaller bits, and lines them up in a list. The best part? LlamaIndex doesn’t even need to use the LLM when it builds this type <span class="No-Break">of index.</span></p>
			<h3 id="f_6__idParaDest-57" data-type="sect2" class="sect2" title2="Are we there yet?" no2="3.2.8"><a id="_idTextAnchor056"></a>3.2.8. Are we there yet?</h3>
			<p>Almost. Indexes are<a id="_idIndexMarker160"></a> great for<a id="_idIndexMarker161"></a> organizing <a id="_idIndexMarker162"></a>data, but how do we get answers from them? That’s where <strong class="bold">retrievers</strong> and <strong class="bold">response synthesizers</strong> <span class="No-Break">come in!</span></p>
			<p>Let’s use the Lionel Messi index we just created as an example. Say you ask, “What is Messi’s hometown?” See <span class="No-Break">the following:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_21" title2="(no caption)" no2="">query_engine = index.as_query_engine()
response = query_engine.query("What is Messi's hometown?")
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This is <span class="No-Break">the output:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_22" title2="(no caption)" no2="">Messi's hometown is Rosario.</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The summary<a id="_idIndexMarker163"></a> index organizes all Nodes sequentially in <span class="No-Break">a list.</span></p>
			<p>When queried, it retrieves all Nodes, allowing the synthesis of a response with <span class="No-Break">full context.</span></p>
			<h3 id="f_6__idParaDest-58" data-type="sect2" class="sect2" title2="How does this actually work under the hood?" no2="3.2.9"><a id="_idTextAnchor057"></a>3.2.9. How does this actually work under the hood?</h3>
			<p><strong class="source-inline">QueryEngine</strong> contains<a id="_idIndexMarker164"></a> a retriever, which is responsible for retrieving relevant Nodes from the index for the query. The retriever does a lookup to fetch and rank relevant Nodes from the index for that query. It grabs Nodes from the index that are likely to contain information about <span class="No-Break">Messi’s hometown.</span></p>
			<p>But just getting back a list of Nodes isn’t very useful. Another part of <strong class="source-inline">QueryEngine</strong> called<a id="_idIndexMarker165"></a> the <strong class="bold">node postprocessor</strong> comes into play at this point. This part enables the transformation, re-ranking, or filtering of Nodes after they’ve been retrieved and before the final response is crafted. There are many types of postprocessors available, and each can be configured and customized depending on the <span class="No-Break">use case.</span></p>
			<p>The <strong class="source-inline">QueryEngine</strong> object also contains a response synthesizer, which takes the retrieved Nodes and crafts the final response using the LLM by performing the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>The response synthesizer takes the Nodes selected by the retriever and processed by the node postprocessor and formats them into an <span class="No-Break">LLM prompt.</span></li>
				<li>The prompt contains the query along with context from <span class="No-Break">the Nodes.</span></li>
				<li>This prompt is given to the LLM to generate <span class="No-Break">a response.</span></li>
				<li>Any necessary postprocessing is done on the raw response using the LLM to return the final natural <span class="No-Break">language answer.</span></li>
			</ol>
			<p>So, <strong class="source-inline">index.as_query_engine()</strong> is creating <a id="_idIndexMarker166"></a>a full query engine for us, containing a default version of the three elements: retriever, node postprocessor, and <span class="No-Break">response synthesizer.</span></p>
			<p>We’ll get into a lot more detail on these three elements in <em class="italic">Chapters 6</em> <span class="No-Break">and </span><span class="No-Break"><em class="italic">7</em></span><span class="No-Break">.</span></p>
			<p>The final result of running this engine will be a natural language answer such as <strong class="source-inline">Messi's hometown </strong><span class="No-Break"><strong class="source-inline">is Rosario</strong></span><span class="No-Break">.</span></p>
			<p class="callout-heading">Remember</p>
			<p class="callout">This is just a basic example using a particular type of index called <strong class="source-inline">SummaryIndex</strong>. Each index type behaves differently as we will discuss in <a href="#_idTextAnchor104"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>. For example: a <strong class="source-inline">TreeIndex</strong> arranges Nodes in a hierarchy, allowing for summarization and a <strong class="source-inline">KeywordIndex</strong> maps keywords for fast lookup. The index structure impacts performance and determines its best use cases. By itself, the index structure defines the data management logic. As we have seen, the index needs to be combined with a retriever, postprocessor, and response synthesizer to form a complete query pipeline, allowing applications to leverage the <span class="No-Break">indexed data.</span></p>
			<p>More details will be added in the upcoming chapters. But, at this point, you should have a high-level idea of Indexes and <span class="No-Break">their role.</span></p>
			<p>Let’s have a look at <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.5</em> for an overview of the <span class="No-Break">complete flow.</span></p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/B21861_03_5.jpg" alt="Figure 3.5 – The complete RAG workflow with LlamaIndex" width="1107" height="679" data-type="figure" id="untitled_figure_17" title2="– The complete RAG workflow with LlamaIndex" no2="3.5">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – The complete RAG workflow with LlamaIndex</p>
			<p>As shown in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.5</em>, the process <a id="_idIndexMarker167"></a>involves the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Loading data <span class="No-Break">as Documents</span></li>
				<li>Parsing Documents into <span class="No-Break">coherent Nodes</span></li>
				<li>Building an optimized index <span class="No-Break">from Nodes</span></li>
				<li>Running queries over the index to retrieve <span class="No-Break">relevant Nodes</span></li>
				<li>Synthesizing the <span class="No-Break">final response</span></li>
			</ol>
			<p>Too much to remember? Let’s recap the building blocks <span class="No-Break">of LlamaIndex.</span></p>
			<h3 id="f_6__idParaDest-59" data-type="sect2" class="sect2" title2="A quick recap of the key concepts" no2="3.2.10"><a id="_idTextAnchor058"></a>3.2.10. A quick recap of the key concepts</h3>
			<p>Here is a quick rundown of what we have covered <span class="No-Break">so far:</span></p>
			<ul>
				<li><strong class="bold">Documents</strong>: The<a id="_idIndexMarker168"></a> raw <span class="No-Break">data ingested</span></li>
				<li><strong class="bold">Nodes</strong>: Logical <a id="_idIndexMarker169"></a>chunks extracted <span class="No-Break">from Documents</span></li>
				<li><strong class="bold">Indexes</strong>: Data <a id="_idIndexMarker170"></a>structures organizing Nodes based on <span class="No-Break">use case</span></li>
				<li><strong class="bold">QueryEngine</strong>: This <a id="_idIndexMarker171"></a>contains a retriever, node postprocessor, and <span class="No-Break">response synthesizer</span></li>
			</ul>
			<p>Understanding these building blocks is crucial for working with LlamaIndex. They allow you to effectively structure and connect external data <span class="No-Break">to LLMs.</span></p>
			<p>Now, you have a conceptual foundation. Next, let’s solidify this knowledge by looking at a simplified workflow model and building an <span class="No-Break">actual application.</span></p>
			<h2 id="f_6__idParaDest-60" data-type="sect1" class="sect1" title2="Building our first interactive, augmented LLM application" no2="3.3"><a id="_idTextAnchor059"></a>3.3. Building our first interactive, augmented LLM application</h2>
			<p>It’s time to connect the dots <a id="_idIndexMarker172"></a>and do something practical with all this knowledge. If we put all the previous code together, we can now build our first <span class="No-Break">LlamaIndex application.</span></p>
			<p>For this next step, make sure you’ve already taken care of the technical requirements mentioned at the beginning of the chapter. For the following code example, we’ll need the Wikipedia package to be able to parse a certain Wikipedia article and extract our sample data <span class="No-Break">from there.</span></p>
			<p>Once the Wikipedia package has been successfully installed, the sample app should run without issues. Here is <span class="No-Break">the code:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_23" title2="(no caption)" no2="">from llama_index.core import Document, SummaryIndex
from llama_index.core.node_parser import SimpleNodeParser
from llama_index.readers.wikipedia import WikipediaReader
loader = WikipediaReader()
documents = loader.load_data(pages=["Messi Lionel"])
parser = SimpleNodeParser.from_defaults()
nodes = parser.get_nodes_from_documents(documents)
index = SummaryIndex(nodes)
query_engine = index.as_query_engine()
print("Ask me anything about Lionel Messi!")
while True:
&nbsp;&nbsp;&nbsp;&nbsp;question = input("Your question: ")
&nbsp;&nbsp;&nbsp;&nbsp;if question.lower() == "exit":
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;break
&nbsp;&nbsp;&nbsp;&nbsp;response = query_engine.query(question)
&nbsp;&nbsp;&nbsp;&nbsp;print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>It should be noted that this<a id="_idIndexMarker173"></a> does not function as a genuine chat system because it does not retain the context of the conversation. It could be more accurately described as a simple <span class="No-Break">Q&amp;A system.</span></p>
			<p>Here’s a quick walk-through for <span class="No-Break">the code:</span></p>
			<ol>
				<li>We start by loading a Wikipedia page on Lionel Messi as a Document using the <strong class="source-inline">WikipediaReader</strong> data loader. This ingests the raw <span class="No-Break">text data</span></li>
				<li>Next, we parse the Document into smaller Node chunks using <strong class="source-inline">SimpleNodeParser</strong>. This splits the text into <span class="No-Break">logical segments</span></li>
				<li>We then build <strong class="source-inline">SummaryIndex</strong> from the Nodes. This organizes the Nodes sequentially for full <span class="No-Break">context retrieval</span></li>
				<li>We define <strong class="source-inline">QueryEngine</strong>, forming a complete <span class="No-Break">query pipeline</span></li>
				<li>Finally, we create a loop that queries the index, passing our question to <strong class="source-inline">QueryEngine</strong>. This handles retrieving relevant Nodes, prompting the LLM, and returning the <span class="No-Break">final response</span></li>
			</ol>
			<p>Again, you can have a look at <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.5</em> to visualize the overall workflow – ingesting data, parsing it into Nodes, building an index, and querying it to retrieve and synthesize the <span class="No-Break">final answer.</span></p>
			<p>But what if we want to know exactly what happens behind <span class="No-Break">the scenes?</span></p>
			<h3 id="f_6__idParaDest-61" data-type="sect2" class="sect2" title2="Using the logging features of LlamaIndex to understand the logic and debug our applications" no2="3.3.1"><a id="_idTextAnchor060"></a>3.3.1. Using the logging features of LlamaIndex to understand the logic and debug our applications</h3>
			<p>When you run code<a id="_idIndexMarker174"></a> like in our previous example, you might feel like there’s some <em class="italic">magic</em> happening behind the scenes. You pass in some text, call a simple indexing method, and boom – you can start querying an AI assistant powered by your <span class="No-Break">own data.</span></p>
			<p>But as your applications get more complex, you’ll want to understand exactly how LlamaIndex is doing its thing under the hood. This is where <strong class="bold">logging</strong> becomes important. LlamaIndex provides<a id="_idIndexMarker175"></a> tons of helpful log statements that show you step-by-step what’s going on during indexing and querying. It’s like having a little debug narrator describing <span class="No-Break">each action.</span></p>
			<p>Enabling basic logging is as simple as adding <span class="No-Break">this code:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_24" title2="(no caption)" no2="">import logging
logging.basicConfig(level=logging.DEBUG)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>With debug logging enabled, you’ll see how LlamaIndex does things, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li>Parses your Documents <span class="No-Break">into Nodes</span></li>
				<li>Decide which indexing structure <span class="No-Break">to use</span></li>
				<li>Formats prompts for <span class="No-Break">the LLM</span></li>
				<li>Retrieves relevant Nodes based on <span class="No-Break">your queries</span></li>
				<li>Synthesizes a response from <span class="No-Break">the Nodes</span></li>
			</ul>
			<p>As we’ll see in the next chapters, logging also reveals useful data such as <span class="No-Break">the following:</span></p>
			<ul>
				<li>The number of tokens used for <span class="No-Break">API calls</span></li>
				<li><span class="No-Break">Latency information</span></li>
				<li>Any warnings <span class="No-Break">or errors</span></li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">When things aren’t working as expected, don’t panic! Just check the logs. They provide crucial clues for identifying issues. For now, using the basic logging feature should do fine. With this feature enabled, most of the backstage activities will now be displayed during run time so you’ll be able to monitor the flow of your app step by step. We’ll talk more about advanced debugging during <a href="#_idTextAnchor197"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Customizing and Deploying Our </em><span class="No-Break"><em class="italic">LlamaIndex Project</em></span><span class="No-Break">.</span></p>
			<p>Now, how about <span class="No-Break">some tweaking?</span></p>
			<h3 id="f_6__idParaDest-62" data-type="sect2" class="sect2" title2="Customizing the LLM used by LlamaIndex" no2="3.3.2"><a id="_idTextAnchor061"></a>3.3.2. Customizing the LLM used by LlamaIndex</h3>
			<p>Let’s say we <a id="_idIndexMarker176"></a>would like to configure the framework to use another LLM. By default, LlamaIndex uses the OpenAI API with<a id="_idIndexMarker177"></a> the <strong class="bold">GPT-3.5-Turbo</strong> model. Here’s an overview <a id="_idIndexMarker178"></a>of the key features <span class="No-Break">of GPT-3.5-Turbo:</span></p>
			<ul>
				<li>It’s faster and cheaper to run compared <a id="_idIndexMarker179"></a><span class="No-Break">to </span><span class="No-Break"><strong class="bold">GPT-4</strong></span></li>
				<li>While not as advanced as other models, such as GPT-4, it’s still a very capable generative and <span class="No-Break">conversational model</span></li>
				<li>It can perform very well on a variety <a id="_idIndexMarker180"></a>of <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) tasks such as classification, summarization, <span class="No-Break">or translation</span></li>
			</ul>
			<p>You can see why the creators of LlamaIndex have chosen this model. All things considered, it provides a good balance of performance and cost for most use cases. For most applications, it’s probably sufficient. As you have seen already if you’ve tested the application, it handles the questions about Lionel Messi <span class="No-Break">pretty well.</span></p>
			<p>But what if we need to customize that for a more specific case? Let’s say we need the best possible performance of GPT-4, the larger context provided<a id="_idIndexMarker181"></a> by <strong class="bold">Claude-2</strong>, or maybe we want to use an open-source AI for <span class="No-Break">our purposes.</span></p>
			<h3 id="f_6__idParaDest-63" data-type="sect2" class="sect2" title2="Easy as 1-2-3" no2="3.3.3"><a id="_idTextAnchor062"></a>3.3.3. Easy as 1-2-3</h3>
			<p>We <a id="_idIndexMarker182"></a>only need to add three lines of code at the beginning of <span class="No-Break">our app:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_25" title2="(no caption)" no2="">from llama_index.llms.openai import OpenAI
from llama_index.core.settings import Settings
Settings.llm = OpenAI(temperature=0.8, model="gpt-4")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Make sure you add the <strong class="source-inline">Settings.llm</strong> line immediately after your imports so that it applies to all the other operations. Here’s the explanation for <span class="No-Break">each step:</span></p>
			<ol>
				<li>The first line imports the OpenAI class from <strong class="source-inline">llama_index.llms.openai</strong> so that we can use it to initialize an <span class="No-Break">OpenAI LLM</span></li>
				<li>The second import is responsible for the <strong class="source-inline">Settings</strong> class. We’ll use it to customize <span class="No-Break">the LLM</span></li>
				<li>Next, we configure <strong class="source-inline">Settings</strong> with an OpenAI LLM instance using the GPT-4 model and set the <strong class="source-inline">temperature</strong> to <strong class="source-inline">0.8</strong>, overriding the <span class="No-Break">default LLM</span></li>
			</ol>
			<p>We just configured LlamaIndex to use GPT-4 for all operations instead of the default GPT-3.5-Turbo model. The next part of the code will build an index and run a simple query<a id="_idIndexMarker183"></a> using the newly <span class="No-Break">configured LLM:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_26" title2="(no caption)" no2="">from llama_index.core.schema import TextNode
from llama_index.core import SummaryIndex
nodes = [
&nbsp;&nbsp;&nbsp;&nbsp;TextNode(text="Lionel Messi's hometown is Rosario."),
&nbsp;&nbsp;&nbsp;&nbsp;TextNode(text="He was born on June 24, 1987.")
]
index = SummaryIndex(nodes)
query_engine = index.as_query_engine()
response = query_engine.query(
&nbsp;&nbsp;&nbsp;&nbsp;"What is Messi's hometown?"
)
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Next, we need to talk about the <span class="No-Break"><strong class="source-inline">temperature</strong></span><span class="No-Break"> parameter.</span></p>
			<h3 id="f_6__idParaDest-64" data-type="sect2" class="sect2" title2="The temperature parameter" no2="3.3.4"><a id="_idTextAnchor063"></a>3.3.4. The temperature parameter</h3>
			<p>On OpenAI models such as <a id="_idIndexMarker184"></a>GPT-3.5 and GPT-4, this parameter controls the randomness and creativity of the AI’s responses. Check out <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.6</em> for <span class="No-Break">an overview:</span></p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/B21861_03_6.jpg" alt="Figure 3.6 – Effect of temperature on output variability" width="959" height="500" data-type="figure" id="untitled_figure_18" title2="– Effect of temperature on output variability" no2="3.6">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6 – Effect of temperature on output variability</p>
			<p>The <strong class="source-inline">temperature</strong> values for the OpenAI models range from <strong class="source-inline">0</strong> to <strong class="source-inline">2</strong>. Higher values produce more random, creative output. Lower values produce more focused, <span class="No-Break">deterministic output.</span></p>
			<p>A <strong class="source-inline">temperature</strong> value of <strong class="source-inline">0</strong> will produce almost the same output every time for the same input prompt. You noticed that I’ve used the word “almost.” That is because even with the <strong class="source-inline">0</strong> setting, most models will probably still produce slight answer variations given the same prompt. This is caused by inherent randomness in the model’s initialization or subtle variations in the model’s internal state that can occur due to factors such as floating-point precision limitations or the stochastic nature of certain operations within the neural network. Even with a <strong class="source-inline">temperature</strong> value of <strong class="source-inline">0</strong>, which aims to minimize randomness, these small variations can lead to slightly different outputs for <span class="No-Break">identical inputs.</span></p>
			<p>Setting the right <strong class="source-inline">temperature</strong> depends on your use case – whether you want responses strongly based on factual data or more imaginative ones. For code generation or data analysis tasks, a <strong class="source-inline">temperature</strong> value of <strong class="source-inline">0.2</strong> would be appropriate, while more creativity-focused tasks such as writing or chatbot responses would benefit from a setting of <strong class="source-inline">0.5</strong> <span class="No-Break">and higher.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">If you have a use case that really requires consistent responses for multiple iterations using the same prompt, here’s some practical advice. In my experimental research, I have achieved the most consistent results using the GPT-3.5-Turbo-1106 model with a <strong class="source-inline">temperature</strong> value <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">0</strong></span><span class="No-Break">.</span></p>
			<p>Apart from <strong class="source-inline">temperature</strong>, there are several other parameters you can tune by passing them as a dictionary to the <strong class="source-inline">additional_kwargs</strong> argument. If you plan on using OpenAI models in your RAG workflow, I advise you to familiarize yourself with these LLM settings, as they can be very important in an RAG scenario. Apart from <strong class="source-inline">temperature</strong>, the <strong class="source-inline">top_p</strong> and <strong class="source-inline">seed</strong> parameters are particularly useful as they can be leveraged to control the randomness of the outputs. For a detailed list, you can consult the official OpenAI documentation <span class="No-Break">here: </span><a href="https://platform.openai.com/docs/models" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://platform.openai.com/docs/models</span></a><span class="No-Break">.</span></p>
			<p>Here’s a simple <a id="_idIndexMarker185"></a>playground that you could use for experimenting with different <span class="No-Break">LLM settings:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_27" title2="(no caption)" no2="">from llama_index.llms.openai import OpenAI
llm = OpenAI(
&nbsp;&nbsp;&nbsp;&nbsp;model="gpt-3.5-turbo-1106",
&nbsp;&nbsp;&nbsp;&nbsp;temperature=0.2,
&nbsp;&nbsp;&nbsp;&nbsp;max_tokens=50,
&nbsp;&nbsp;&nbsp;&nbsp;additional_kwargs={
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"seed": 12345678,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"top_p": 0.5
&nbsp;&nbsp;&nbsp;&nbsp;}
)
response = llm.complete(
&nbsp;&nbsp;&nbsp;&nbsp;"Explain the concept of gravity in one sentence"
)
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Using the previous code, you can experiment with different settings, examining the output and finding the best configuration for your particular <span class="No-Break">use case.</span></p>
			<p>If you are wondering what different LLMs available right now can do for your RAG purposes, here is a side-by-side comparison extracted from the LlamaIndex documentation. This list <a id="_idIndexMarker186"></a>was built by the LlamaIndex community by testing various <span class="No-Break">LLMs: </span><a href="https://docs.llamaindex.ai/en/stable/module_guides/models/llms.html" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/stable/module_guides/models/llms.html</span></a><span class="No-Break">.</span></p>
			<h3 id="f_6__idParaDest-65" data-type="sect2" class="sect2" title2="Understanding how Settings can be used for customization" no2="3.3.5"><a id="_idTextAnchor064"></a>3.3.5. Understanding how Settings can be used for customization</h3>
			<p>You’ve <a id="_idIndexMarker187"></a>probably noticed that I have used something called <strong class="source-inline">Settings</strong> to customize the AI model in the previous section. A brief explanation is <span class="No-Break">in order.</span></p>
			<p><strong class="source-inline">Settings</strong> is a key component in LlamaIndex that allows you to customize and configure the <em class="italic">elements</em> used during indexing and querying. It contains common objects needed across LlamaIndex such as <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="source-inline">LLM</strong>: This allows for the overriding of the default LLM with a custom one as we’ve seen in the <span class="No-Break">previous example</span></li>
				<li><strong class="source-inline">Embedding model</strong>: This is used for generating vectors for text to enable semantic search. These vectors are <a id="_idIndexMarker188"></a>called <strong class="bold">embeddings</strong> and we’ll talk about them in much more detail during <a href="#_idTextAnchor104"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Indexing </em><span class="No-Break"><em class="italic">with LlamaIndex</em></span></li>
				<li><strong class="source-inline">NodeParser</strong>: This is used for setting the default <span class="No-Break">node parser</span></li>
				<li><strong class="source-inline">CallbackManager</strong>: This handles callbacks for events within LlamaIndex. As we will see later, this is used for debugging and tracing <span class="No-Break">our apps</span></li>
			</ul>
			<p>There are also other parameters that can be tweaked in <strong class="source-inline">Settings</strong>. We’ll dive much deeper into different customization options during <a href="#_idTextAnchor197"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Customizing and Deploying Our LlamaIndex Project</em>. Regardless of what you want to change, the customization <a id="_idIndexMarker189"></a>will be done like in the previous example. Once you’ve defined your custom <strong class="source-inline">Settings</strong>, all subsequent operations will use <span class="No-Break">this configuration.</span></p>
			<p>OK. We’ve covered enough concepts for one chapter. How about <span class="No-Break">some coding?</span></p>
			<h2 id="f_6__idParaDest-66" data-type="sect1" class="sect1" title2="Starting our PITS project – hands-on exercise" no2="3.4"><a id="_idTextAnchor065"></a>3.4. Starting our PITS project – hands-on exercise</h2>
			<p>Are <a id="_idIndexMarker190"></a>you ready for a bit of hands-on practice? It’s time to start building our PITS project. We have enough theoretical groundwork laid out and, in this chapter, we’ll begin the preparation for the more advanced elements <span class="No-Break">to come.</span></p>
			<p>I’ve tried to build the project in a modular structure. I believe it helps a lot with code clarity and will enable us to go through some of the important concepts from LlamaIndex one by one. As I mentioned in the previous chapter, you can either write the code alongside reading the book or download and study it in full using the GitHub repository that I’ve made available <span class="No-Break">to you.</span></p>
			<p class="callout-heading">Disclaimer</p>
			<p class="callout">There are many aspects that can be improved in the existing code base, and quite a few features are missing from it for PITS to be considered a production-ready application. For example, in my implementation, there is no authentication and the application is a single user. Also, to keep the code short, I’ve not dealt much with error handling. But, of course, these are not bugs but features. This way, you can continue the story of PITS, adding the missing elements and transforming it into a commercial-grade application. <span class="No-Break">Why not?</span></p>
			<p>Before we start, I’d like to briefly explain the code structure that will underpin our application. Here’s a list of Python source code files used by our PITS along with brief descriptions <span class="No-Break">for each:</span></p>
			<ul>
				<li><strong class="source-inline">app.py</strong>: The main entry point for the Streamlit app. This handles the initialization of the application and manages the navigation between different screens based on the <span class="No-Break">application logic</span></li>
				<li><strong class="source-inline">document_uploader.py</strong>: This interfaces with LlamaIndex to ingest and index <span class="No-Break">uploaded Documents</span></li>
				<li><strong class="source-inline">training_material_builder.py</strong>: This constructs the learning materials (slides and narration) based on the user’s current knowledge. It utilizes uploaded and <a id="_idIndexMarker191"></a>indexed materials to generate the <span class="No-Break">learning content</span></li>
				<li><strong class="source-inline">training_interface.py</strong>: This is where the actual teaching will take place. It displays the slides and the tutor narration together with the conversational side panel for <span class="No-Break">user interactions</span></li>
				<li><strong class="source-inline">quiz_builder.py</strong>: This generates quizzes based on the ingested materials and the user’s <span class="No-Break">current knowledge</span></li>
				<li><strong class="source-inline">quiz_interface.py</strong>: This administers quizzes and evaluates the user’s knowledge level depending on the results – what everyone hated in <span class="No-Break">high school</span></li>
				<li><strong class="source-inline">conversation_engine.py</strong>: This manages the conversational side panel, responding to user queries and providing explanations. It also keeps track of the context of conversations with the tutor to avoid repetition and ensure relevant assistance. It also retrieves summaries of previous discussions and ensures the tutor picks up where it <span class="No-Break">left off</span></li>
				<li><strong class="source-inline">storage_manager.py</strong>: This handles all file operations, such as saving and loading session states and user uploads. It manages local file storage and can be later adapted for cloud <span class="No-Break">storage solutions</span></li>
				<li><strong class="source-inline">session_functions.py</strong>: This handle storing and retrieving session information locally – and eventually in <span class="No-Break">the cloud</span></li>
				<li><strong class="source-inline">logging_functions.py</strong>: This handles the logging of all user interactions with the app. Writes descriptive log statements with timestamps to track the user’s actions throughout the app. Stores and retrieves application logs locally – and eventually in <span class="No-Break">the cloud</span></li>
				<li><strong class="source-inline">global_settings.py</strong>: This contains application settings, configurations, and eventually <a id="_idIndexMarker192"></a>Streamlit’s secrets for deployment. It centralizes parameters for easy management <span class="No-Break">and updates</span></li>
				<li><strong class="source-inline">user_onboarding.py</strong>: This module takes care of the user <span class="No-Break">onboarding steps</span></li>
				<li><strong class="source-inline">index_builder.py</strong>: This module builds the indexes used throughout <span class="No-Break">the application</span></li>
			</ul>
			<p>Keep in mind that, currently, the application is designed to run locally. During <a href="#_idTextAnchor197"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Customizing and Deploying Our LlamaIndex Project</em>, we will discuss the deployment options available with Streamlit apps in more detail. Before continuing, make sure you have installed the second package mentioned at the beginning of the chapter – the YAML package <span class="No-Break">for Python.</span></p>
			<p>This one will be required by PITS for its <strong class="source-inline">session_functions</strong> module. I will explain it in a <span class="No-Break">few moments.</span></p>
			<p>To install it, use the <span class="No-Break">following code:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_28" title2="(no caption)" no2="">pip install pyyaml</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>For now, we <a id="_idIndexMarker193"></a>will focus on three of the <span class="No-Break">PITS modules:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">global_settings.py</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">session_functions.py</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">logging_functions.py</strong></span></li>
			</ul>
			<h3 id="f_6__idParaDest-67" data-type="sect2" class="sect2" title2="Let’s have a look at the source code" no2="3.4.1"><a id="_idTextAnchor066"></a>3.4.1. Let’s have a look at the source code</h3>
			<p>We will first start with <a id="_idIndexMarker194"></a>the global settings <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">global_settings.py</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_29" title2="(no caption)" no2="">LOG_FILE = "session_data/user_actions.log"
SESSION_FILE = "session_data/user_session_state.yaml"
CACHE_FILE = "cache/pipeline_cache.json"
CONVERSATION_FILE = "cache/chat_history.json"
QUIZ_FILE = "cache/quiz.csv"
SLIDES_FILE = "cache/slides.json"
STORAGE_PATH = "ingestion_storage/"
INDEX_STORAGE = "index_storage"
QUIZ_SIZE = 5
ITEMS_ON_SLIDE = 4</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This is where we will store our global configurations. We’ll use the different parameters here to customize the experience of PITS and adjust some of its <span class="No-Break">internal settings.</span></p>
			<p>For now, the only two <a id="_idIndexMarker195"></a>parameters I would like to emphasize are <strong class="source-inline">LOG_FILE</strong> and <strong class="source-inline">SESSION_FILE</strong>. They are used to define the storage location for “our log file” and session-related data. The <strong class="source-inline">log</strong> file will be used to remember all user interactions and maintain conversational context. The <strong class="source-inline">session</strong> file will allow resuming existing sessions while maintaining the <span class="No-Break">session state.</span></p>
			<p>Now, let’s move on <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">session_functions.py</strong></span><span class="No-Break">.</span></p>
			<p>The <strong class="source-inline">session_functions.py</strong> module contains functions that handle the saving, loading, and deleting of a user’s <span class="No-Break">session state:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_30" title2="(no caption)" no2="">from global_settings import SESSION_FILE
import yaml
import os
def save_session(state):
&nbsp;&nbsp;&nbsp;&nbsp;state_to_save = {key: value for key, value in state.items()}
&nbsp;&nbsp;&nbsp;&nbsp;with open(SESSION_FILE, 'w') as file:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;yaml.dump(state_to_save, file)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The <strong class="source-inline">save_session</strong> function takes the current state as an argument, which includes all the necessary information about the user’s session and writes it to a file named <strong class="source-inline">SESSION_FILE</strong>. The state is converted into YAML format before saving, which ensures that it can be easily <span class="No-Break">reloaded later.</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_31" title2="(no caption)" no2="">def load_session(state):
&nbsp;&nbsp;&nbsp;&nbsp;if os.path.exists(SESSION_FILE):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with open(SESSION_FILE, 'r') as file:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;try:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loaded_state = yaml.safe_load(file) or {}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for key, value in loaded_state.items():
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;state[key] = value
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return True
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;except yaml.YAMLError:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return False
&nbsp;&nbsp;&nbsp;&nbsp;return False</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This function attempts <a id="_idIndexMarker196"></a>to read <strong class="source-inline">SESSION_FILE</strong>, if it exists, and loads the stored session data into the provided state object. If the file is read successfully and the YAML content is correctly parsed, it returns <strong class="source-inline">True</strong>, indicating that the session state has been restored. Otherwise, it <span class="No-Break">returns </span><span class="No-Break"><strong class="source-inline">False</strong></span><span class="No-Break">.</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_32" title2="(no caption)" no2="">def delete_session(state):
&nbsp;&nbsp;&nbsp;&nbsp;if os.path.exists(SESSION_FILE):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;os.remove(SESSION_FILE)
&nbsp;&nbsp;&nbsp;&nbsp;for key in list(state.keys()):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;del state[key]</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>When a session needs to be cleared, this function deletes <strong class="source-inline">SESSION_FILE</strong> and removes all the keys from the passed state object, effectively resetting <span class="No-Break">the session.</span></p>
			<p class="callout-heading">Why YAML?</p>
			<p class="callout">I’ve used YAML as the format for serialization instead of Streamlit own persistence format because it’s human readable and platform independent. YAML works well with hierarchical data structures, making it easy to read and edit outside of the application if necessary. It allows the session state to be stored in a structured, standard format that can easily be transferred or modified as needed. YAML is often used for configuration files, but it’s also suitable for storing simple data structures such as, in our case, the <span class="No-Break">session state.</span></p>
			<p>We also need to create <strong class="source-inline">logging_functions.py</strong>. Here is <span class="No-Break">the code:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_33" title2="(no caption)" no2="">from datetime import datetime
from global_settings import LOG_FILE
import os
def log_action(action, action_type):
&nbsp;&nbsp;&nbsp;&nbsp;timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
&nbsp;&nbsp;&nbsp;&nbsp;log_entry = f"{timestamp}: {action_type} : {action}\n"
&nbsp;&nbsp;&nbsp;&nbsp;with open(LOG_FILE, 'a') as file:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;file.write(log_entry)
def reset_log():
&nbsp;&nbsp;&nbsp;&nbsp;with open(LOG_FILE, 'w') as file:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;file.truncate(0)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The <strong class="source-inline">logging_functions.py</strong> module is responsible for recording events, user actions, and <a id="_idIndexMarker197"></a>other significant occurrences during the execution of the application into a log file. I’ve designed it to keep track of user actions and system events mainly to provide context for the PITS agent during its interactions with the user but also for monitoring and <span class="No-Break">debugging purposes.</span></p>
			<p>Here’s what the<a id="_idIndexMarker198"></a> functions in the <span class="No-Break">module do:</span></p>
			<ul>
				<li><strong class="source-inline">log_action(action, action_type)</strong>: This function records an action or event. It accepts two arguments: <strong class="source-inline">action</strong>, which is a string describing what occurred, and <strong class="source-inline">action_type</strong>, which categorizes the action. The function gets the current <strong class="source-inline">timestamp</strong>, formats it with the action and type, and appends this entry to <strong class="source-inline">LOG_FILE</strong>. This helps maintain a chronological record of actions <span class="No-Break">and events</span></li>
				<li><strong class="source-inline">reset_log()</strong>: In the current implementation, when the users return to an existing session, they have the option to start a new one. When that happens, we clear the log file to avoid collecting too much data. This function opens <strong class="source-inline">LOG_FILE</strong> and truncates its content, effectively deleting all the logged entries. This is usually not a common thing to do in production environments, as logs are valuable for historical data analysis, but in our case, it simplifies <span class="No-Break">the flow</span></li>
			</ul>
			<p>I know I’ve <a id="_idIndexMarker199"></a>promised we’ll have fun writing the code for PITS, and I am perfectly aware that logging seems less <em class="italic">ha-ha</em> and more <em class="italic">ho-hum</em>, but trust me, there’s no fun if you can’t debug your app. We needed to lay the foundations here and we’ll continue with the rest of the modules in the <span class="No-Break">next chapters.</span></p>
			<h2 id="f_6__idParaDest-68" data-type="sect1" class="sect1" title2="Summary" no2="3.5"><a id="_idTextAnchor067"></a>3.5. Summary</h2>
			<p>This chapter covered foundational concepts such as Documents, Nodes, and indexes – the core building blocks of LlamaIndex. I’ve demonstrated a simple workflow to load data as Documents, parse it into coherent Nodes using parsers, build an optimized index from the Nodes, and then query the index to retrieve relevant Nodes and synthesize <span class="No-Break">a response.</span></p>
			<p>The logging features of LlamaIndex were introduced as an important tool for understanding the underlying logic and debugging applications. Logs reveal how LlamaIndex parses, indexes, prompts the LLM, retrieves Nodes, and synthesizes responses. Customizing the LLM and other services used by LlamaIndex was shown using the <span class="No-Break"><strong class="source-inline">Settings</strong></span><span class="No-Break"> class.</span></p>
			<p>We’ve also started to build our PITS tutoring application, laying the groundwork with session management and logging functions. This modular structure will enable the exploration of LlamaIndex’s capabilities incrementally as the app is <span class="No-Break">built up.</span></p>
			<p>With the foundational knowledge established, it’s time to move on to more advanced LlamaIndex features. The <span class="No-Break">journey continues!</span></p>
		</div>
<div id="f_7__idContainer039" data-type="chapter" class="chapter" file="B21861_04_xhtml" title2="Ingesting Data into Our RAG Workflow" no2="4">
			<h1 id="f_7__idParaDest-69" class="chapter-number"><a id="_idTextAnchor068"></a>4</h1>
			<h1 id="f_7__idParaDest-70"><a id="_idTextAnchor069"></a>Ingesting Data into Our RAG Workflow</h1>
			<p>We’ve taken a good look at the overall structure of LlamaIndex from afar. It’s now time to get much closer and understand the small details of this framework. It’s bound to get more technical but also more intriguing as we <span class="No-Break">go further.</span></p>
			<p>Ready to go deeper down the rabbit hole? <span class="No-Break">Follow me!</span></p>
			<p>In this chapter, we will learn about <span class="No-Break">the following:</span></p>
			<ul>
				<li>Using the LlamaHub connectors to ingest <span class="No-Break">our data</span></li>
				<li>Taking advantage of the many text-chunking tools <span class="No-Break">in LlamaIndex</span></li>
				<li>Infusing our nodes with metadata <span class="No-Break">and relationships</span></li>
				<li>Keeping our data private and our <span class="No-Break">budget safe</span></li>
				<li>Creating ingestion pipelines for better efficiency and <span class="No-Break">lower costs</span></li>
			</ul>
			<h2 id="f_7__idParaDest-71" data-type="sect1" class="sect1" title2="Technical requirements" no2="4.1"><a id="_idTextAnchor070"></a>4.1. Technical requirements</h2>
			<p>You will need to install the following Python libraries in your environment to be able to run the examples included in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">LangChain</strong></span><span class="No-Break">: </span><a href="https://www.langchain.com/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://www.langchain.com/</span></a></li>
				<li><span class="No-Break"><strong class="bold">Py-Tree-Sitter</strong></span><span class="No-Break">: </span><a href="https://pypi.org/project/tree-sitter/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pypi.org/project/tree-sitter/</span></a></li>
			</ul>
			<p>In addition, several LlamaIndex Integration packages will <span class="No-Break">be required:</span></p>
			<ul>
				<li><strong class="bold">Entity </strong><span class="No-Break"><strong class="bold">extractor</strong></span><span class="No-Break">: </span><a href="https://pypi.org/project/llama-index-extractors-entity/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pypi.org/project/llama-index-extractors-entity/</span></a></li>
				<li><strong class="bold">Hugging Face </strong><span class="No-Break"><strong class="bold">LLMs</strong></span><span class="No-Break">: </span><a href="https://pypi.org/project/llama-index-llms-huggingface/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pypi.org/project/llama-index-llms-huggingface/</span></a></li>
				<li><strong class="bold">Database </strong><span class="No-Break"><strong class="bold">reader</strong></span><span class="No-Break">: </span><a href="https://pypi.org/project/llama-index-readers-database/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pypi.org/project/llama-index-readers-database/</span></a></li>
				<li><strong class="bold">Web </strong><span class="No-Break"><strong class="bold">reader</strong></span><span class="No-Break">: </span><a href="https://pypi.org/project/llama-index-readers-web/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pypi.org/project/llama-index-readers-web/</span></a></li>
			</ul>
			<p>All the code examples in this chapter can be found in the <em class="italic">ch4</em> subfolder of this book’s GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex</span></a><span class="No-Break">.</span></p>
			<h2 id="f_7__idParaDest-72" data-type="sect1" class="sect1" title2="Ingesting data via LlamaHub" no2="4.2"><a id="_idTextAnchor071"></a>4.2. Ingesting data via LlamaHub</h2>
			<p>As we saw in <a href="#_idTextAnchor045"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Kickstarting Your Journey with LlamaIndex</em>, one of the first steps in a RAG workflow is to ingest and process our proprietary data. We already discovered <a id="_idIndexMarker200"></a>the concepts of documents and nodes, which are used to organize the data and prepare it for indexing. I’ve also briefly introduced <a id="_idIndexMarker201"></a>the LlamaHub data loaders as a way to easily ingest data into LlamaIndex. It’s time to examine these steps in more detail and gradually learn how to infuse LLM applications with our own, proprietary knowledge. Before we continue, though, I’d like to emphasize some very common challenges encountered at <span class="No-Break">this step:</span></p>
			<ol>
				<li>No matter how effective our RAG pipeline is, at the end of the day, the quality of the final result will largely depend on the quality of the initial data. To overcome this challenge, make sure you start by cleaning up your data first. Eliminate potential duplicates and errors. While not exactly duplicates, redundant information can also clutter your knowledge base and confuse the RAG system. Be on the lookout for ambiguous, biased, incomplete, or outdated information. I’ve seen many cases of poorly structured and insufficiently maintained knowledge repositories that were completely useless for users looking for quick and accurate answers. Ask yourself this question: <em class="italic">If I were to manually search through this data, how easy would it be to find the information I need?</em> Before moving on with building the pipeline, do yourself a favor and prepare your data thoroughly until you’re satisfied with the answer to <span class="No-Break">that question.</span></li>
				<li>Our data is dynamic. An organizational knowledge repository is rarely a static, permanent data source. It evolves with the business, reflecting new insights, discoveries, and changes in the external environment. Recognizing this fluid nature is key to maintaining a relevant and effective system. To overcome this challenge, in a production RAG application, you’ll have to implement a systematic method for periodically reviewing and updating the content, ensuring that new information is incorporated and outdated or incorrect data <span class="No-Break">is removed.</span></li>
				<li>Data comes in many flavors, shapes, and sizes. Sometimes, it’s structured, sometimes not. A well-built RAG system should be able to properly ingest all kinds of <a id="_idIndexMarker202"></a>formats and document types. While LlamaIndex provides a huge number of data loaders for many different APIs, databases, and document types, building an automated ingestion system can still prove to <a id="_idIndexMarker203"></a>be challenging. To overcome this particular challenge, later in this section, we’ll cover <strong class="bold">LlamaParse</strong> – an innovative hosted service designed to automatically ingest and process data from different <span class="No-Break">data sources.</span></li>
			</ol>
			<p>Now that we know what kind of problems await along the way, let’s start our journey by first discussing the simplest ways of ingesting the data into the RAG pipeline – by using the available LlamaHub <span class="No-Break">data loaders.</span></p>
			<h2 id="f_7__idParaDest-73" data-type="sect1" class="sect1" title2="An overview of LlamaHub" no2="4.3"><a id="_idTextAnchor072"></a>4.3. An overview of LlamaHub</h2>
			<p>LlamaHub is an extensive library of integrations that augments the capabilities of the core framework. Among many <a id="_idIndexMarker204"></a>other types of integrations, LlamaHub <a id="_idIndexMarker205"></a>contains numerous <strong class="bold">connectors</strong> – also known as <strong class="bold">data readers</strong> or <strong class="bold">data loaders</strong> – specially built to allow seamless integration of external data with LlamaIndex. There are over 180 readily available data readers spanning a wide range of data <a id="_idIndexMarker206"></a>sources and formats, and the list is <span class="No-Break">constantly increasing.</span></p>
			<p>These connectors act as a standard way to ingest data, extracting data from sources such as databases, APIs, files, and websites and converting it into LlamaIndex <strong class="source-inline">Document</strong> objects. This relieves you from the burden of writing customized parsers and connectors for every new data source. But of course, if you’re not satisfied with the existing connectors, you can always build your own and contribute to <span class="No-Break">the collection.</span></p>
			<p>LlamaHub empowers you to tap into diverse data sources with just a few lines of code. The resulting Document objects can then be parsed into nodes and indexed as required by your application. The unified output as LlamaIndex <strong class="source-inline">Document</strong> objects means your core business <a id="_idIndexMarker207"></a>logic does not have to worry about handling various data types. The complexity is abstracted by <span class="No-Break">the framework.</span></p>
			<p class="callout-heading">Why do we need so many integrations?</p>
			<p class="callout">In <a href="#_idTextAnchor023"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, <em class="italic">LlamaIndex: The Hidden Jewel - An Introduction to the LlamaIndex Ecosystem</em>, in the <em class="italic">Familiarizing ourselves with the structure of the LlamaIndex code repository</em> section, I explained the motives behind the framework’s modular architecture. Because of this modular architecture, many RAG components provided by LlamaIndex are not included in the core elements that are installed together with the rest of the framework. This means that before using any data loader for the first time, we have to install the corresponding integration package. Once the package has been installed, we’ll be able to import the reader into our code and use its functionality. Some readers also utilize specialized libraries and tools tailored to each data type. For example, <strong class="source-inline">PDFReader</strong> leverages Camelot and Tika for parsing PDF content. <strong class="source-inline">AirbyteSalesforceReader</strong> uses the Salesforce API client, and so on. This allows us to efficiently adapt to the format and interface of each source but may require us to install additional packages in our <span class="No-Break">development environment.</span></p>
			<p>All available readers are listed on the LlamaHub website and usually come with detailed documentation and usage samples. Therefore, I’ll briefly cover just a few examples to give a general idea of how you can use them in <span class="No-Break">your applications.</span></p>
			<p>I strongly encourage you to take your time and go through the entire list of data readers when building your LlamaIndex apps instead of spending valuable time building one from scratch. Chances are you’ll just be reinventing <span class="No-Break">the wheel.</span></p>
			<p>If you’re looking to consult the source code for the readers, you’ll find them all included in the Llama-index GitHub repository, under the <strong class="source-inline">llama-index-integrations/readers</strong> <span class="No-Break">subfolder: </span><a href="https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/readers" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/readers</span></a><span class="No-Break">.</span></p>
			<p>The LlamaHub documentation for each data reader lists its installation requirements and usage guidance, so before trying to use them, make sure you also install any additional dependencies required by specific connectors you want <span class="No-Break">to use.</span></p>
			<h2 id="f_7__idParaDest-74" data-type="sect1" class="sect1" title2="Using the LlamaHub data loaders to ingest content" no2="4.4"><a id="_idTextAnchor073"></a>4.4. Using the LlamaHub data loaders to ingest content</h2>
			<p>Apart from the <em class="italic">Wikipedia</em> reader that we discussed in the previous chapter, to get a better <a id="_idIndexMarker208"></a>understanding of how data readers work, let’s look at a few more examples of LlamaHub readers that we can use to <span class="No-Break">ingest data.</span></p>
			<h3 id="f_7__idParaDest-75" data-type="sect2" class="sect2" title2="Ingesting data from a web page" no2="4.4.1"><a id="_idTextAnchor074"></a>4.4.1. Ingesting data from a web page</h3>
			<p><strong class="source-inline">SimpleWebPageReader</strong> can extract text content from <span class="No-Break">web pages.</span></p>
			<p>To use it, we <a id="_idIndexMarker209"></a>must first install the <span class="No-Break">corresponding integration:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_34" title2="(no caption)" no2="">pip install llama-index-readers-web</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Once installed, it’s really easy <span class="No-Break">to use:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_35" title2="(no caption)" no2="">from llama_index.readers.web import SimpleWebPageReader
urls = ["https://docs.llamaindex.ai"]
documents = SimpleWebPageReader().load_data(urls)
for doc in documents:
&nbsp;&nbsp;&nbsp;&nbsp;print(doc.text)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This loads and displays the text content of the specified web pages <span class="No-Break">into documents.</span></p>
			<p>At its core, <strong class="source-inline">SimpleWebPageReader</strong> serves as a bridge between the vast, unstructured world of the internet and the structured environment of the LlamaIndex RAG pipeline. To better understand its inner workings, let’s explore what happens under the hood when it extracts text content from <span class="No-Break">web pages.</span></p>
			<p>When loading the data, <strong class="source-inline">SimpleWebPageReader</strong> iterates over a list of URLs provided by the user. For each URL, it performs a web request to fetch the page content. The response, initially in HTML format, can be transformed into plain text if the <strong class="source-inline">html_to_text</strong> flag is set to <strong class="source-inline">True</strong>. This transformation strips away the HTML tags and converts the web page content into a more digestible text format. However, remember what I’ve said about external dependencies for these readers? In this case, the HTML-to-text conversion feature requires the <strong class="source-inline">html2text</strong> package, which has to be <span class="No-Break">installed first.</span></p>
			<p>Another significant aspect of this reader is its ability to attach metadata to the scraped documents. Through the <strong class="source-inline">metadata_fn</strong> parameter, we can pass a custom function that takes a URL as input and returns a dictionary of metadata. This flexibility allows for the enrichment of documents with additional information or any relevant tags that might be <a id="_idIndexMarker210"></a>useful in categorizing and understanding the context of the data better. Should the user provide a <strong class="source-inline">metadata_fn</strong> parameter, the reader then applies this function to the current URL to extract metadata, enriching the final <strong class="source-inline">Document</strong> object with this additional layer <span class="No-Break">of information.</span></p>
			<p class="callout-heading">A practical use case for the metadata_fn function</p>
			<p class="callout">We could, for example, use a function that simply returns the current date and time. That way, we could ingest the same URL at different moments and build a chronological timeline highlighting different versions of that page at various points in time. This could prove useful in scenarios such as browsing a code repository or answering questions about a developing <span class="No-Break">news story.</span></p>
			<p>Finally, each web page’s content, along with its URL and optionally added metadata, is encapsulated in a <strong class="source-inline">Document</strong> object. These objects are then collected into a list, providing a structured representation of the text content and metadata extracted from each <span class="No-Break">web page.</span></p>
			<p class="callout-heading">One thing to keep in mind</p>
			<p class="callout">As its name suggests, this reader is a simple tool. While it can be effective for reading simple web pages, for more advanced cases such as pages requiring interaction (for example, navigating a login process or handling JavaScript-rendered content), <strong class="source-inline">SimpleWebPageReader</strong> might not be sufficient. Websites that dynamically generate content based on user interactions or rely heavily on client-side scripting can pose challenges that this basic scraper is not designed <span class="No-Break">to handle.</span></p>
			<p>Through <strong class="source-inline">SimpleWebPageReader</strong>, the task of ingesting and structuring basic web content is simplified. The great thing about these readers is that they allow us to focus on building <a id="_idIndexMarker211"></a>and enhancing the logic of our RAG applications instead of spending precious time on building compatible ingestion tools for each type of data in our <span class="No-Break">knowledge base.</span></p>
			<h3 id="f_7__idParaDest-76" data-type="sect2" class="sect2" title2="Ingesting data from a database" no2="4.4.2"><a id="_idTextAnchor075"></a>4.4.2. Ingesting data from a database</h3>
			<p>Using databases is not only a common practice but also a highly efficient method for managing and retrieving structured information. Databases offer a robust platform for storing <a id="_idIndexMarker212"></a>a vast array of data types, from simple text to complex relationships between entities, making them an indispensable asset in <span class="No-Break">data management.</span></p>
			<p>The <strong class="source-inline">DatabaseReader</strong> connector allows querying many database systems. First, we need to install the necessary <span class="No-Break">integration package:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_36" title2="(no caption)" no2="">pip install llama-index-readers-database</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Here’s an example of how you can easily fetch the contents of an <span class="No-Break">SQLite database:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_37" title2="(no caption)" no2="">from llama_index.readers.database import DatabaseReader
reader = DatabaseReader(
&nbsp;&nbsp;&nbsp;&nbsp;uri="sqlite:///files/db/example.db"
)
query = "<strong class="bold">SELECT * FROM products</strong>"
documents = reader.load_data(query=query)
for doc in documents:
&nbsp;&nbsp;&nbsp;&nbsp;print(doc.text)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Under the hood, <strong class="source-inline">DatabaseReader</strong> connects to various databases to fetch data and transform it <a id="_idIndexMarker213"></a>into a format usable by the RAG pipeline. It supports connection through a <strong class="source-inline">SQLDatabase</strong> instance, a <strong class="bold">SQLAlchemy Engine</strong>, a connection URI, or a set of database credentials – provided through the <strong class="source-inline">scheme</strong>, <strong class="source-inline">host</strong>, <strong class="source-inline">port</strong>, <strong class="source-inline">user</strong>, <strong class="source-inline">password</strong>, and <strong class="source-inline">dbname</strong> arguments. Once set up, it executes a provided SQL query to retrieve data. After connecting to the database, the reader executes the provided <strong class="source-inline">query</strong>. The resulting rows are then converted into Document objects, with each row from the query result forming a single Document. The conversion process involves concatenating each column-value pair into a string, which is then assigned as the text of <span class="No-Break">a document.</span></p>
			<p>The example I have provided executes the SQL query against an SQLite database stored in the <strong class="source-inline">ch4/files/db</strong> folder, loads each returned row as a Document, and displays the results. You can find a more general example on the official project documentation <span class="No-Break">website: </span><a href="https://docs.llamaindex.ai/en/stable/examples/data_connectors/DatabaseReaderDemo.html" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/data_connectors/DatabaseReaderDemo.html</span></a><span class="No-Break">.</span></p>
			<p>Alright – I think you understand the workflow now. As you’ve probably noticed, the approach for using LlamaHub readers is very straightforward. In all the examples, first, we install <a id="_idIndexMarker214"></a>the required integration package, as described on LlamaHub, and then use it to import and load data from the reader. Apart from the examples I have provided, you’ll find a huge number of data readers available on LlamaHub. From Office documents, Gmail accounts, videos and images, YouTube videos, and RSS feeds to GitHub repositories and Discord chats, pretty much every popular data format <span class="No-Break">is supported.</span></p>
			<p>But apart from reading individual files using dedicated data readers, in the next section, we will also explore more efficient methods that can be used for ingesting multiple documents <span class="No-Break">at once.</span></p>
			<h3 id="f_7__idParaDest-77" data-type="sect2" class="sect2" title2="Bulk-ingesting data from sources with multiple file formats" no2="4.4.3"><a id="_idTextAnchor076"></a>4.4.3. Bulk-ingesting data from sources with multiple file formats</h3>
			<p>Loading data into LlamaIndex is a crucial first step. But sifting through the wide range of data <a id="_idIndexMarker215"></a>loaders in LlamaHub and figuring out how to configure each one can feel overwhelming early on. That’s why I’m going to show you two different methods that can greatly simplify and reduce the burden of data ingestion for your <span class="No-Break">RAG systems.</span></p>
			<p>We’ll start with the simple <span class="No-Break">method first.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Using SimpleDirectoryReader to ingest multiple data formats" no2="4.4.3.1">4.4.3.1. Using SimpleDirectoryReader to ingest multiple data formats</h4>
			<p>When <a id="_idIndexMarker216"></a>you just want to get started fast or have a simple use case, <strong class="source-inline">SimpleDirectoryReader</strong> comes to the rescue. Think of this reader as your trusty pocketknife for bulk data ingestion. It’s easy to use, requires minimal setup, and automatically adapts to different file types. To load data, you simply point the reader to a folder or list of files. Loading a folder <a id="_idIndexMarker217"></a>containing PDFs, Word docs, plain text files, and CSVs is very straightforward. Here’s <span class="No-Break">a demonstration:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_38" title2="(no caption)" no2="">from llama_index.core import SimpleDirectoryReader
reader = SimpleDirectoryReader(
&nbsp;&nbsp;&nbsp;&nbsp;input_dir="files",
&nbsp;&nbsp;&nbsp;&nbsp;recursive=True
)
documents = reader.load_data()
for doc in documents:
&nbsp;&nbsp;&nbsp;&nbsp;print(doc.metadata)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p class="callout-heading">Under the hood</p>
			<p class="callout"><strong class="source-inline">SimpleDirectoryReader</strong> has built-in methods to determine which reader works best for <a id="_idIndexMarker218"></a>each file type. You don’t need to worry about those details. It will automatically detect formats such as PDF, DOCX, CSV, plain text, and others based on the file extensions. Then, it chooses the best tool to extract the content into Document objects. For plain text files, it simply reads the text content. For binary files such as PDFs and Office docs, it uses libraries such as PyPDF and Pillow to extract <span class="No-Break">the text.</span></p>
			<p><strong class="source-inline">SimpleDirectoryReader</strong> effortlessly handles the different files and returns the parsed content as documents. By default, it only processes files in the directory’s top level. To include subdirectories, you can set the <strong class="source-inline">recursive</strong> parameter <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">True</strong></span><span class="No-Break">.</span></p>
			<p>You can also pass in a list of specific files to load, <span class="No-Break">like this:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_39" title2="(no caption)" no2="">files = ["<strong class="bold">file1.pdf</strong>", "<strong class="bold">file2.docx</strong>", "<strong class="bold">file3.txt</strong>"]
reader = SimpleDirectoryReader(files)
documents = reader.load_data()</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The result is a batch of Document objects ready for indexing in just a few lines of code. No <a id="_idIndexMarker219"></a>headaches setting up separate data readers for each file type. When you want quick and easy data ingestion without the complexity, let <strong class="source-inline">SimpleDirectoryReader</strong> handle the hard work! It’s versatile <span class="No-Break">and automated.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Parsing like a pro with the help of LlamaParse" no2="4.4.3.2">4.4.3.2. Parsing like a pro with the help of LlamaParse</h4>
			<p>While <strong class="source-inline">SimpleDirectoryReader</strong> is great for quick and easy data ingestion, sometimes, you need more advanced parsing capabilities, especially for complex file formats. Most of <a id="_idIndexMarker220"></a>the time, we have to deal with complex file structures containing a mix of data. For example, a PDF file may include images, charts, code snippets, mathematical formulas, and other elements alongside its text content. The naive readers included in the LlamaHub integration library will be overwhelmed by such cases. They would most probably fail to extract the entire content or – even worse – mess up the extracted data and complicate its <span class="No-Break">further processing.</span></p>
			<p>This is where <a id="_idIndexMarker221"></a>LlamaParse shines. Provided through the LlamaCloud enterprise platform (<a href="https://cloud.llamaindex.ai/parse" target="_blank" rel="noopener noreferrer">https://cloud.llamaindex.ai/parse</a>), this reader is implemented through a cutting-edge hosted service that integrates seamlessly with the other components of the framework. It uses multi-modal capabilities and LLM intelligence under the hood to provide industry-leading document parsing, including exceptional support for tricky formats such as PDFs containing tables, figures, <span class="No-Break">and equations.</span></p>
			<p>One of the standout features of <strong class="source-inline">LlamaParse</strong> is that it allows you to provide natural language instructions to guide the parsing by using the <strong class="source-inline">parsing_instruction </strong>parameter. Since you know your documents best, you can tell <strong class="source-inline">LlamaParse</strong> exactly what kind of output you need and how that information should be extracted from <span class="No-Break">the files.</span></p>
			<p class="callout-heading">For instance:</p>
			<p class="callout">When parsing a technical whitepaper, you could instruct it to extract all the section headings, ignore the footnotes, and output any code snippets in markdown format. <strong class="source-inline">LlamaParse</strong> will follow your instructions to parse the <span class="No-Break">document accurately.</span></p>
			<p>In addition to the instruction-guided parsing mode, <strong class="source-inline">LlamaParse</strong> also offers a JSON output mode that provides rich structured data about the parsed document, including marking tables, headings, extracting images, and more. Also, for bulk-ingesting entire folders in one go, <strong class="source-inline">LlamaParse</strong> can be used in combination with <strong class="source-inline">SimpleDirectoryReader</strong>, as you will see in the next example. This gives you full flexibility to build custom RAG applications over a complex collection of documents. You could also accomplish this manually by using specialized data readers for each file format in your <a id="_idIndexMarker222"></a>collection of data. However, using <strong class="source-inline">LlamaParse</strong> will greatly simplify this process, improve the overall quality, and save you a lot <span class="No-Break">of time.</span></p>
			<p><strong class="source-inline">LlamaParse</strong> supports a wide and expanding range of file types beyond just PDFs, including Word docs, PowerPoint, RTF, ePub, and many more. It offers a generous free tier to <span class="No-Break">get started.</span></p>
			<p>The necessary <strong class="source-inline">LlamaParse</strong> integration package should already be installed along with the LlamaIndex components, so no additional installation is required to run the code example in <span class="No-Break">this section.</span></p>
			<p>The next step is to create a free account on <a href="https://cloud.llamaindex.ai" target="_blank" rel="noopener noreferrer">https://cloud.llamaindex.ai</a> and obtain an API key. Once you have obtained the key, you can use it directly in your code, but for a more secure approach, I strongly encourage you to follow the same steps we followed in <a href="#_idTextAnchor023"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, <em class="italic">LlamaIndex: The Hidden Jewel - An Introduction to the LlamaIndex Ecosystem</em>, and add the key as a variable in your local environment under the name <strong class="source-inline">LLAMA_CLOUD_API_KEY</strong>. To demonstrate the capabilities of this tool, I’ve designed a sample PDF with a more complex structure, as can be seen in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/B21861_04_1.jpg" alt="Figure 4.1 – A sample PDF containing multiple articles, images, and tables" width="1650" height="840" data-type="figure" id="untitled_figure_19" title2="– A sample PDF containing multiple articles, images, and tables" no2="4.1">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – A sample PDF containing multiple articles, images, and tables</p>
			<p>Here’s a basic code example that uses <strong class="source-inline">LlamaParse</strong> to ingest <span class="No-Break">this PDF:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_40" title2="(no caption)" no2="">from llama_parse import LlamaParse
from llama_index.core import SimpleDirectoryReader
from llama_index.core import VectorStoreIndex</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The first part of the code imported the necessary modules. Next, we’ll configure <strong class="source-inline">LlamaParse</strong> and pass it to <strong class="source-inline">SimpleDirectoryReader</strong> as a <span class="No-Break"><strong class="source-inline">file_extractor</strong></span><span class="No-Break"> argument:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_41" title2="(no caption)" no2="">parser = LlamaParse(result_type="text")
file_extractor = {".pdf": parser}
reader = SimpleDirectoryReader(
&nbsp;&nbsp;&nbsp;&nbsp;"./files/pdf",
&nbsp;&nbsp;&nbsp;&nbsp;file_extractor=file_extractor
)
docs = reader.load_data()</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Once the PDF <a id="_idIndexMarker223"></a>content has been ingested into a new Document object, it’s time to build an index and run a query against <span class="No-Break">our data:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_42" title2="(no caption)" no2="">index = VectorStoreIndex.from_documents(docs)
qe = index.as_query_engine()
response = qe.query(
&nbsp;&nbsp;&nbsp;&nbsp;"<strong class="bold">List all large dog breeds mentioned in Table 2</strong> "
)
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The output of this script should be similar to <span class="No-Break">the following:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_43" title2="(no caption)" no2="">Started parsing the file under job_id &lt;…&gt;
<strong class="bold">German Shepherd</strong>, <strong class="bold">Golden Retriever</strong>, <strong class="bold">Labrador Retriever</strong></pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p class="callout-heading">Important note</p>
			<p class="callout">One important consideration with using a hosted service such as <strong class="source-inline">LlamaParse</strong> is data privacy. Before submitting your proprietary data through the API, be sure to carefully review their privacy policy to ensure it aligns with your data protection requirements. While the service offers powerful parsing capabilities, it’s crucial to safeguard <span class="No-Break">sensitive information.</span></p>
			<p>Keep in mind that this is a paid service. The great news, however, is that you can take advantage of their generous <strong class="bold">free tier</strong>. For higher volume needs, the current pricing can be found on the website. If you want to unlock the full potential of <strong class="source-inline">LlamaParse</strong> to build advanced <a id="_idIndexMarker224"></a>document retrieval systems or deploy it on your private cloud for maximal data security, that option is available <span class="No-Break">as well.</span></p>
			<p>For professional, production-ready applications, <strong class="source-inline">LlamaParse</strong> is a powerful tool that puts you in full control of parsing your data to maximize the quality of your knowledge base and <span class="No-Break">RAG applications.</span></p>
			<p>Now that we’ve got the data, let’s make it easier to handle by breaking it down into <span class="No-Break">smaller pieces.</span></p>
			<h2 id="f_7__idParaDest-78" data-type="sect1" class="sect1" title2="Parsing the documents into nodes" no2="4.5"><a id="_idTextAnchor077"></a>4.5. Parsing the documents into nodes</h2>
			<p>As we saw in <a href="#_idTextAnchor045"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Kickstarting Your Journey with LlamaIndex</em>, the next step is to split the documents into nodes. In many cases, documents tend to be very large, so we need to break <a id="_idIndexMarker225"></a>them down into smaller units called nodes. Working at this granular level allows for better handling of our content while maintaining an accurate <a id="_idIndexMarker226"></a>representation of its internal structure. This is the basic mechanism that LlamaIndex uses to manage our proprietary data content <span class="No-Break">more easily.</span></p>
			<p>Now is the <a id="_idIndexMarker227"></a>time to understand how nodes can be generated in LlamaIndex and what customization opportunities we have along the way. In the previous chapter, we talked about how to manually create nodes. But that was merely a way to simplify the explanation and help you better understand their mechanics. In a real application, most likely, we will want to use some automatic methods to generate them from the ingested documents. So, that’s what we’ll focus on <span class="No-Break">going forward.</span></p>
			<p>In this section, we will discover different ways of chunking a document. We’ll start by understanding <a id="_idIndexMarker228"></a>simple <strong class="bold">text splitters</strong> – which operate on raw text – and then we’ll cover <a id="_idIndexMarker229"></a>the more advanced <strong class="bold">node parsers</strong> – which are capable of interpreting more complex formats and following the document structure when extracting <span class="No-Break">the nodes.</span></p>
			<h3 id="f_7__idParaDest-79" data-type="sect2" class="sect2" title2="Understanding the simple text splitters" no2="4.5.1"><a id="_idTextAnchor078"></a>4.5.1. Understanding the simple text splitters</h3>
			<p><strong class="bold">Text splitters</strong> <em class="italic">break down</em> the document into smaller pieces operating at the raw text level. They are useful when the content has a <em class="italic">flat</em> structure and does not come in a <span class="No-Break">specific format.</span></p>
			<p>To run the <a id="_idIndexMarker230"></a>following examples, make sure you add the necessary imports and the document reading logic, using <strong class="source-inline">FlatReader</strong>, at the beginning of your code for <span class="No-Break">all examples:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_44" title2="(no caption)" no2="">from llama_index.core.node_parser import &lt;<strong class="bold">Splitter_Module</strong>&gt;
from llama_index.readers.file import FlatReader
from pathlib import Path
reader = FlatReader()
document = reader.load_data(Path(&lt;<strong class="bold">file_name</strong>&gt;))</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Also, if you want to see the actual nodes generated by the code, you can add something like this <em class="italic">after</em> running <span class="No-Break">the parsers:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_45" title2="(no caption)" no2="">for node in nodes:
&nbsp;&nbsp;&nbsp;&nbsp;print(f"Metadata {node.metadata} \nText: {node.text}")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Alright. Let’s see what’s in store in the <em class="italic">text </em><span class="No-Break"><em class="italic">splitter</em></span><span class="No-Break"> category.</span></p>
			<h4 data-type="sect3" class="sect3" title2="SentenceSplitter" no2="4.5.1.1">4.5.1.1. SentenceSplitter</h4>
			<p>This one splits text <a id="_idIndexMarker231"></a>while maintaining sentence boundaries, providing <a id="_idIndexMarker232"></a>nodes containing groups of sentences. You saw an example of using this parser in <a href="#_idTextAnchor045"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Kickstarting Your Journey with LlamaIndex</em>, in the <em class="italic">Automatically extracting nodes from documents using </em><span class="No-Break"><em class="italic">splitters</em></span><span class="No-Break"> section.</span></p>
			<h4 data-type="sect3" class="sect3" title2="TokenTextSplitter" no2="4.5.1.2">4.5.1.2. TokenTextSplitter</h4>
			<p>This splitter <a id="_idIndexMarker233"></a>breaks down text while respecting sentence <a id="_idIndexMarker234"></a>boundaries to create suitable nodes for further natural language processing. It operates at the <span class="No-Break">token level.</span></p>
			<p>A typical usage in code would look <span class="No-Break">like this:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_46" title2="(no caption)" no2="">splitter = TokenTextSplitter(
&nbsp;&nbsp;&nbsp;&nbsp;chunk_size = 70,
&nbsp;&nbsp;&nbsp;&nbsp;chunk_overlap = 2,
&nbsp;&nbsp;&nbsp;&nbsp;separator = " ",
&nbsp;&nbsp;&nbsp;&nbsp;backup_separators = [".", "!", "?"]
)
nodes = splitter.get_nodes_from_documents(document)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Here are some notes on the parameters of <span class="No-Break">this splitter:</span></p>
			<ul>
				<li><strong class="source-inline">chunk_size</strong>: This sets the maximum number of tokens for <span class="No-Break">each chunk</span></li>
				<li><strong class="source-inline">chunk_overlap</strong>: This defines the overlap in tokens between <span class="No-Break">consecutive chunks</span></li>
				<li><strong class="source-inline">separator</strong>: This is used to determine the primary <span class="No-Break">token boundary</span></li>
				<li><strong class="source-inline">backup_separators</strong>: These can be used for additional splitting points if the primary separator doesn’t split the <span class="No-Break">text sufficiently</span></li>
			</ul>
			<h4 data-type="sect3" class="sect3" title2="CodeSplitter" no2="4.5.1.3">4.5.1.3. CodeSplitter</h4>
			<p>This smart <a id="_idIndexMarker235"></a>splitter knows how to interpret source code. It splits text based on <a id="_idIndexMarker236"></a>programming language and is ideal for managing technical documentation or source code. Before running the example, make sure you install the <span class="No-Break">necessary libraries:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_47" title2="(no caption)" no2="">pip install tree_sitter
pip install tree_sitter_languages</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Let’s have a look at an example of how to use this splitter in <span class="No-Break">your code:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_48" title2="(no caption)" no2="">code_splitter = CodeSplitter.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;language = 'python',
&nbsp;&nbsp;&nbsp;&nbsp;chunk_lines = 5,
&nbsp;&nbsp;&nbsp;&nbsp;chunk_lines_overlap = 2,
&nbsp;&nbsp;&nbsp;&nbsp;max_chars = 150
)
nodes = code_splitter.get_nodes_from_documents(document)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>As you <a id="_idIndexMarker237"></a>can see, there are several parameters you can tune with <a id="_idIndexMarker238"></a><span class="No-Break">this splitter:</span></p>
			<ul>
				<li><strong class="source-inline">language</strong>: This specifies the language of <span class="No-Break">the code</span></li>
				<li><strong class="source-inline">chunk_lines</strong>: This defines the number of lines <span class="No-Break">per chunk</span></li>
				<li><strong class="source-inline">chunk_lines_overlap</strong>: This defines the lines overlap <span class="No-Break">between chunks</span></li>
				<li><strong class="source-inline">max_chars</strong>: This defines the maximum characters <span class="No-Break">per chunk</span></li>
			</ul>
			<p class="callout-heading">Quick side note on CodeSplitter</p>
			<p class="callout">This splitter is <a id="_idIndexMarker239"></a>cleverly built around a concept called <strong class="bold">abstract syntax tree</strong> (<strong class="bold">AST</strong>). An AST is a key idea in computer science that’s mainly used in creating programs that translate or interpret code. It’s like a branching diagram that shows the basic structure of the code written in a programming language. Each point on the diagram represents a different part or piece of the code. Because of this splitter’s awareness of AST, when you’re splitting code, you keep related statements together as much as possible, which is vital when you need to maintain the logical flow of code to understand or process <span class="No-Break">it later.</span></p>
			<h3 id="f_7__idParaDest-80" data-type="sect2" class="sect2" title2="Using more advanced node parsers" no2="4.5.2"><a id="_idTextAnchor079"></a>4.5.2. Using more advanced node parsers</h3>
			<p>Text splitters <a id="_idIndexMarker240"></a>only provide basic logic for breaking down text, mostly by using simple rules. We also have more advanced tools for chunking text into nodes. These are designed to process various standard file formats or can be used for more specific types <span class="No-Break">of content.</span></p>
			<p>Before we continue, keep in mind that all the node parsers that we will discuss here are derived from the generic class called <strong class="source-inline">NodeParser</strong>. Each parser has various parameters that can be <a id="_idIndexMarker241"></a>configured according to the use case, but at the base, three common elements can be customized for <span class="No-Break">all parsers:</span></p>
			<ul>
				<li><strong class="source-inline">include_metadata</strong>: This determines whether the parser should take into account the metadata or not. By default, this is set <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">True</strong></span></li>
				<li><strong class="source-inline">Include_prev_next_rel</strong>: This determines whether the parser should automatically include <strong class="bold">prev/next</strong> type relationships between nodes. Again, the default value <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">True</strong></span></li>
				<li><strong class="source-inline">Callback_manager</strong>: This can <a id="_idIndexMarker242"></a>be used to define a specific <strong class="bold">callback function</strong>. These functions can be used for debugging, tracing, and cost analysis, among other functions. We will talk more about them in <a href="#_idTextAnchor214"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Prompt Engineering Guidelines and </em><span class="No-Break"><em class="italic">Best Practices</em></span></li>
			</ul>
			<p>Apart from these three general options, each parser provides specific parameters to customize. You can get a complete list of configurable parameters for each parser by consulting the <span class="No-Break">official documentation.</span></p>
			<p>Let’s explore the node parsers available <span class="No-Break">in LlamaIndex.</span></p>
			<h4 data-type="sect3" class="sect3" title2="SentenceWindowNodeParser" no2="4.5.2.1">4.5.2.1. SentenceWindowNodeParser</h4>
			<p>Based on the simple <strong class="bold">SentenceSplitter</strong>, this parser <a id="_idIndexMarker243"></a>splits text into individual <a id="_idIndexMarker244"></a>sentences and also <a id="_idIndexMarker245"></a>includes a <em class="italic">window</em> of surrounding sentences in the metadata of each node. It is useful for building more context around each sentence. During the querying process, that context will be fed into the LLM and allow for better responses. We can use it <span class="No-Break">like this:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_49" title2="(no caption)" no2="">parser = SentenceWindowNodeParser.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;window_size=2,
&nbsp;&nbsp;&nbsp;&nbsp;window_metadata_key="text_window",
&nbsp;&nbsp;&nbsp;&nbsp;original_text_metadata_key="original_sentence"
)
nodes = parser.get_nodes_from_documents(document)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>For this <a id="_idIndexMarker246"></a>parser, three specific parameters <a id="_idIndexMarker247"></a>can <span class="No-Break">be customized:</span></p>
			<ul>
				<li><strong class="source-inline">Window_size</strong>: This defines the number of sentences on each side to include in <span class="No-Break">the window</span></li>
				<li><strong class="source-inline">window_metadata_key</strong>: This defines the metadata key for the <span class="No-Break">window sentences</span></li>
				<li><strong class="source-inline">original_text_metadata_key</strong>: This defines the metadata key for the <span class="No-Break">original sentence</span></li>
			</ul>
			<h4 data-type="sect3" class="sect3" title2="LangchainNodeParser" no2="4.5.2.2">4.5.2.2. LangchainNodeParser</h4>
			<p>If you prefer <a id="_idIndexMarker248"></a>using the LangChain splitters, this parser allows <a id="_idIndexMarker249"></a>using any text splitter from the Langchain collection, extending the parsing options offered <span class="No-Break">by LlamaIndex.</span></p>
			<p>As a prerequisite for the next example, you’ll have to install the <span class="No-Break"><strong class="source-inline">LangChain</strong></span><span class="No-Break"> library:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_50" title2="(no caption)" no2="">pip install langchain</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Here’s a simple example of how to use <span class="No-Break">this parser:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_51" title2="(no caption)" no2="">from langchain.text_splitter import CharacterTextSplitter
from llama_index.core.node_parser import LangchainNodeParser
parser = LangchainNodeParser(CharacterTextSplitter())
nodes = parser.get_nodes_from_documents(document)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p class="callout-heading">A quick note on LangChain</p>
			<p class="callout">The LangChain framework is similar in purpose to LlamaIndex and provides a versatile toolkit specialized <a id="_idIndexMarker250"></a>in advanced natural language processing capabilities. Its collection of text segmentation, summarization, and language understanding models assist in splitting and digesting textual data into coherent chunks ready for indexing in a similar way to LlamaIndex. When dealing with large data sources requiring nuanced linguistic analysis, LangChain empowers users to finely control the breakdown and ingestion of text - ensuring context and clarity are retained for downstream retrieval and querying. As you can see, the two can complement each other in a RAG scenario. Want to know more? Check <span class="No-Break">out </span><a href="https://www.langchain.com/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://www.langchain.com/</span></a><span class="No-Break">.</span></p>
			<p>Let’s see what other parsers we <span class="No-Break">have available.</span></p>
			<h4 data-type="sect3" class="sect3" title2="SimpleFileNodeParser" no2="4.5.2.3">4.5.2.3. SimpleFileNodeParser</h4>
			<p>This one automatically decides which of the following three node parsers should be used based <a id="_idIndexMarker251"></a>on file types. It can automatically handle these file formats <a id="_idIndexMarker252"></a>and transform them into nodes, simplifying the process of interacting with various types <span class="No-Break">of content:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_52" title2="(no caption)" no2="">parser = SimpleFileNodeParser()
nodes = parser.get_nodes_from_documents(documents)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>You can simply rely on <strong class="source-inline">FlatReader</strong> to load the file into your <strong class="source-inline">Document</strong> object; <strong class="source-inline">SimpleFileNodeParser</strong> will know what to do <span class="No-Break">from there.</span></p>
			<h4 data-type="sect3" class="sect3" title2="HTMLNodeParser" no2="4.5.2.4">4.5.2.4. HTMLNodeParser</h4>
			<p>This parser uses <strong class="bold">Beautiful Soup</strong> to parse HTML files and convert them into nodes based on <a id="_idIndexMarker253"></a>selected HTML tags. This parser simplifies <a id="_idIndexMarker254"></a>the HTML file by extracting text from standard text elements and merging adjacent nodes of the same type. The parser can be used <span class="No-Break">like this:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_53" title2="(no caption)" no2="">my_tags = ["<strong class="bold">p</strong>", "<strong class="bold">span</strong>"]
html_parser = HTMLNodeParser(tags=my_tags)
nodes = html_parser.get_nodes_from_documents(document)
print('&lt;span&gt; elements:')
for node in nodes:
&nbsp;&nbsp;&nbsp;&nbsp;if node.metadata['tag']=='span':
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(node.text)
print('&lt;p&gt; elements:')
for node in nodes:
&nbsp;&nbsp;&nbsp;&nbsp;if node.metadata['tag']=='p':
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(node.text)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>As you <a id="_idIndexMarker255"></a>can see, you have the option to customize <a id="_idIndexMarker256"></a>the HTML <strong class="source-inline">tags</strong> from where you want to <span class="No-Break">retrieve content.</span></p>
			<h4 data-type="sect3" class="sect3" title2="MarkdownNodeParser" no2="4.5.2.5">4.5.2.5. MarkdownNodeParser</h4>
			<p>This parser <a id="_idIndexMarker257"></a>processes raw <em class="italic">markdown</em> text and generates nodes reflecting its structure and content. The markdown node parser divides the content into <a id="_idIndexMarker258"></a>nodes for each header encountered in the file and incorporates the header hierarchy into the metadata. Here’s how to <span class="No-Break">use </span><span class="No-Break"><strong class="source-inline">MarkdownNodeParser</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_54" title2="(no caption)" no2="">parser = MarkdownNodeParser.from_defaults()
nodes = parser.get_nodes_from_documents(document)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<h4 data-type="sect3" class="sect3" title2="JSONNodeParser" no2="4.5.2.6">4.5.2.6. JSONNodeParser</h4>
			<p>This parser <a id="_idIndexMarker259"></a>is specialized in processing and querying <a id="_idIndexMarker260"></a>structured data in JSON format. In a similar way to the Markdown parser, the JSON parser can be used <span class="No-Break">like this:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_55" title2="(no caption)" no2="">json_parser = JSONNodeParser.from_defaults()
nodes = json_parser.get_nodes_from_documents(document)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<h3 id="f_7__idParaDest-81" data-type="sect2" class="sect2" title2="Using relational parsers" no2="4.5.3"><a id="_idTextAnchor080"></a>4.5.3. Using relational parsers</h3>
			<p><strong class="bold">Relational parsers</strong> parse <a id="_idIndexMarker261"></a>information into nodes that are linked to <a id="_idIndexMarker262"></a>each other through relationships. Relationships add a whole new dimension to our data and allow for more advanced retrieval techniques in our <span class="No-Break">RAG workflow.</span></p>
			<h4 data-type="sect3" class="sect3" title2="HierarchicalNodeParser" no2="4.5.3.1">4.5.3.1. HierarchicalNodeParser</h4>
			<p>This parser organizes the nodes into hierarchies across multiple levels. It will generate a hierarchy <a id="_idIndexMarker263"></a>of nodes, starting with top-level nodes with larger section sizes, down to child nodes with smaller section sizes, where each <a id="_idIndexMarker264"></a>child node has a parent node with a larger section size (<span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.1</em>). By default, the parser uses <strong class="source-inline">SentenceSplitter</strong> to chunk text. The node hierarchy looks <span class="No-Break">like this:</span></p>
			<ul>
				<li><em class="italic">Level 1</em>: Section <span class="No-Break">size 2,048</span></li>
				<li><em class="italic">Level 2</em>: Section <span class="No-Break">size 512</span></li>
				<li><em class="italic">Level 3</em>: Section <span class="No-Break">size 128</span></li>
			</ul>
			<p>The top-level nodes, with larger sections, can provide high-level summaries, while the lower nodes can allow for a more detailed analysis of text sections. Have a look at <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.2</em> for a visual representation of <span class="No-Break">this concept:</span></p>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/B21861_04_2.jpg" alt="Figure 4.2 – Hierarchical nodes of 2,048, 512, and 128 chunk sizes" width="1291" height="581" data-type="figure" id="untitled_figure_20" title2="– Hierarchical nodes of 2,048, 512, and 128 chunk sizes" no2="4.2">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – Hierarchical nodes of 2,048, 512, and 128 chunk sizes</p>
			<p>In this way, the different node levels can be used to adjust the accuracy and depth of search results, allowing users to find information at different granularity levels. Here’s an example of how to use this parser in <span class="No-Break">your code:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_56" title2="(no caption)" no2="">hierarchical_parser = HierarchicalNodeParser.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;chunk_sizes=[128, 64, 32],
&nbsp;&nbsp;&nbsp;&nbsp;chunk_overlap=0,
)
nodes = hierarchical_parser.get_nodes_from_documents(document)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>There <a id="_idIndexMarker265"></a>are two <a id="_idIndexMarker266"></a>specific parameters to customize for <span class="No-Break">this parser:</span></p>
			<ul>
				<li><strong class="source-inline">chunk_sizes</strong>: The values in this list define your hierarchy levels based on <span class="No-Break">content size</span></li>
				<li><strong class="source-inline">chunk_overlap</strong>: This defines the overlap size <span class="No-Break">between chunks</span></li>
			</ul>
			<h4 data-type="sect3" class="sect3" title2="UnstructuredElementNodeParser" no2="4.5.3.2">4.5.3.2. UnstructuredElementNodeParser</h4>
			<p>I left this one last because it is used for more special situations. Sometimes, our documents <a id="_idIndexMarker267"></a>may include a mix of text and data tables, which can make parsing inefficient by <span class="No-Break">conventional methods.</span></p>
			<p>This parser <a id="_idIndexMarker268"></a>can process and split these documents into interpretable nodes, distinguishing between text sections and other embedded structures such as tables. We’ll talk about it in more detail toward the end of <span class="No-Break">this chapter.</span></p>
			<h3 id="f_7__idParaDest-82" data-type="sect2" class="sect2" title2="Confused about node parsers and text splitters?" no2="4.5.4"><a id="_idTextAnchor081"></a>4.5.4. Confused about node parsers and text splitters?</h3>
			<p>You may have noticed that I use the two terms quite loosely. Categorizing parsing modules into <a id="_idIndexMarker269"></a>these two groups might initially cause some confusion. To simplify, a node <a id="_idIndexMarker270"></a>parser is a more sophisticated mechanism than a simple splitter. While both serve the same basic function and operate at different levels of complexity, they differ in <span class="No-Break">their implementations.</span></p>
			<p>Text splitters such as <strong class="source-inline">SentenceSplitter</strong> can divide long flat texts into nodes, based on certain rules or limitations, such as <strong class="bold">chunk_size</strong> or <strong class="bold">chunk_overlap</strong>. The nodes could represent lines, paragraphs, or sentences, and may also include additional metadata or links to the <span class="No-Break">original document.</span></p>
			<p>Node parsers <a id="_idIndexMarker271"></a>are more sophisticated and can involve additional data <a id="_idIndexMarker272"></a>processing logic. Beyond simply dividing text into nodes, they can perform extra tasks, such as analyzing the structure of HTML or JSON files and producing nodes enriched with <span class="No-Break">contextual information.</span></p>
			<h3 id="f_7__idParaDest-83" data-type="sect2" class="sect2" title2="Understanding chunk_size and chunk_overlap" no2="4.5.5"><a id="_idTextAnchor082"></a>4.5.5. Understanding chunk_size and chunk_overlap</h3>
			<p>As you have probably understood by now, text splitters are a basic but important component. They <a id="_idIndexMarker273"></a>control how text in documents gets split into nodes during parsing. For each text splitter type, LlamaIndex provides several parameters to customize the <a id="_idIndexMarker274"></a>text <span class="No-Break">splitting behavior.</span></p>
			<p>Probably two of the most important parameters for a text splitter are <strong class="source-inline">chunk_size</strong> and <strong class="source-inline">chunk_overlap</strong>. The text splitters themselves, such as <strong class="source-inline">SentenceSplitter</strong>, <strong class="source-inline">TokenTextSplitter</strong>, <strong class="source-inline">TextSplitter</strong>, and others, take in the <strong class="source-inline">chunk_size</strong> and <strong class="source-inline">chunk_overlap</strong> arguments to control how they break text into smaller chunks during node creation. <strong class="source-inline">chunk_size</strong> controls the maximum length of text chunks in nodes. This is useful for ensuring nodes don’t take long for the LLM to process. Note that in LlamaIndex, the default <strong class="source-inline">chunk_size</strong> is 1,024, while the default <strong class="source-inline">chunk_overlap</strong> <span class="No-Break">is 20.</span></p>
			<p><strong class="bold">chunk size</strong> is an important setting when building an RAG system. If chunks are too small, important context may be lost and the quality of the LLM response will be lower. On the other hand, large chunks increase the size of the prompts, increasing both computational cost and response generation time. An experimental approach was used when the default values were selected for <span class="No-Break">LlamaIndex: </span><a href="https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5</span></a><span class="No-Break">.</span></p>
			<p><strong class="source-inline">chunk_overlap</strong> creates overlapping nodes by re-including some tokens from the previous Node. This helps provide context so that the LLM can understand the continuity of ideas when processing <span class="No-Break">adjacent Nodes.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.3</em> provides a visual representation of <span class="No-Break">this concept:</span></p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B21861_04_3.jpg" alt="Figure 4.3 – chunk_size and chunk_overlap explained" width="1650" height="738" data-type="figure" id="untitled_figure_21" title2="– chunk_size and chunk_overlap explained" no2="4.3">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – chunk_size and chunk_overlap explained</p>
			<p>The concept <a id="_idIndexMarker275"></a>is similar to the way <strong class="source-inline">SentenceWindowNodeParser</strong> works – that is, it <a id="_idIndexMarker276"></a>extracts a <em class="italic">window</em> of context for each sentence. For example, with <strong class="source-inline">chunk_size=100</strong> and <strong class="source-inline">chunk_overlap=10</strong>, let’s say we had the <span class="No-Break">following text:</span></p>
			<p><em class="italic">Gardening is not only a relaxing hobby but also an art form. Cultivating plants, designing landscapes, and nurturing nature bring a sense of accomplishment. Many find it therapeutic and rewarding, especially when they see their </em><span class="No-Break"><em class="italic">garden flourish</em></span><span class="No-Break">.</span></p>
			<p>It would get split into the <span class="No-Break">following areas:</span></p>
			<ul>
				<li><em class="italic">Node 1 (first 100 characters)</em>: “Gardening is not only a relaxing hobby but also an art form. Cultivating plants, designing <span class="No-Break">landscapes, an”</span></li>
				<li><em class="italic">Node 2 (starts from the 75th character, next 100 characters)</em>: “designing landscapes, and nurturing nature bring a sense of accomplishment. Many find it therapeutic <span class="No-Break">and re”</span></li>
				<li><em class="italic">Node 3 (starts from the 150th character to the end of the text)</em>: “Many find it therapeutic and rewarding, especially when they see their <span class="No-Break">garden flourish.”</span></li>
			</ul>
			<p>In this setup, the overlap between node 1 and node 2 is “<strong class="source-inline">designing</strong> landscapes, an,” whereas the overlap between node 2 and node 3 is “Many find it therapeutic <span class="No-Break">and re.”</span></p>
			<p>These overlaps mean that one node re-includes parts of the previous node. This mechanism ensures continuity and context between the chunks, making each part more meaningful when <a id="_idIndexMarker277"></a>read in sequence. Of course, choosing the right values for <a id="_idIndexMarker278"></a>these two parameters is very important. The biggest impact will be on creating vector indexes. We’ll talk about them later, during <a href="#_idTextAnchor104"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Indexing </em><span class="No-Break"><em class="italic">with LlamaIndex</em></span><span class="No-Break">.</span></p>
			<p>Next, let’s have a quick overview of <span class="No-Break">node relationships.</span></p>
			<h3 id="f_7__idParaDest-84" data-type="sect2" class="sect2" title2="Including relationships with include_prev_next_rel" no2="4.5.6"><a id="_idTextAnchor083"></a>4.5.6. Including relationships with include_prev_next_rel</h3>
			<p>Let’s talk a bit <a id="_idIndexMarker279"></a>about another important parameter that can dictate the behavior of our parsers: the <strong class="source-inline">include_prev_next_rel</strong> option. When set to <strong class="source-inline">True</strong>, this option makes the parser automatically add <strong class="source-inline">NEXT</strong> and <strong class="source-inline">PREVIOUS</strong> relationships between consecutive nodes. Here’s <span class="No-Break">an example:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_57" title2="(no caption)" no2="">node_parser = SentenceWindowNodeParser.from_defaults(
&nbsp;&nbsp;include_prev_next_rel=True
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This helps capture the sequencing between nodes. Then, later when querying, you can optionally retrieve previous or next nodes for more context using features such as <strong class="source-inline">PrevNextNodePostprocessor</strong>. More on that in <a href="#_idTextAnchor131"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Querying Our Data, Part 1 – </em><span class="No-Break"><em class="italic">Context Retrieval</em></span><span class="No-Break">.</span></p>
			<p>The relationships get added to the <strong class="source-inline">.relationships</strong> dictionary on <span class="No-Break">each node.</span></p>
			<p>So, node 1 would now be <span class="No-Break">as follows:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_58" title2="(no caption)" no2="">node1.relationships[PREVIOUS] = RelatedNodeInfo(node_id=node0.node_id)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Node 2 would be <span class="No-Break">as follows:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_59" title2="(no caption)" no2="">node2.relationships[NEXT] = RelatedNodeInfo(node_id=node3.node_id)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Capturing these sequences helps provide contextual continuity in long documents and brings a lot of other benefits that I listed in more detail in the <span class="No-Break">previous chapter.</span></p>
			<p>Among other things, having a previous-next relationship enables <em class="italic">cluster retrieval</em>: you can get a cluster of related nodes by following the relationships to fetch nearby connected nodes. This provides a more focused context instead of randomly scattered nodes. Maintaining a <a id="_idIndexMarker280"></a>cohesive narrative thread through the content when following a story or a dialogue is another good reason for having these relationships <span class="No-Break">between nodes.</span></p>
			<p>Next, let’s have a look at how to use these parsers and splitters in <span class="No-Break">our workflow.</span></p>
			<h3 id="f_7__idParaDest-85" data-type="sect2" class="sect2" title2="Practical ways of using these node creation models" no2="4.5.7"><a id="_idTextAnchor084"></a>4.5.7. Practical ways of using these node creation models</h3>
			<p>How you implement the node parsers or text splitters in your code depends on how much you <a id="_idIndexMarker281"></a>want to customize the process but, in the end, it all comes down to three <span class="No-Break">main options:</span></p>
			<ol>
				<li>Using them standalone by calling <strong class="source-inline">get_nodes_from_documents()</strong>, like in <span class="No-Break">this example:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_60" title2="(no caption)" no2="">from llama_index.core import Document
from llama_index.core.node_parser import SentenceWindowNodeParser
doc = Document(
&nbsp;&nbsp;&nbsp;&nbsp;text="Sentence 1. Sentence 2. Sentence 3."
)
parser = SentenceWindowNodeParser.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;window_size=2&nbsp;&nbsp;,
&nbsp;&nbsp;&nbsp;&nbsp;window_metadata_key="ContextWindow",
&nbsp;&nbsp;&nbsp;&nbsp;original_text_metadata_key="node_text"
)
nodes = parser.get_nodes_from_documents([doc])</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div><p class="list-inset">This code will produce three nodes. If we look at the second node, for example, by running <strong class="source-inline">print(nodes[1])</strong>, we’ll get the <span class="No-Break">following output:</span></p><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_61" title2="(no caption)" no2="">Node ID: 0715876a-61e6-4e77-95ba-b93e10de1c67
Text: Sentence 2.</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div><p class="list-inset">As you can see, the parser extracted the second sentence and allocated a random ID to the node. But if we take a peek at the node’s metadata by running <strong class="source-inline">print(nodes[1].metadata)</strong>, we’ll also see the context it gathered, using the keys <span class="No-Break">we specified:</span></p><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_62" title2="(no caption)" no2="">{'ContextWindow': 'Sentence 1.&nbsp;&nbsp;Sentence 2.&nbsp;&nbsp;Sentence 3.', 'node_text': 'Sentence 2. '}</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div><p class="list-inset">This metadata <a id="_idIndexMarker282"></a>can later be used when building queries to provide more context for each sentence and improve the <span class="No-Break">LLM responses.</span></p><p class="list-inset">We’ll explore this in more detail during <a href="#_idTextAnchor131"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Querying Our Data, Part 1 – </em><span class="No-Break"><em class="italic">Context Retrieval</em></span><span class="No-Break">.</span></p></li>				<li>Configuring them <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">Settings</strong></span><span class="No-Break">.</span><p class="list-inset">The second option is a bit more general and convenient when you need to automatically use the same parser for multiple purposes in <span class="No-Break">your app:</span></p><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_63" title2="(no caption)" no2="">from llama_index.core import Settings, Document, 
&nbsp;&nbsp;&nbsp;&nbsp;VectorStoreIndex
from llama_index.core.node_parser import 
&nbsp;&nbsp;&nbsp;&nbsp;SentenceWindowNodeParser
doc = Document(
&nbsp;&nbsp;&nbsp;&nbsp;text="Sentence 1. Sentence 2. Sentence 3."
)
text_splitter = SentenceWindowNodeParser.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;window_size=2&nbsp;&nbsp;,
&nbsp;&nbsp;&nbsp;&nbsp;window_metadata_key="ContextWindow",
&nbsp;&nbsp;&nbsp;&nbsp;original_text_metadata_key="node_text"
)
Settings.text_splitter = text_splitter
index = VectorStoreIndex.from_documents([doc])</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div><p class="list-inset">This time, after we define and configure our custom <strong class="source-inline">text_splitter</strong>, we pre-load it in <strong class="source-inline">Settings</strong>. From this point forward, whenever we call any function that relies on text splitting, our custom <strong class="source-inline">text_splitter</strong> will be used <span class="No-Break">by default.</span></p><p class="list-inset">Of course, this actual example is a bit of overkill. You’ve probably noticed that I’ve used a node <a id="_idIndexMarker283"></a>parser in place of a simple text splitter. The index we’re building with the nodes won’t benefit in any way from the additional context metadata created by the parser. I just wanted to emphasize my previous point regarding parsers <span class="No-Break">and splitters.</span></p></li>				<li>Defining the parsers as a <strong class="bold">transformation</strong> step in an <span class="No-Break"><strong class="bold">ingestion pipeline</strong></span><span class="No-Break">.</span><p class="list-inset">An ingestion pipeline is an automatic and structured process for ingesting data.  It’s running the <a id="_idIndexMarker284"></a>data through a series of steps (called <strong class="bold">transformations</strong>) one <span class="No-Break">by one.</span></p><p class="list-inset">I will explain how this works and what it can be used for later in this chapter, in the <em class="italic">Using the ingestion pipeline to increase efficiency</em> section. You’ll also get to see the code for implementing the parser as a transformation in <span class="No-Break">the pipeline.</span></p></li>
			</ol>
			<p>Next, we’ll talk about metadata and how metadata can be used to improve our <span class="No-Break">RAG application.</span></p>
			<h2 id="f_7__idParaDest-86" data-type="sect1" class="sect1" title2="Working with metadata to improve the context" no2="4.6"><a id="_idTextAnchor085"></a>4.6. Working with metadata to improve the context</h2>
			<p>What is <strong class="bold">metadata</strong>? It’s simply <a id="_idIndexMarker285"></a>additional information we can attach to our documents <a id="_idIndexMarker286"></a>and nodes. This extra context helps LlamaIndex better understand our data. It provides additional context about data and can be customized in terms of visibility <span class="No-Break">and format.</span></p>
			<p>For example, let’s say you’ve <em class="italic">ingested</em> some PDF reports as documents. You could then simply add some metadata <span class="No-Break">like this:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_64" title2="(no caption)" no2="">document.metadata = {
&nbsp;&nbsp;&nbsp;&nbsp;"report_name": "Sales Report April 2022",
&nbsp;&nbsp;&nbsp;&nbsp;"department": "Sales",
&nbsp;&nbsp;&nbsp;&nbsp;"author": "Jane Doe"
}</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This metadata gives vital clues when querying the data later. In this example, we can use it to locate reports by <a id="_idIndexMarker287"></a>department or author. You can store anything useful as metadata – categories, timestamps, locations, <span class="No-Break">and more.</span></p>
			<p>And here’s a neat trick – any metadata you set on a document automatically flows down to child nodes! So, if I set an <strong class="source-inline">author</strong> field on a document, all nodes derived from that document will inherit the <strong class="source-inline">author</strong> metadata. This propagation saves time and prevents duplicating metadata <span class="No-Break">across nodes.</span></p>
			<p>There are multiple ways of <span class="No-Break">defining metadata:</span></p>
			<ol>
				<li>Setting the metadata values directly in the <strong class="source-inline">Document</strong> constructor <span class="No-Break">as follows:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_65" title2="(no caption)" no2="">document = Document(
&nbsp;&nbsp;&nbsp;&nbsp;text="...",
&nbsp;&nbsp;&nbsp;&nbsp;metadata={"author": "John Doe"}
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>				<li>Adding the metadata after <span class="No-Break">document creation:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_66" title2="(no caption)" no2="">document.metadata = {"category": "finance"}</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>				<li>Automatically setting metadata in the ingestion process, when using data connectors such <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">SimpleDirectoryReader</strong></span><span class="No-Break">:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_67" title2="(no caption)" no2="">def set_metadata(filename):
&nbsp;&nbsp;&nbsp;&nbsp;return {"file_name": filename}
documents = SimpleDirectoryReader(
&nbsp;&nbsp;&nbsp;&nbsp;"./data",
&nbsp;&nbsp;&nbsp;&nbsp;file_metadata=set_metadata("file1.txt")
).load_data()</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>				<li>Using standalone, dedicated extractors provided by LlamaIndex. <strong class="bold">Metadata</strong> <strong class="bold">extractors</strong> are a <a id="_idIndexMarker288"></a>powerful way to generate relevant metadata from text using the power of LLMs. This extracted metadata can then be attached to documents and nodes to provide <span class="No-Break">additional context</span></li>
				<li>Defining the <a id="_idIndexMarker289"></a>extractor as a <em class="italic">transformation</em> step in an ingestion pipeline. Just like in the case of node parsers, extractors can also become part of the pipeline. We’ll cover this approach later in this chapter in the <em class="italic">Using the ingestion pipeline to increase </em><span class="No-Break"><em class="italic">efficiency</em></span><span class="No-Break"> section</span></li>
			</ol>
			<p>But first, let’s put our magnifying glass on these specialized metadata extractors to better understand how <span class="No-Break">they work.</span></p>
			<p>Before you go further, if you want to run the following code examples, make sure to include the necessary imports, document ingestion, and node parsing logic at the beginning of your code by adding the <span class="No-Break">following lines:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_68" title2="(no caption)" no2="">From llama_index.core import SimpleDirectoryReader
from llama_index.core.node_parser import SentenceSplitter
reader = SimpleDirectoryReader('files')
documents = reader.load_data()
parser = SentenceSplitter(include_prev_next_rel=True)
nodes = parser.get_nodes_from_documents(documents)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This boilerplate code prepares your data – ingested from the <strong class="source-inline">files</strong> subfolder – and puts everything you need into <strong class="source-inline">Nodes</strong>. We’ll store our metadata in a variable called <strong class="source-inline">metadata_list</strong>. I’ve added <strong class="source-inline">print(metadata_list)</strong> at the end of each example so that we’ll see an output of the extracted metadata. Apart from describing their logic, I’ve also highlighted practical uses for each one of <span class="No-Break">the extractors.</span></p>
			<h3 id="f_7__idParaDest-87" data-type="sect2" class="sect2" title2="SummaryExtractor" no2="4.6.1"><a id="_idTextAnchor086"></a>4.6.1. SummaryExtractor</h3>
			<p>This extractor generates summaries of the text contained by the node. Optionally, it can generate <a id="_idIndexMarker290"></a>summaries for the previous and next adjacent nodes. Here’s <span class="No-Break">an example:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_69" title2="(no caption)" no2="">from llama_index.core.extractors import SummaryExtractor
summary_extractor = SummaryExtractor(summaries=["prev", "self", 
&nbsp;&nbsp;&nbsp;&nbsp;"next"])
metadata_list = summary_extractor.extract(nodes)
print(metadata_list)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This extractor <a id="_idIndexMarker291"></a>generates concise summaries for each node or adjacent node. These are essential during the retrieve phase in an RAG architecture. This ensures that the search can consider the summary of the documents without having to process the entirety of <span class="No-Break">their content.</span></p>
			<p class="callout-heading">Practical use case</p>
			<p class="callout">Imagine a customer support knowledge base on which <strong class="source-inline">SummaryExtractor</strong> can provide summaries of customer issues and resolutions. Then, when a new support request comes in, our app can retrieve the most relevant past cases to help generate a detailed and <span class="No-Break">contextual solution.</span></p>
			<p>You can customize the type of <strong class="source-inline">summaries</strong> to generate by setting the values in the summaries list and the actual prompt that will be used with the LLM by defining the prompt in the <span class="No-Break"><strong class="source-inline">prompt_template</strong></span><span class="No-Break"> parameter.</span></p>
			<h3 id="f_7__idParaDest-88" data-type="sect2" class="sect2" title2="QuestionsAnsweredExtractor" no2="4.6.2"><a id="_idTextAnchor087"></a>4.6.2. QuestionsAnsweredExtractor</h3>
			<p>This extractor <a id="_idIndexMarker292"></a>generates a specified number of questions <a id="_idIndexMarker293"></a>the node text <span class="No-Break">can answer.</span></p>
			<p>The following example should give you a <span class="No-Break">usage guideline:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_70" title2="(no caption)" no2="">from llama_index.core.extractors import QuestionsAnsweredExtractor
qa_extractor = QuestionsAnsweredExtractor(questions=5)
metadata_list = qa_extractor.extract(nodes)
print(metadata_list)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This extractor identifies questions that the text is uniquely positioned to answer, allowing the retrieval process to focus on nodes that explicitly address <span class="No-Break">specific inquiries.</span></p>
			<p class="callout-heading">Practical use case</p>
			<p class="callout">For an FAQ system, the extractor identifies unique questions answered by articles, making it easier to find precise answers to <span class="No-Break">user queries.</span></p>
			<p>You can <a id="_idIndexMarker294"></a>customize the number of questions it <a id="_idIndexMarker295"></a>generates but also the actual prompt that will be used with the LLM – by setting the <strong class="source-inline">prompt_template</strong> parameter. There is also an <strong class="source-inline">embedding_only</strong> Boolean parameter that – if set to <strong class="source-inline">True</strong> – will make the metadata available only for embeddings. More on that in <a href="#_idTextAnchor104"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Indexing </em><span class="No-Break"><em class="italic">with LlamaIndex</em></span><span class="No-Break">.</span></p>
			<h3 id="f_7__idParaDest-89" data-type="sect2" class="sect2" title2="TitleExtractor" no2="4.6.3"><a id="_idTextAnchor088"></a>4.6.3. TitleExtractor</h3>
			<p>This one <a id="_idIndexMarker296"></a>extracts a title for the text. Here’s <span class="No-Break">an example:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_71" title2="(no caption)" no2="">from llama_index.core.extractors import TitleExtractor
title_extractor = TitleExtractor ()
metadata_list = title_extractor.extract(nodes)
print(metadata_list)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p><strong class="source-inline">TitleExtractor</strong> specializes in pulling out meaningful titles from larger texts, assisting in the <a id="_idIndexMarker297"></a>quick identification and retrieval of documents. In digital libraries, for example, <strong class="source-inline">TitleExtractor</strong> can help categorize documents by extracting titles from untitled texts, making retrieval more efficient when titles are used as search keywords. There are several parameters you can tweak for <span class="No-Break">this extractor:</span></p>
			<ul>
				<li><strong class="source-inline">nodes</strong>: This sets the <a id="_idIndexMarker298"></a>number of nodes to use for <span class="No-Break">title extraction</span></li>
				<li><strong class="source-inline">node_template</strong>: This changes the default prompt template that’s used for extracting <span class="No-Break">the titles</span></li>
				<li><strong class="source-inline">combine_template</strong>: This changes the prompt template for combining multiple <a id="_idIndexMarker299"></a>node-level titles in a <span class="No-Break">document-wide title</span></li>
			</ul>
			<p>Now, let’s look <span class="No-Break">at </span><span class="No-Break"><strong class="source-inline">EntityExtractor</strong></span><span class="No-Break">.</span></p>
			<h3 id="f_7__idParaDest-90" data-type="sect2" class="sect2" title2="EntityExtractor" no2="4.6.4"><a id="_idTextAnchor089"></a>4.6.4. EntityExtractor</h3>
			<p>This will <a id="_idIndexMarker300"></a>extract entities such as people, locations, organizations, and more from <a id="_idIndexMarker301"></a>the node text by using the <strong class="bold">span-marker</strong> package. This <a id="_idIndexMarker302"></a>package is installed automatically together with the <strong class="source-inline">EntityExtractor</strong> integration, so no <a id="_idIndexMarker303"></a>additional installations are required. It provides the ability to perform <strong class="bold">named entity recognition</strong> (<strong class="bold">NER</strong>) and relies on a <a id="_idIndexMarker304"></a>tokenizer provided by the <strong class="bold">Natural Language Toolkit</strong> (<strong class="bold">NLTK</strong>) <span class="No-Break">package: </span><a href="https://www.nltk.org/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://www.nltk.org/</span></a><span class="No-Break">.</span></p>
			<p class="callout-heading">A quick note on NER</p>
			<p class="callout">NER is a technique that’s used by computers to identify and label specific entities in text, such as people’s names, company names, places, and dates. This helps the computer to better understand the content and provides useful context in an <span class="No-Break">RAG scenario.</span></p>
			<p>Here’s a code example for using <span class="No-Break">this extractor:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_72" title2="(no caption)" no2="">from llama_index.core.extractors import EntityExtractor
entity_extractor = EntityExtractor (
&nbsp;&nbsp;&nbsp;&nbsp;label_entities = True,
&nbsp;&nbsp;&nbsp;&nbsp;device = "cpu"
)
metadata_list = entity_extractor.extract(nodes)
print(metadata_list)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The extractor identifies named entities from the text, labels them, and adds them to the metadata, enabling a retrieval system to focus on nodes with <span class="No-Break">specific references.</span></p>
			<p class="callout-heading">Practical use case</p>
			<p class="callout">Imagine a legal document archive having this metadata attached to each node. This extractor <a id="_idIndexMarker305"></a>could ease the retrieval of documents mentioning particular people, locations, or organizations, thus providing the best context for <span class="No-Break">our query.</span></p>
			<p>There’s a <a id="_idIndexMarker306"></a>long list of parameters that you can tune for <span class="No-Break">this extractor:</span></p>
			<ul>
				<li><strong class="source-inline">model_name</strong>: This sets the name of the model to be used <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">SpanMarker</strong></span></li>
				<li><strong class="source-inline">prediction_threshold</strong>: This changes the default 0.5 minimum prediction threshold for named entities. As you may have guessed, entity recognition is usually not a 100% accurate process. However, you can experiment with different values here until you find the <span class="No-Break">best compromise</span></li>
				<li><strong class="source-inline">span_joiner</strong>: This changes the default string used to join <span class="No-Break">the spans</span></li>
				<li><strong class="source-inline">label_entities</strong>: If set to <strong class="source-inline">True</strong>, it will make the extractor label every entity name with an entity type. This could be useful later, in the retrieval and querying phase. By default, this is set <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">False</strong></span></li>
				<li><strong class="source-inline">device</strong>: This controls the device on which the model runs. It defaults to <strong class="source-inline">cpu</strong>, but if your system allows, it can be set <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">cuda</strong></span></li>
				<li><strong class="source-inline">entity_map</strong>: This allows you to customize the labels for each entity type. The extractor comes with a predefined entity map that includes labels for people, organizations, places, events, and <span class="No-Break">many others</span></li>
				<li><strong class="source-inline">Tokenizer</strong>: This allows you to change the default tokenizer function – which defaults to the <span class="No-Break">NLTK tokenizer</span></li>
			</ul>
			<p>Now, let’s discuss how to extract keywords <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">KeywordExtractor</strong></span><span class="No-Break">.</span></p>
			<h3 id="f_7__idParaDest-91" data-type="sect2" class="sect2" title2="KeywordExtractor" no2="4.6.5"><a id="_idTextAnchor090"></a>4.6.5. KeywordExtractor</h3>
			<p>This extractor <a id="_idIndexMarker307"></a>extracts important keywords from the text. Let’s have a look at <span class="No-Break">an example:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_73" title2="(no caption)" no2="">from llama_index.core.extractors import KeywordExtractor
key_extractor = KeywordExtractor (keywords=3)
metadata_list = key_extractor.extract(nodes)
print(metadata_list)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This one <a id="_idIndexMarker308"></a>identifies important words or phrases, making it an invaluable tool for retrieving the most relevant nodes based on <span class="No-Break">user queries.</span></p>
			<p class="callout-heading">Practical use case</p>
			<p class="callout">Integrating <strong class="source-inline">KeywordExtractor</strong> into a content recommendation engine can significantly enhance its effectiveness. By aligning the keywords extracted from content nodes with the terms used in user searches, the engine can more accurately match and recommend content that aligns with user interests. This keyword-based matching ensures that recommendations are not only relevant but also tailored to the specific inquiries or topics users <span class="No-Break">are exploring.</span></p>
			<p>You can customize the number of keywords it generates by changing the <strong class="source-inline">keywords</strong> parameter to a <span class="No-Break">specific value.</span></p>
			<h3 id="f_7__idParaDest-92" data-type="sect2" class="sect2" title2="PydanticProgramExtractor" no2="4.6.6"><a id="_idTextAnchor091"></a>4.6.6. PydanticProgramExtractor</h3>
			<p>This extractor <a id="_idIndexMarker309"></a>extracts metadata using a <strong class="bold">Pydantic</strong> structure. Have <a id="_idIndexMarker310"></a>a look here for a complete example of using this <span class="No-Break">extractor: </span><a href="https://docs.llamaindex.ai/en/stable/examples/metadata_extraction/PydanticExtractor.html#pydantic-extractor" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/metadata_extraction/PydanticExtractor.html#pydantic-extractor</span></a><span class="No-Break">.</span></p>
			<p>This <em class="italic">Swiss Army knife</em> enables the creation of complex and structured metadata schemas with a single LLM call by making use of Pydantic models. One of the main advantages it has over other extractors is that it can pull multiple fields of data using a single LLM call making it a very efficient way to extract metadata. This data will be nicely organized in a model of <span class="No-Break">our design.</span></p>
			<p class="callout-heading">A quick introduction to Pydantic models</p>
			<p class="callout">A <strong class="bold">Pydantic model</strong> is like a <a id="_idIndexMarker311"></a>blueprint or a set of rules that you define as a class in a Python program. It helps you make sure that the data you receive or work with follows certain rules and is in the right format. Think of it as a way to define how your data should look – Pydantic helps you enforce those rules and make sure the data fits in your <span class="No-Break">desired structure.</span></p>
			<p>For example, imagine you have a program that deals with user data such as names, ages, and email addresses. You can create a Pydantic model that specifies that a user’s name should <a id="_idIndexMarker312"></a>be a string, their age should be a number, and their <a id="_idIndexMarker313"></a>email address should be a valid email format. If input data doesn’t follow these rules, Pydantic will raise an error, telling you that the data is not correct. LlamaIndex embraces this mechanism whenever it needs to ensure the consistency and correctness of the data it handles, especially as it often works with complex structures and <span class="No-Break">interrelated data.</span></p>
			<h3 id="f_7__idParaDest-93" data-type="sect2" class="sect2" title2="MarvinMetadataExtractor" no2="4.6.7"><a id="_idTextAnchor092"></a>4.6.7. MarvinMetadataExtractor</h3>
			<p>This <a id="_idIndexMarker314"></a>extractor extracts metadata using the <strong class="bold">Marvin AI engineering framework</strong><span class="P---URL"> </span>(https://www.askmarvin.ai/). Taking advantage of the <a id="_idIndexMarker315"></a>Marvin AI engineering framework, this extractor is <a id="_idIndexMarker316"></a>capable of trustworthy and scalable metadata extraction and augmentation. Its sophistication lies in providing type-safe schemas for text – similar to Pydantic models -  but also supporting business logic transformations. You can find a detailed example <span class="No-Break">here: </span><span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/metadata_extraction/MarvinMetadataExtractorDemo.html</span><span class="No-Break">.</span></p>
			<h3 id="f_7__idParaDest-94" data-type="sect2" class="sect2" title2="Defining your custom extractor" no2="4.6.8"><a id="_idTextAnchor093"></a>4.6.8. Defining your custom extractor</h3>
			<p>Just in case <a id="_idIndexMarker317"></a>none of these ready-made extractors satisfies <a id="_idIndexMarker318"></a>your needs, you can always define your own extractor function. Here is a simple example of how to define a <span class="No-Break">custom extractor:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_74" title2="(no caption)" no2="">from llama_index.core.extractors import BaseExtractor
from typing import List, Dict
class CustomExtractor(BaseExtractor):
&nbsp;&nbsp;&nbsp;&nbsp;async def aextract(self, nodes) -&gt; List[Dict]:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata_list = [
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"node_length":&nbsp;&nbsp;str(len(node.text))
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for node in nodes
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return metadata_list</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This basic <a id="_idIndexMarker319"></a>extractor measures the length in characters for <a id="_idIndexMarker320"></a>each node and saves these values in the metadata. Of course, you could replace that with any logic required by <span class="No-Break">your app.</span></p>
			<p>Having so many tools and methods available at our disposal is a great thing. But then a new question arises: <em class="italic">do we need that much metadata?</em> Let’s find out <span class="No-Break">the answer.</span></p>
			<h3 id="f_7__idParaDest-95" data-type="sect2" class="sect2" title2="Is having all that metadata always a good thing?" no2="4.6.9"><a id="_idTextAnchor094"></a>4.6.9. Is having all that metadata always a good thing?</h3>
			<p>Not necessarily. A key detail is that metadata gets injected into the text that’s sent to the LLM and <a id="_idIndexMarker321"></a>embedding model. This can potentially induce some bias in the models. This means that sometimes, you may not want all metadata to be visible. For example, filenames may help embeddings but may <em class="italic">distract</em> the LLM because the LLM might not understand them as filenames but as other entities instead, and also because the filenames may have no relevance in the context of the prompt. You can selectively hide metadata with the <span class="No-Break">following command:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_75" title2="(no caption)" no2="">document.excluded_llm_metadata_keys = ["file_name"]</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This hides <strong class="source-inline">file_name</strong> from the LLM. You can also hide metadata from embeddings if <span class="No-Break">you want:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_76" title2="(no caption)" no2="">document.excluded_embed_metadata_keys = ["file_name"]</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Also, you can customize the metadata format <span class="No-Break">like this:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_77" title2="(no caption)" no2="">document.metadata_template = "{key}::{value}"</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Here is a pro tip when dealing with metadata mode. LlamaIndex has an enum called <strong class="source-inline">MetadataMode</strong> that controls <span class="No-Break">metadata visibility:</span></p>
			<ul>
				<li><strong class="source-inline">MetadataMode.ALL</strong>: Shows <span class="No-Break">all metadata</span></li>
				<li><strong class="source-inline">MetadataMode.LLM</strong>: Only metadata visible to <span class="No-Break">the LLM</span></li>
				<li><strong class="source-inline">MetadataMode.EMBED</strong>: Only metadata visible <span class="No-Break">to embeddings</span></li>
			</ul>
			<p>You can test <a id="_idIndexMarker322"></a>the visibility of metadata with the <span class="No-Break">following command:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_78" title2="(no caption)" no2="">print(document.get_content(metadata_mode=MetadataMode.LLM))</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>So, in summary, metadata gives your data much-needed context. You have full control over its format and visibility to different models. These customizations let you mold metadata to match your <span class="No-Break">use case!</span></p>
			<p>With that topic exhausted, it’s time to talk <span class="No-Break">about money.</span></p>
			<h2 id="f_7__idParaDest-96" data-type="sect1" class="sect1" title2="Estimating the potential cost of using metadata extractors" no2="4.7"><a id="_idTextAnchor095"></a>4.7. Estimating the potential cost of using metadata extractors</h2>
			<p>A key consideration when utilizing the various metadata extractors in LlamaIndex is the associated <a id="_idIndexMarker323"></a>LLM compute costs. As mentioned earlier, most of these extractors rely on LLMs under the hood to analyze text and generate <span class="No-Break">descriptive metadata.</span></p>
			<p>Repeatedly calling LLMs to process large volumes of text can quickly add up in charges. For example, if you are extracting summaries and keywords from thousands of document nodes using <strong class="source-inline">SummaryExtractor</strong> and <strong class="source-inline">KeywordExtractor</strong>, those constant LLM invocations will carry a <span class="No-Break">significant cost.</span></p>
			<h3 id="f_7__idParaDest-97" data-type="sect2" class="sect2" title2="Follow these simple best practices to minimize your costs" no2="4.7.1"><a id="_idTextAnchor096"></a>4.7.1. Follow these simple best practices to minimize your costs</h3>
			<p>Let’s talk <a id="_idIndexMarker324"></a>about some common best practices for minimizing your <span class="No-Break">LLM costs:</span></p>
			<ul>
				<li>Batch content into fewer LLM calls instead of individual calls per node. This amortizes the overhead because you consume fewer tokens compared to multiple separate calls. Using the Pydantic extractor is very useful for this purpose since it generates multiple fields in a single <span class="No-Break">LLM call</span></li>
				<li>Use cheaper LLM models with lower compute requirements if full accuracy is not necessary. However, be careful – you may introduce errors in your data, and these errors have the bad habit of propagating and <span class="No-Break">amplifying downstream</span></li>
				<li>Cache previous extractions and reuse them without having to re-invoke LLMs every time. I’m going to show you how to accomplish that using <em class="italic">ingestion pipelines</em> later in this chapter, in the <em class="italic">Using the ingestion pipeline to increase </em><span class="No-Break"><em class="italic">efficiency</em></span><span class="No-Break"> section</span></li>
				<li>Restrict metadata extraction only to select subsets of critical nodes rather than full coverage. This may be difficult to implement in an <span class="No-Break">automated scenario</span></li>
				<li>Consider offline LLMs to eliminate cloud costs. Depending on your hardware, this may or may not be <span class="No-Break">a solution</span></li>
			</ul>
			<p>While these guidelines should help you greatly reduce the extraction costs, it’s still a good idea to make sure you run some estimates before processing <span class="No-Break">large datasets.</span></p>
			<h3 id="f_7__idParaDest-98" data-type="sect2" class="sect2" title2="Estimate your maximal costs before running the actual extractors" no2="4.7.2"><a id="_idTextAnchor097"></a>4.7.2. Estimate your maximal costs before running the actual extractors</h3>
			<p>Here is a <a id="_idIndexMarker325"></a>basic example of how we can estimate LLM <a id="_idIndexMarker326"></a>costs by using a <strong class="bold">MockLLM</strong> before running the extractor on the <span class="No-Break">real one:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_79" title2="(no caption)" no2="">from llama_index.core import 
ettings
from llama_index.core.extractors import QuestionsAnsweredExtractor
from llama_index.core.llms.mock import MockLLM
from llama_index.core.schema import TextNode
from llama_index.core.callbacks import (
&nbsp;&nbsp;&nbsp;&nbsp;CallbackManager,
&nbsp;&nbsp;&nbsp;&nbsp;TokenCountingHandler
)
llm = MockLLM(max_tokens=256)
counter = TokenCountingHandler(verbose=False)
callback_manager = CallbackManager([counter])
Settings.llm = llm
Settings.callback_manager = CallbackManager([counter])
sample_text = (
&nbsp;&nbsp;&nbsp;&nbsp;"LlamaIndex is a powerful tool used "
&nbsp;&nbsp;&nbsp;&nbsp;"to create efficient indices from data."
)
nodes= [TextNode(text=sample_text)]
extractor = QuestionsAnsweredExtractor(
&nbsp;&nbsp;&nbsp;&nbsp;show_progress=False
)
Questions_metadata = extractor.extract(nodes)
print(f"Prompt Tokens: {counter.prompt_llm_token_count}")
print(f"Completion Tokens: {counter.completion_llm_token_count}")
print(f"Total Token Count: {counter.total_llm_token_count}")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>You’ll notice that we’re using some specialized tools to run the actual estimation. Let’s have a quick overview of the code. <strong class="source-inline">MockLLM</strong> – as its name implies – is a stand-in LLM that simulates the behavior of an LLM without any actual <span class="No-Break">API calls.</span></p>
			<p>When you create a <strong class="source-inline">MockLLM</strong> instance, you have the option to set a <strong class="source-inline">max_tokens</strong> parameter. This parameter represents the maximum number of tokens that the mock model is <a id="_idIndexMarker327"></a>supposed to generate for any given prompt, mirroring the behavior you’d expect from a real language model – but without actually generating any <span class="No-Break">meaningful content.</span></p>
			<p class="callout-heading">How does the max_token parameter work?</p>
			<p class="callout">The goal here is to predict a <em class="italic">worst-case</em> scenario, but your actual cost will vary depending <a id="_idIndexMarker328"></a>on the LLM response size and in most regular scenarios should be lower than the <strong class="source-inline">max_tokens</strong> value. It’s still a very useful tool because it helps you understand how different metadata extraction strategies applied to different datasets can affect your total cost. For metadata extraction, this total cost will depend on the prompt and response size multiplied by the total number of calls the <span class="No-Break">extractor performs.</span></p>
			<p><strong class="bold">CallbackManager</strong> is a debugging <a id="_idIndexMarker329"></a>mechanism that’s implemented in LlamaIndex that we will cover in more detail in <a href="#_idTextAnchor214"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Prompt Engineering Guidelines and Best Practices</em>. In our example, <strong class="source-inline">CallbackManager</strong> is used in <a id="_idIndexMarker330"></a>combination with the <strong class="bold">TokenCountingHandler</strong> module, which is specialized in counting the tokens that are used for various operations involving an LLM. When defining <strong class="source-inline">TokenCountingHandler</strong>, you can also specify a <span class="No-Break"><strong class="source-inline">tokenizer</strong></span><span class="No-Break"> parameter.</span></p>
			<p class="callout-heading">What is the tokenizer and why do we need it?</p>
			<p class="callout">The <strong class="bold">tokenizer</strong> is responsible for <em class="italic">tokenization</em> of the text – that is, converting it into tokens – since LLMs work <a id="_idIndexMarker331"></a>with tokens and also measure their usage using tokens. When running a cost prediction for a specific prompt on a specific LLM, it’s important to use a tokenizer that is compatible with that specific LLM. Each LLM is often trained with a particular tokenizer, which determines how the text is split into tokens. Using the correct tokenizer is important if you want to make more accurate cost predictions. By default, LlamaIndex uses the <strong class="source-inline">CL100K</strong> tokenizer, which is specific for GPT-4. So, if you plan on using other LLMs, you may want to customize the tokenizer. More on this topic and on how we can optimize the costs of our RAG app will be covered in <a href="#_idTextAnchor214"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Prompt Engineering Guidelines and </em><span class="No-Break"><em class="italic">Best Practices</em></span><span class="No-Break">.</span></p>
			<p>Going back to our example, what happens under the hood is that when we run the extractor, it uses <strong class="source-inline">MockLLM</strong> – so, everything stays locally. Then, <strong class="source-inline">TokenCountingHandler</strong> <em class="italic">intercepts</em> both the prompt and the response from this <strong class="source-inline">MockLLM</strong> and counts the actual number of <span class="No-Break">tokens used.</span></p>
			<p>We will discuss a similar mechanism that can be used for estimating the costs of generating <a id="_idIndexMarker332"></a>certain types of Indexes and running queries later in <em class="italic">Chapters 5</em> <span class="No-Break">and </span><span class="No-Break"><em class="italic">6</em></span><span class="No-Break">.</span></p>
			<p>In this example, I’ve shown you how to estimate the cost for only one type of extractor, <strong class="source-inline">QuestionsAnsweredExtractor</strong>. If you need to estimate the individual cost for more than one extractor in the same run, you can use the <strong class="source-inline">token_counter.reset_counts()</strong> method to reset the counters to zero before running the next <span class="No-Break">extraction round.</span></p>
			<p class="callout-heading">The main lesson of this section</p>
			<p class="callout">While rich metadata unlocks many capabilities, overuse without conscious optimization can negatively impact operating costs and ruin your day. Make sure you take that into account. Apply best practices to minimize the costs and always estimate before running extractors on <span class="No-Break">large datasets.</span></p>
			<p>Next, let’s talk about another very important aspect to consider <span class="No-Break">data privacy.</span></p>
			<h2 id="f_7__idParaDest-99" data-type="sect1" class="sect1" title2="Preserving privacy with metadata extractors, and not only" no2="4.8"><a id="_idTextAnchor098"></a>4.8. Preserving privacy with metadata extractors, and not only</h2>
			<p>Augmenting LLMs with your proprietary data – which, by the way, may belong to your customers in <a id="_idIndexMarker333"></a>many instances – can prove to be a challenging task in terms of <strong class="bold">data privacy</strong>. While a cloud based LLM solution can enrich your proprietary data and offer numerous advantages, <em class="italic">uncontrolled data sharing with external parties can quickly turn into a legal, security, and </em><span class="No-Break"><em class="italic">regulatory nightmare</em></span><span class="No-Break">.</span></p>
			<p>Although the topic of data privacy is more stringent in the case of indexing and querying, utilizing metadata extractors can also raise potential privacy concerns to be aware of. Therefore, I believe a brief warning is <span class="No-Break">required already.</span></p>
			<p>Since most extractors rely on processing content via LLMs to generate metadata, this means your actual data gets transmitted to and analyzed by external <span class="No-Break">cloud services.</span></p>
			<p>There is a risk <a id="_idIndexMarker334"></a>of exposure or mishandling of any personal or confidential information contained in this data, whether due to security lapses, insider risks at the LLM vendor, or <span class="No-Break">malicious activities.</span></p>
			<p class="callout-heading">It’s not just OUR privacy at stake here</p>
			<p class="callout">Speaking of privacy issues, remember the example LlamaHub connectors we discussed earlier? Ingesting messages with <strong class="source-inline">DiscordReader</strong> transfers data from Discord servers. Given that Discord messages may contain private conversations, there is a potential privacy concern, especially if Discord’s terms of service and the expectations of the message senders are not taken into account. So, if your data includes private identities, healthcare details, financial information, and so on, allowing unrestrained extraction workflows could <span class="No-Break">be problematic.</span></p>
			<p>Here are some ways to mitigate <span class="No-Break">privacy risks:</span></p>
			<ul>
				<li>Scrubbing personal data before ingestion into LlamaIndex using, for example, <strong class="source-inline">PIINodePostprocessor</strong> in combination with a local LLM. Check out the next section for a simple implementation guideline for <span class="No-Break">this option</span></li>
				<li>Restricting metadata extraction to only non-sensitive subsets of nodes. Of course, this assumes that you manually classify the sensitivity of each Node. That would be impractical for automated <span class="No-Break">processing pipelines</span></li>
				<li>Running LLMs locally instead of in the cloud where possible to limit external exposure. That depends, of course, on your available hardware and <span class="No-Break">model choice</span></li>
				<li>Enabling encryption mechanisms if such features are available with certain LLM vendors. If privacy is a big concern in your implementation, you might want to consider <a id="_idIndexMarker335"></a>and read more about <strong class="bold">fully homomorphic encryption</strong> (<span class="No-Break"><strong class="bold">FHE</strong></span><span class="No-Break">): </span><a href="https://huggingface.co/blog/encrypted-llm" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://huggingface.co/blog/encrypted-llm</span></a></li>
			</ul>
			<p>These concerns and best practices apply to any type of interaction with an LLM. This subject has been discussed and analyzed in many available lectures and articles, so I’m not going <a id="_idIndexMarker336"></a>to go into further detail here. But that doesn’t mean it’s <span class="No-Break">not important!</span></p>
			<p class="callout-heading">Key message</p>
			<p class="callout">What you should understand is that using an LLM already poses a privacy risk for your data. Augmenting that LLM with an additional framework such as LlamaIndex means also augmenting the privacy <span class="No-Break">risks involved.</span></p>
			<p>In essence, additional diligence is needed when dealing with private data to ensure convenience does not override <span class="No-Break">security requirements.</span></p>
			<h3 id="f_7__idParaDest-100" data-type="sect2" class="sect2" title2="Scrubbing personal data and other sensitive information" no2="4.8.1"><a id="_idTextAnchor099"></a>4.8.1. Scrubbing personal data and other sensitive information</h3>
			<p>In a world filled with nosy onlookers and data rulebooks, it’s crucial to be as cautious with your <a id="_idIndexMarker337"></a>data as a squirrel guarding its acorns in a crowded park! The good news is that there are solutions for ensuring privacy. And a convenient one <a id="_idIndexMarker338"></a>is already provided by the <span class="No-Break">LlamaIndex framework.</span></p>
			<p><strong class="bold">Node post-processors</strong> can solve this problem <span class="No-Break">for us.</span></p>
			<p>In the <a id="_idIndexMarker339"></a>previous chapter, we discovered how node post-processors are used in a query engine. They are applied to the nodes that are returned from a retriever, before the response synthesis step, to apply different transformations on the nodes or node data itself. This is, at least, their most common <span class="No-Break">use case.</span></p>
			<p class="callout-heading">But there’s also another reason to use them</p>
			<p class="callout">It turns out we can also use node processors outside of the query engine. Among other things, they can be used to clean up any sensitive data before extracting metadata using external LLMs, <span class="No-Break">for example.</span></p>
			<p>There are two methods available: <strong class="source-inline">PIINodePostprocessor</strong> and <strong class="source-inline">NERPIINodePostprocessor</strong>. The first one is designed to work with any local LLM that you may have on hand, while the other is customized for using a specialized NER model. In case <a id="_idIndexMarker340"></a>you’re not familiar with the acronym, <strong class="bold">PII</strong> stands for <strong class="bold">Personally </strong><span class="No-Break"><strong class="bold">Identifiable Information</strong></span><span class="No-Break">.</span></p>
			<p>Here’s a simple example of using <strong class="source-inline">NERPIINodePostprocessor</strong> to clean up the data. This method uses a NER model from <strong class="bold">Hugging Face</strong> to do the job. Because I wanted to keep it simple, I didn’t specify a particular model. Therefore, you may expect a warning and the HuggingFaceLLM will probably default to using the <strong class="source-inline">dbmdz/bert-large-cased-finetuned-conll03-english</strong> model, as documented <span class="No-Break">here: </span><span class="No-Break">https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english</span><span class="No-Break">.</span></p>
			<p>Make sure <a id="_idIndexMarker341"></a>you install the corresponding integration <span class="No-Break">package first:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_80" title2="(no caption)" no2="">pip install llama-index-llms-huggingface</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Also, on the first run, the code will download the model from Hugging Face and you’ll need to make sure you have at least 1.5 GB of free space available on <span class="No-Break">your machine.</span></p>
			<p>Here is <span class="No-Break">the code:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_81" title2="(no caption)" no2="">from llama_index.core.postprocessor import NERPIINodePostprocessor
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core.schema import NodeWithScore, TextNode
original = (
&nbsp;&nbsp;&nbsp;&nbsp;"Dear Jane Doe. Your address has been recorded in "
&nbsp;&nbsp;&nbsp;&nbsp;"our database. Please confirm it is valid: 8804 Vista "
&nbsp;&nbsp;&nbsp;&nbsp;"Serro Dr. Cabo Robles, California(CA)."
)
node = TextNode(text=original)
processor = NERPIINodePostprocessor()
clean_nodes = processor.postprocess_nodes(
&nbsp;&nbsp;&nbsp;&nbsp;[NodeWithScore(node=node)]
)
print(clean_nodes[0].node.get_text())</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The output should be similar <span class="No-Break">to this:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_82" title2="(no caption)" no2="">Dear [PER_5]. Your address has been recorded in our database. Please confirm it is valid: 8804 [LOC_95] Dr. [LOC_111], [LOC_124]([LOC_135]).</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Looking at the results, we can see that the names have been replaced with placeholders so that the <a id="_idIndexMarker342"></a>data can now be safely passed to any external LLM. The beauty of this method is that, on return, the answer can be processed back and the placeholders can be replaced with the original data, resulting in a seamless <span class="No-Break">user experience.</span></p>
			<p>The actual mapping between placeholders and real data will be stored in <strong class="source-inline">clean_nodes[0].node.metadata</strong>. This metadata will not be sent to the LLM and can later be used to produce the original names during <span class="No-Break">response synthesis.</span></p>
			<p>Next, we’ll discuss how to improve the efficiency of the <span class="No-Break">ingestion pipeline.</span></p>
			<h2 id="f_7__idParaDest-101" data-type="sect1" class="sect1" title2="Using the ingestion pipeline to increase efficiency" no2="4.9"><a id="_idTextAnchor100"></a>4.9. Using the ingestion pipeline to increase efficiency</h2>
			<p>Starting with <strong class="source-inline">version 0.9</strong>, the LlamaIndex framework introduced a really neat concept: the <a id="_idIndexMarker343"></a>so-called <span class="No-Break"><strong class="bold">ingestion pipeline</strong></span><span class="No-Break">.</span></p>
			<p class="callout-heading">A simple analogy</p>
			<p class="callout">An ingestion pipeline is a bit like a conveyor belt in a factory. In the context of LlamaIndex, it’s a setup that takes your raw data and gets it ready to be integrated into your RAG workflow. It does <a id="_idIndexMarker344"></a>this by running the data through a series of steps – called <strong class="bold">transformations</strong> – one by one. The key idea is to break the ingestion process into a series of reusable transformations that are applied to input data. This helps standardize and customize ingestion flows for different use cases. Think of transformations as different workstations along this conveyor belt. As your raw data moves along, it hits different stations where something specific happens. It might be split into sentences at one station – that’s your <strong class="source-inline">SentenceSplitter</strong> – and have a title extracted at another – such as <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">TitleExtractor</strong></span><span class="No-Break">.</span></p>
			<p>If the factory’s default workstations don’t quite cut it for you, no worries! Let’s say you have this special tool you want to use on your raw data. LlamaIndex makes it easy to plug in your custom tool – <strong class="bold">custom transformation</strong>. Just say what your tool does – for example, replacing acronyms with complete names using a dictionary – and LlamaIndex will happily add it <a id="_idIndexMarker345"></a>to your pipeline. <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.4</em> provides an ingestion <span class="No-Break">pipeline schematic:</span></p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/B21861_04_4.jpg" alt="Figure 4.4 – An ingestion pipeline at work" width="1650" height="509" data-type="figure" id="untitled_figure_22" title2="– An ingestion pipeline at work" no2="4.4">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – An ingestion pipeline at work</p>
			<p>The most important thing about the ingestion pipeline is that <em class="italic">it remembers the data it has already processed</em>. It runs a hashing function on the combination of each node data and each transformation run. On any future runs of the same transformation on the same nodes, the hashes will be identical, so the cached, already processed data will be used instead of re-running <span class="No-Break">the transformation.</span></p>
			<p class="callout-heading">What does this mean for me?</p>
			<p class="callout">If you send the same document through the pipeline again, it’s like having a fast-track lane where it skips the line because it’s already been handled. This is cool because it saves you both time and money by avoiding useless multiple processing of the <span class="No-Break">same data.</span></p>
			<p>By default, the cache is stored locally but you can customize the storage options and use any external database provider you prefer. Let’s cover an example of how the pipeline could be implemented. I will explain the code section by section to make it easier <span class="No-Break">to follow.</span></p>
			<p>Let’s start with the first section of <span class="No-Break">the code:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_83" title2="(no caption)" no2="">from llama_index.core import SimpleDirectoryReader
from llama_index.core.extractors import SummaryExtractor,QuestionsAnsweredExtractor
from llama_index.core.node_parser import TokenTextSplitter
from llama_index.core.ingestion import IngestionPipeline, IngestionCache
from llama_index.core.schema import TransformComponent
class CustomTransformation(TransformComponent):
&nbsp;&nbsp;def __call__(self, nodes, **kwargs):
&nbsp;&nbsp;&nbsp;&nbsp;# run any node transformation logic here
&nbsp;&nbsp;&nbsp;&nbsp;return nodes</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>After taking care <a id="_idIndexMarker346"></a>of the required imports, to show you how to customize your pipeline, I have defined a class called <strong class="source-inline">CustomTransformation</strong>. This will <a id="_idIndexMarker347"></a>be fed into the pipeline later. In my example, no actual processing takes place, so this will return the <span class="No-Break">nodes unchanged.</span></p>
			<p>Let’s continue with the <span class="No-Break">second section:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_84" title2="(no caption)" no2="">reader = SimpleDirectoryReader('files')
documents = reader.load_data()
try:
&nbsp;&nbsp;&nbsp;&nbsp;cached_hashes = IngestionCache.from_persist_path(
"./ingestion_cache.json"
)
&nbsp;&nbsp;&nbsp;&nbsp;print("Cache file found. Running using cache...")
except:
&nbsp;&nbsp;&nbsp;&nbsp;cached_hashes = ""
&nbsp;&nbsp;&nbsp;&nbsp;print("No cache file found. Running without cache...")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The preceding code is <a id="_idIndexMarker348"></a>responsible for fetching all content of the <strong class="source-inline">files</strong> subfolder into documents. Next, the code checks if the cache file already exists and attempts to load it into memory. Remember, the cache file contains the hashes and the results generated by the previous runs. The first time you run the code, there will be no file, so the code won’t load any <span class="No-Break">cached values.</span></p>
			<p>Let’s move on to the <span class="No-Break">third section:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_85" title2="(no caption)" no2="">pipeline = IngestionPipeline(
&nbsp;&nbsp;&nbsp;&nbsp;transformations = [
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CustomTransformation(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TokenTextSplitter(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;separator=" ",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chunk_size=512,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chunk_overlap=128),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SummaryExtractor(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;QuestionsAnsweredExtractor(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;questions=3
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;],
&nbsp;&nbsp;&nbsp;&nbsp;cache=cached_hashes
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This is the part where we define our pipeline. As you can see, it will contain four transformations. The first is our <strong class="source-inline">CustomTransformation</strong>, followed by <strong class="source-inline">TokenTextSplitter</strong>, which is responsible for breaking each document into smaller chunks and generating nodes. The third transformation extracts the summaries metadata and the last one extracts a set of questions that each node <span class="No-Break">can answer.</span></p>
			<p>If you want to take a peek at the result, you could add <strong class="source-inline">print(nodes[0])</strong> at the end of the entire script. Notice that, in the <strong class="source-inline">cache</strong> parameter, we also specify the source of the cache for the pipeline. If that is empty, it will be ignored; otherwise, it will be used to avoid any <a id="_idIndexMarker349"></a>unnecessary processing by retrieving values from <span class="No-Break">the cache.</span></p>
			<p>And now, the <span class="No-Break">final part:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_86" title2="(no caption)" no2="">nodes = pipeline.run(
&nbsp;&nbsp;&nbsp;&nbsp;documents=documents,
&nbsp;&nbsp;&nbsp;&nbsp;show_progress=True,
)
pipeline.cache.persist("./ingestion_cache.json")
print("All documents loaded")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This is where we run the pipeline with the <strong class="source-inline">show_progress</strong> option set to <strong class="source-inline">True</strong>. This will make the pipeline’s progress visible and help you better understand what’s happening in the background. In the end, we save the results in the cache file to avoid re-processing in the <span class="No-Break">next run.</span></p>
			<p class="callout-heading">A quick side note:</p>
			<p class="callout">Even if you saved a cache file, any changes you make in your pipeline logic will not be cached and will have to be processed at the <span class="No-Break">next run.</span></p>
			<p>You should also know there is an alternative to manually defining and running the pipeline every time you want to ingest more data. Just like with the node parsers, we can define the transformations inside <strong class="source-inline">Settings</strong> <span class="No-Break">like this:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_87" title2="(no caption)" no2="">from llama_index.core import Settings
Settings.transformations = [
&nbsp;&nbsp;&nbsp;&nbsp;CustomTransformation(),
&nbsp;&nbsp;&nbsp;&nbsp;TokenTextSplitter(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;separator=" ",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chunk_size=512,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chunk_overlap=128
&nbsp;&nbsp;&nbsp;&nbsp;),
&nbsp;&nbsp;&nbsp;&nbsp;SummaryExtractor(),
&nbsp;&nbsp;&nbsp;&nbsp;QuestionsAnsweredExtractor(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;questions=3
&nbsp;&nbsp;&nbsp;&nbsp;)
]</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In conclusion, an ingestion pipeline is a super-efficient way to get your data automatically <a id="_idIndexMarker350"></a>prepped and polished by running it through customizable sets of transformations until it’s just right for your app <span class="No-Break">or database.</span></p>
			<p>As we build up the PITS tutoring app, we’ll leverage ingestion pipelines and you’ll get the opportunity to experiment more with <span class="No-Break">this concept.</span></p>
			<p>Next, let’s talk about more <span class="No-Break">complex scenarios.</span></p>
			<h2 id="f_7__idParaDest-102" data-type="sect1" class="sect1" title2="Handling documents that contain a mix of text and tabular data" no2="4.10"><a id="_idTextAnchor101"></a>4.10. Handling documents that contain a mix of text and tabular data</h2>
			<p>Data is not always simple. Many real-world documents, such as research papers, financial reports, and others, contain a mix of unstructured text, as well as structured tabular data <a id="_idIndexMarker351"></a>in tables. Ingesting such heterogeneous <a id="_idIndexMarker352"></a>documents presents an additional challenge - we need to not only extract text but also identify, parse, and process tables embedded within the text. Because, sometimes you get tables, sometimes you get text and sometimes you have to deal with a mix <span class="No-Break">of both.</span></p>
			<p>LlamaIndex provides <strong class="source-inline">UnstructuredElementNodeParser</strong> to tackle such documents containing both free-form text as well as tables and other structured elements. It leverages the <strong class="source-inline">Unstructured</strong> library to analyze the document layout and delineate text sections <span class="No-Break">from tables.</span></p>
			<p>This parser works exclusively on HTML files and can extract two types <span class="No-Break">of nodes:</span></p>
			<ul>
				<li><strong class="bold">Text nodes</strong>: Containing <a id="_idIndexMarker353"></a>the <span class="No-Break">text </span><span class="No-Break"><a id="_idIndexMarker354"></a></span><span class="No-Break">chunks</span></li>
				<li><strong class="bold">Table nodes</strong>: Containing <a id="_idIndexMarker355"></a>the table data and metadata, such <a id="_idIndexMarker356"></a><span class="No-Break">as coordinates</span></li>
			</ul>
			<p>Storing these elements as separate nodes allows more modular and meaningful processing later in the RAG workflow. The text can be indexed and searched normally with elements like keywords. The tables can be loaded into a <strong class="bold">pandas DataFrame</strong> or any structured database for SQL-based access. So, in complex cases involving mixed data types, leveraging <strong class="source-inline">UnstructuredElementNodeParser</strong> before ingestion enables better <span class="No-Break">data organization.</span></p>
			<p>You can find <a id="_idIndexMarker357"></a>a complete demo for using <strong class="source-inline">UnstructuredElementNodeParser</strong> in the official LlamaIndex <span class="No-Break">documentation: </span><span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/query_engine/sec_tables/tesla_10q_table.html</span><span class="No-Break">.</span></p>
			<p>With these new concepts in our toolbox, let’s continue building our <span class="No-Break">tutoring project.</span></p>
			<h2 id="f_7__idParaDest-103" data-type="sect1" class="sect1" title2="Hands-on – ingesting study materials into our PITS" no2="4.11"><a id="_idTextAnchor102"></a>4.11. Hands-on – ingesting study materials into our PITS</h2>
			<p>It’s time for some practice. We now have everything we need to continue building our project. Let’s write the <span class="No-Break"><strong class="source-inline">documend_uploader.py</strong></span><span class="No-Break"> module.</span></p>
			<p>This module <a id="_idIndexMarker358"></a>will take care of ingesting and preparing our available study material. The user can upload any available books, technical documentation, or existing articles to provide more context to <span class="No-Break">our tutor.</span></p>
			<ol>
				<li>First, we have <span class="No-Break">the imports:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_88" title2="(no caption)" no2="">from global_settings import STORAGE_PATH, CACHE_FILE
from logging_functions import log_action
from llama_index import SimpleDirectoryReader, VectorStoreIndex
from llama_index.ingestion import IngestionPipeline, IngestionCache
from llama_index.text_splitter import TokenTextSplitter
from llama_index.extractors import SummaryExtractor
from llama_index.embeddings import OpenAIEmbedding</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>				<li>Next, we must define the main function that’s responsible for handling the ingestion process. You’ll notice that it uses an ingestion pipeline to both streamline the <a id="_idIndexMarker359"></a>code but also benefit <span class="No-Break">from caching:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_89" title2="(no caption)" no2="">def ingest_documents():
&nbsp;&nbsp;&nbsp;&nbsp;documents = SimpleDirectoryReader(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;STORAGE_PATH,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;filename_as_id = True
&nbsp;&nbsp;&nbsp;&nbsp;).load_data()
&nbsp;&nbsp;&nbsp;&nbsp;for doc in documents:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(doc.id_)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;log_action(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f"File '{doc.id_}' uploaded user",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;action_type="UPLOAD"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div><ul><li>The function loads all readable documents available in <strong class="source-inline">STORAGE_PATH</strong>, which was defined <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">global_settings.py</strong></span><span class="No-Break">.</span></li><li>For each document processed, a new event is stored in our log file using <strong class="source-inline">log_action</strong> <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">logging_functions.py</strong></span><span class="No-Break">.</span></li></ul></li>				<li>Next, the function checks if there’s already cached pipeline data <span class="No-Break">to use:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_90" title2="(no caption)" no2="">&nbsp;&nbsp;&nbsp;&nbsp;try:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cached_hashes = IngestionCache.from_persist_path(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CACHE_FILE
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("Cache file found. Running using cache...")
&nbsp;&nbsp;&nbsp;&nbsp;except:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cached_hashes = ""
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("No cache file found. Running without...")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>				<li>The next step <a id="_idIndexMarker360"></a>is to define and run the pipeline. If hashes from the cache file correspond, no operations should be processed; instead, the values will be directly loaded from <span class="No-Break">the cache:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_91" title2="(no caption)" no2="">&nbsp;&nbsp;&nbsp;&nbsp;pipeline = IngestionPipeline(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;transformations=[
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TokenTextSplitter(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chunk_size=1024,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chunk_overlap=20
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SummaryExtractor(summaries=['self']),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;OpenAIEmbedding()
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cache=cached_hashes
&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;nodes = pipeline.run(documents=documents)
&nbsp;&nbsp;&nbsp;&nbsp;pipeline.cache.persist(CACHE_FILE)
&nbsp;&nbsp;&nbsp;&nbsp;return nodes</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div><p class="list-inset">We run three transformations in <span class="No-Break">the pipeline:</span></p><ol><li class="upper-roman">Basic chunking <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">TokenTextSplitter</strong></span><span class="No-Break">.</span></li><li class="upper-roman">A metadata extractor that summarizes <span class="No-Break">each node.</span></li><li class="upper-roman">Embedding generation using <strong class="source-inline">OpenAIEmbedding</strong>. Don’t worry about this step for now. I will explain it thoroughly in <a href="#_idTextAnchor104"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Indexing </em><span class="No-Break"><em class="italic">with LlamaIndex</em></span><span class="No-Break">.</span></li></ol></li>				<li>In the end, the function <a id="_idIndexMarker361"></a>saves the current data in the cache file and returns the <span class="No-Break">processed nodes.</span></li>
			</ol>
			<p>That’s it for now. We have now uploaded and prepared the study materials for future processing. We’ll continue with the indexing part in the <span class="No-Break">next chapter.</span></p>
			<h2 id="f_7__idParaDest-104" data-type="sect1" class="sect1" title2="Summary" no2="4.12"><a id="_idTextAnchor103"></a>4.12. Summary</h2>
			<p>LlamaHub offers a variety of pre-built data loaders, streamlining the process of importing data from various sources as documents. This eliminates the need for creating unique parsers for different <span class="No-Break">data formats.</span></p>
			<p>After data is imported, it undergoes further processing into nodes, and we discussed various customization <span class="No-Break">options available.</span></p>
			<p>There’s a broad range of options for metadata extraction, and the parsing process can be tailored to meet <span class="No-Break">specific requirements.</span></p>
			<p>Developing pipelines for data ingestion is an invaluable tool for enhancing the efficiency, both in terms of cost and time, of our RAG applications. It’s also vital to keep privacy considerations <span class="No-Break">in mind.</span></p>
			<p>With data ingestion completed, let’s continue our journey and discover the indexing powers <span class="No-Break">of LlamaIndex.</span></p>
		</div>
<div id="f_8__idContainer052" data-type="chapter" class="chapter" file="B21861_05_xhtml" title2="Indexing with LlamaIndex" no2="5">
			<h1 id="f_8__idParaDest-105" class="chapter-number"><a id="_idTextAnchor104"></a>5</h1>
			<h1 class="H1---Chapter" id="f_8__idParaDest-106"><a id="_idTextAnchor105"></a>Indexing with LlamaIndex</h1>
<p style="font-style: italic;">As this ebook edition doesn't have fixed pagination, the page numbers below are hyperlinked for reference only, based on the printed edition of this book.</p>
			<p>This chapter provides an in-depth look at the different types of indexes available in LlamaIndex. It explains how indexes work, and their key capabilities, customization options, underlying architectures, and use cases. Overall, this chapter serves as a guide for leveraging the indexing functionality within LlamaIndex to build performant and scalable RAG systems. Let’s <span class="No-Break">get started!</span></p>
			<p>Throughout this chapter, we’ll cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Indexing data – a <span class="No-Break">bird’s-eye view</span></li>
				<li>Understanding the Vector <span class="No-Break">Store Index</span></li>
				<li><span class="No-Break">Understanding embeddings</span></li>
				<li>Persisting and <span class="No-Break">re-using Indexes</span></li>
				<li>Exploring other Index types <span class="No-Break">in LlamaIndex</span></li>
				<li>Building Indexes on top of other Indexes <span class="No-Break">with ComposableGraph</span></li>
				<li>Estimating the potential cost of building and <span class="No-Break">querying Indexes</span></li>
			</ul>
			<h2 id="f_8__idParaDest-107" data-type="sect1" class="sect1" title2="Technical requirements" no2="5.1"><a id="_idTextAnchor106"></a>5.1. Technical requirements</h2>
			<p>For this chapter, you will need to install the following package in <span class="No-Break">your environment:</span></p>
			<ul>
				<li><span class="No-Break"><em class="italic">ChromaDB</em></span><span class="No-Break">: </span><a href="https://www.trychroma.com/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://www.trychroma.com/</span></a></li>
			</ul>
			<p>In addition, there are two integration packages that will be required by the <span class="No-Break">sample code:</span></p>
			<ul>
				<li><em class="italic">Chroma Vector </em><span class="No-Break"><em class="italic">Store</em></span><span class="No-Break">: </span><a href="https://pypi.org/project/llama-index-vector-stores-chroma/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pypi.org/project/llama-index-vector-stores-chroma/</span></a></li>
				<li><em class="italic">Hugging Face </em><span class="No-Break"><em class="italic">embeddings</em></span><span class="No-Break">: </span><a href="https://pypi.org/project/llama-index-embeddings-huggingface/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pypi.org/project/llama-index-embeddings-huggingface/</span></a></li>
			</ul>
			<p>All code samples from this chapter can be found in the <strong class="source-inline">ch5</strong> subfolder of the book’s GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex</span></a></p>
			<h2 id="f_8__idParaDest-108" data-type="sect1" class="sect1" title2="Indexing data – a bird’s-eye view" no2="5.2"><a id="_idTextAnchor107"></a>5.2. Indexing data – a bird’s-eye view</h2>
			<p>We briefly discussed the importance <a id="_idIndexMarker362"></a>and general functioning of Indexes in a RAG application in <a href="#_idTextAnchor045"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, in the section titled <em class="italic">Uncovering the essential building blocks of LlamaIndex – documents, nodes, indexes</em>. Now, it is time to have a closer look at the different indexing methods available in LlamaIndex with their advantages, disadvantages, and specific <span class="No-Break">use cases.</span></p>
			<p>In principle, data can be accessed even without an Index. But it’s like reading a book without a table of contents. As long as it’s about a story that has continuity and can be read sequentially, section by section, and chapter by chapter, reading will be a pleasure. However, things change when we need to quickly search for a specific topic in that book. Without a table of contents, the search process will be slow <span class="No-Break">and cumbersome.</span></p>
			<p>In LlamaIndex, however, <strong class="bold">Indexes</strong> represent more than just<a id="_idIndexMarker363"></a> a simple table of contents. An Index provides not only the necessary structure for navigation but also the concrete mechanisms to update or access it. That includes the logic for the <strong class="bold">retrievers</strong> and the mechanisms used for fetching<a id="_idIndexMarker364"></a> data, which we will discuss in detail during <a href="#_idTextAnchor131"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>,<em class="italic"> Querying Our Data, Part 1 – </em><span class="No-Break"><em class="italic">Context Retrieval</em></span><span class="No-Break">.</span></p>
			<p>In this book, I’ve kept things simple, giving you the basics of how Indexes work and providing some examples to help you understand their usage. Exploring every possible way to use and mix these Indexes would be a huge task and that’s not what we’re <span class="No-Break">about here.</span></p>
			<p>We’ll talk later about what makes each type of Index unique, but first, let’s see what they all have <span class="No-Break">in common.</span></p>
			<h3 id="f_8__idParaDest-109" data-type="sect2" class="sect2" title2="Common features of all Index types" no2="5.2.1"><a id="_idTextAnchor108"></a>5.2.1. Common features of all Index types</h3>
			<p>Each type of Index in LlamaIndex has its own characteristics and functions, but because all of them inherit the <strong class="source-inline">BaseIndex</strong> class, there are certain features and parameters they share, which can be customized<a id="_idIndexMarker365"></a> for any kind <span class="No-Break">of Index:</span></p>
			<ul>
				<li><strong class="bold">Nodes</strong>: All Indexes are based on nodes and we can choose which Nodes are included in the Index. Plus, all Index types provide methods to insert new Nodes or delete existing ones, allowing for dynamic updates to the Index as your data changes. We can either build an Index from pre-existing Nodes by providing the Nodes directly to the Index constructor like this <strong class="source-inline">vector_index = VectorStoreIndex(nodes)</strong> or we can provide a list of documents as an input using <strong class="source-inline">from_documents()</strong> and let the Index extract the Nodes by itself. Keep in mind that we can use <strong class="source-inline">Settings</strong> – before actually building the Index – to customize its underlying mechanics. As we discussed during <a href="#_idTextAnchor045"><span class="No-Break"><em class="italic">Chapter 3</em></span></a> in the <em class="italic">Understanding how settings can be used for customization</em> section, this simple class allows for different settings such as changing the LLM, embedding model, or default Node parser used by <span class="No-Break">an Index.</span></li>
				<li><strong class="bold">The storage context</strong>: The storage context defines how and where the data (documents and nodes) for the Index is stored. This customization is crucial for managing data storage efficiently, depending on the <span class="No-Break">application’s requirements.</span></li>
				<li><strong class="bold">Progress display</strong>: The <strong class="source-inline">show_progress</strong> option lets us choose whether to display progress bars during long-running operations such as building the Index. Implemented using the <strong class="source-inline">tqdm</strong> Python library, this feature can be useful for monitoring the progress of large <span class="No-Break">indexing tasks.</span></li>
				<li><strong class="bold">Different retrieval modes</strong>: Each Index allows for different pre-defined retrieval modes, which can be set to match the specific needs of your application. And you can also customize or extend the Retriever classes to change how queries are processed and how results are retrieved from the Index. More on that during <a href="#_idTextAnchor131"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Querying Our Data, Part 1 – </em><span class="No-Break"><em class="italic">Context Retrieval.</em></span></li>
				<li><strong class="bold">Asynchronous operations</strong>: The <strong class="source-inline">use_async</strong> parameter implemented by some of the Indexes determines whether certain operations should be performed asynchronously. Asynchronous processing allows the system to manage multiple operations concurrently, rather than waiting for each operation to be completed sequentially. This can be important for performance optimization, especially<a id="_idIndexMarker366"></a> when dealing with large datasets or <span class="No-Break">complex operations.</span></li>
			</ul>
			<p class="callout-heading">Quick note</p>
			<p class="callout">An important thing to consider before diving further and starting to tinker with the sample code is that indexing often relies on LLM calls for summarizing or embedding purposes. Just like in the case of metadata extraction, which we covered in <a href="#_idTextAnchor068"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Ingesting Data into Our RAG Workflow</em>, indexing in LlamaIndex may also raise cost and privacy concerns. Make sure you read the cost-related section at the end of this chapter before running any large-scale experiments to test <span class="No-Break">your ideas.</span></p>
			<p>Let’s start with our first and most frequently used <span class="No-Break">Index type.</span></p>
			<h2 class="sect1" id="f_8__idParaDest-110" data-type="sect1" title2="Understanding the VectorStoreIndex" no2="5.3"><a id="_idTextAnchor109"></a>5.3. Understanding the VectorStoreIndex</h2>
<p style="font-style: italic;">As this ebook edition doesn't have fixed pagination, the page numbers below are hyperlinked for reference only, based on the printed edition of this book.</p>
			<p>In LlamaIndex, the <strong class="source-inline">VectorStoreIndex</strong> stands out<a id="_idIndexMarker367"></a> as the workhorse, being the most commonly utilized type <span class="No-Break">of Index.</span></p>
			<p>For most RAG applications, a <strong class="source-inline">VectorStoreIndex</strong> might be the best solution because it facilitates the construction<a id="_idIndexMarker368"></a> of Indexes on collections of Documents where <strong class="bold">embeddings</strong> for the input text chunks<a id="_idIndexMarker369"></a> are stored within the <strong class="bold">Vector Store</strong> of the Index. Once constructed, this Index can be used for efficient<a id="_idIndexMarker370"></a> querying because it allows for <strong class="bold">similarity searches</strong> over the embedded representations of the text, making it highly suitable for applications requiring fast retrieval of relevant information from a large collection of data. Don’t worry if you’re not yet familiar with terms such as embeddings, vector store, or similarity searching, because we’ll cover them in the following sections. The <strong class="source-inline">VectorStoreIndex</strong> class in LlamaIndex supports these operations by default and also allows for asynchronous calls and progress tracking, which can improve performance and user experience<a id="_idIndexMarker371"></a> in typical <span class="No-Break">RAG scenarios.</span></p>
			<h3 id="f_8__idParaDest-111" data-type="sect2" class="sect2" title2="A simple usage example for the VectorStoreIndex" no2="5.3.1"><a id="_idTextAnchor110"></a>5.3.1. A simple usage example for the VectorStoreIndex</h3>
			<p>Here’s the most basic <a id="_idIndexMarker372"></a>way of constructing <span class="No-Break">a </span><span class="No-Break"><strong class="source-inline">VectorStoreIndex</strong></span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_92" title2="(no caption)" no2="">from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
documents = SimpleDirectoryReader("files").load_data()
index = VectorStoreIndex.from_documents(documents)
print("Index created successfully!")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>As you can see, in just a few lines of code, we have ingested the Documents and the <strong class="source-inline">VectorStoreIndex</strong> took care of everything. Note that using this approach, we have skipped the Node parsing step entirely, because the Index did that by itself by using <span class="No-Break">the </span><span class="No-Break"><strong class="source-inline">from_documents()</strong></span><span class="No-Break">method.</span></p>
			<p>There are several parameters<a id="_idIndexMarker373"></a> that we can customize for <span class="No-Break">the </span><span class="No-Break"><strong class="source-inline">VectorStoreIndex</strong></span><span class="No-Break">:</span></p>
			<ul>
				<li><strong class="source-inline">use_async</strong>: This parameter enables asynchronous calls. By default, it is set <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">False</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">show_progress</strong>: This parameter shows progress bars during Index construction. The default value <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">False</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">store_nodes_override</strong>: This parameter forces LlamaIndex to store Node objects in the Index store and document store, even if the vector store keeps text. This can be useful in scenarios where you need direct access to Node objects, even if their content is already stored in the vector store. We’ll talk in more detail about the Index store, document store, and vector store later in this chapter. The default setting for this parameter <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">False</strong></span><span class="No-Break">.</span></li>
			</ul>
			<p>Let’s have a look at <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.1</em> for a visual representation of this type <span class="No-Break">of Index:</span></p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/B21861_05_1.jpg" alt="Figure 5.1 – The structure of a VectorStoreIndex" width="874" height="522" data-type="figure" id="untitled_figure_23" title2="– The structure of a VectorStoreIndex" no2="5.1">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – The structure of a VectorStoreIndex</p>
			<p>The <strong class="source-inline">VectorStoreIndex</strong> took the ingested Documents, breaking them down into Nodes. It used the default parameters for text splitter, chunk size, chunk overlap, and so on. Of course, we could have customized all these parameters if we <span class="No-Break">wanted to.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout"><strong class="bold">Fixed-size chunking</strong> simply splits text into same-sized<a id="_idIndexMarker374"></a> chunks, optionally with some overlap. Although computationally cheap and simple to implement, this simple chunking may not always be the best approach. Performance testing various chunk sizes is key to optimizing for an application’s <span class="No-Break">particular needs.</span></p>
			<p>The nodes containing chunks<a id="_idIndexMarker375"></a> of the original text were then <em class="italic">embedded</em> into a high-dimensional vector space using a language model. The embedded vectors were stored within the vector store component of the Index. Next, when a query is made, the query text will be similarly embedded and compared against<a id="_idIndexMarker376"></a> the stored vectors using a <strong class="bold">similarity measure</strong> identified with a method called <strong class="bold">cosine similarity</strong>. The most similar <a id="_idIndexMarker377"></a>vectors – and thus the most relevant document chunks – will be returned as the query result. This process enables rapid, semantically aware retrieval of information, leveraging the mathematical properties of vector spaces to find the documents that best answer the <span class="No-Break">user’s query.</span></p>
			<p>Sounds a bit confusing? Let’s go through these concepts together in the <span class="No-Break">next section.</span></p>
			<h3 id="f_8__idParaDest-112" data-type="sect2" class="sect2" title2="Understanding embeddings" no2="5.3.2"><a id="_idTextAnchor111"></a>5.3.2. Understanding embeddings</h3>
			<p>In simple terms, <strong class="bold">vector embeddings</strong> represent a machine-understandable<a id="_idIndexMarker378"></a> data format. They capture meaning and may conceptually represent a word, an entire document, or even non-textual information such as images and sounds. In a way, embeddings represent a standard language of thought for an LLM. In the context of an LLM, they serve as a foundational representation through which the model understands and processes information. They transform diverse and complex data into a uniform, high-dimensional space where the LLM can perform operations such as comparison, association, and prediction more effectively. <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.2</em> provides an illustration of the process of <span class="No-Break">embedding data:</span></p>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/B21861_05_2.jpg" alt="Figure 5.2 – How an embedding model converts data into numerical representations" width="1650" height="478" data-type="figure" id="untitled_figure_24" title2="– How an embedding model converts data into numerical representations" no2="5.2">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – How an embedding model converts data into numerical representations</p>
			<p>Because it’s all about math under the hood. And math works well with numbers – more precisely, large lists of floating-point numbers, where each number represents a dimension in a hypothetical vector space. The LLM can work with these arrays of numbers to understand, interpret, and generate responses based on the input it receives. Essentially, these numbers in the vector embeddings allow the LLM to <em class="italic">see</em> and <em class="italic">think</em> about the data in a way that’s meaningful <span class="No-Break">and structured.</span></p>
			<p>The beauty of this system lies in its ability to handle ambiguity and complexity. The model can understand semantic relationships between words, such as synonyms, antonyms, and more complex linguistic patterns. In the case of polysemous words, the same word can have different meanings in different contexts. For example, the word <em class="italic">bank</em> can refer to the side of a river or a financial institution. Vector embeddings help the LLM understand these nuances by providing context-sensitive representations. So, in one situation, <em class="italic">bank</em> might be closely associated with words such as <em class="italic">river</em> and <em class="italic">shore</em>, while in another, it’s more closely linked to <em class="italic">money</em> <span class="No-Break">and </span><span class="No-Break"><em class="italic">account</em></span><span class="No-Break">.</span></p>
			<p class="callout-heading">Quick note</p>
			<p class="callout">An important factor to consider is that the size of text chunks being embedded impacts precision – too small and context is lost; too large and all that additional detail may dilute <span class="No-Break">the meaning.</span></p>
			<p>In case you’re not very familiar with embeddings yet, the following example could be useful to get a better grasp of the concept. Let’s assign some <em class="italic">arbitrary</em> vector embeddings to three randomly <span class="No-Break">chosen sentences:</span></p>
			<ul>
				<li><strong class="bold">Sentence 1</strong>: The quick brown fox jumps over the <span class="No-Break">lazy dog</span></li>
				<li><strong class="bold">Sentence 2</strong>: A fast dark-colored fox leaps above a <span class="No-Break">sleepy canine</span></li>
				<li><strong class="bold">Sentence 3</strong>: Apples are sweet <span class="No-Break">and crunchy</span></li>
			</ul>
			<p>In a real-world scenario, the embeddings associated with each of these sentences<a id="_idIndexMarker379"></a> would be calculated automatically by using an <strong class="bold">embedding model</strong>. This is a specialized artificial intelligence model used to convert complex data such as text, images, or graphs into a numerical format. The embeddings would also normally be high-dimensional, but for the sake of explanation, I’ll use simple, three-dimensional, arbitrarily chosen vectors. Here are the hypothetical embeddings for the <span class="No-Break">three sentences:</span></p>
			<ul>
				<li><strong class="bold">Sentence 1 Embedding</strong>: [0.8, <span class="No-Break">0.1, 0.3]</span></li>
				<li><strong class="bold">Sentence 2 Embedding</strong>: [0.79, <span class="No-Break">0.14, 0.32]</span></li>
				<li><strong class="bold">Sentence 3 Embedding</strong>: [0.2, <span class="No-Break">0.9, 0.5]</span></li>
			</ul>
			<p>These numbers are purely<a id="_idIndexMarker380"></a> conceptual and are meant to show that sentences 1 and 2, which have similar meanings, have embeddings that are closer to each other in vector space. <em class="italic">Sentence 3</em>, which has a different meaning, has an embedding that is farther away from the first two. Have a look at <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.3</em> for a straightforward visual comparison of the <span class="No-Break">three embeddings:</span></p>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/B21861_05_3.jpg" alt="Figure 5.3 – A comparison of the three embedded sentences in a 3D space" width="735" height="642" data-type="figure" id="untitled_figure_25" title2="– A comparison of the three embedded sentences in a 3D space" no2="5.3">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – A comparison of the three embedded sentences in a 3D space</p>
			<p>When we visualize them in a three-dimensional space, sentences 1 and 2 are plotted near each other, while sentence 3 will be plotted at a distance. This spatial representation is what allows machine learning models to determine <span class="No-Break">semantic similarity.</span></p>
			<p>When you search using a query on a vector store Index in order to retrieve useful context, LlamaIndex converts your search terms into a similar embedding and then finds the closest matches among the pre-computed embeddings of your <span class="No-Break">text chunks.</span></p>
			<p>We call this process <strong class="bold">similarity</strong> or <strong class="bold">distance search</strong>. So, when you encounter<a id="_idIndexMarker381"></a> the term <strong class="bold">top-k similarity search</strong>, you should know that it relies <a id="_idIndexMarker382"></a>on an algorithm<a id="_idIndexMarker383"></a> that calculates<a id="_idIndexMarker384"></a> the similarity between vector embeddings. It takes a vector embedding as an input and returns the most similar <em class="italic">k</em> number of vectors found in the vector store. Because the initial vector and the <em class="italic">top-k</em> returned neighbors are similar to each other, we can consider their meanings to be conceptually similar. Now you understand why<a id="_idIndexMarker385"></a> I have previously called embeddings a <em class="italic">standard language of thought</em> for an LLM. It doesn’t really matter anymore whether they represent text, images, or any other types of information. We measure their similarity <span class="No-Break">in numbers.</span></p>
			<p>The only thing that may be implemented differently, depending on our use case, is the actual formula for defining<a id="_idIndexMarker386"></a> that distance <span class="No-Break">or similarity.</span></p>
			<p>Spoiler alert: a bit of mathematical concepts <span class="No-Break">up next.</span></p>
			<h3 id="f_8__idParaDest-113" data-type="sect2" class="sect2" title2="Understanding similarity search" no2="5.3.3"><a id="_idTextAnchor112"></a>5.3.3. Understanding similarity search</h3>
			<p>In the realms of machine learning<a id="_idIndexMarker387"></a> and deep learning, the concept of similarity search is very important. It forms the backbone of many applications, from recommendation systems and information retrieval to clustering and classification tasks. As models and systems interact with high-dimensional data, identifying patterns and relationships between data points becomes essential. This involves measuring how <em class="italic">close</em> or <em class="italic">similar</em> data elements are, a task that often takes place in a vector space where each item is represented as <span class="No-Break">a vector.</span></p>
			<p>Locating points in this space that are near each other enables machines to assess similarity and, by extension, to make decisions, draw inferences, or, in our case, retrieve information based on that assessment of closeness. With the advent of embeddings in deep learning, the need for effective similarity search has grown. As embeddings capture the semantic meaning of the data they represent, performing similarity searches on these vectors allows machines to understand content at a level approaching <span class="No-Break">human cognition.</span></p>
			<p>Let’s explore the methods that LlamaIndex currently employs to measure the similarity between vectors, each with its unique advantages <span class="No-Break">and applicability.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Cosine similarity" no2="5.3.3.1">5.3.3.1. Cosine similarity</h4>
			<p>This method measures<a id="_idIndexMarker388"></a> the cosine<a id="_idIndexMarker389"></a> of <em class="italic">the angle</em> between two vectors. Imagine two arrows pointing in different directions; the smaller the angle between them, the more similar <span class="No-Break">they are.</span></p>
			<p>Have a look at <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.4</em>, which depicts a cosine similarity comparison between <span class="No-Break">two vectors:</span></p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/B21861_05_4.jpg" alt="Figure 5.4 – How a cosine similarity comparison would look" width="999" height="983" data-type="figure" id="untitled_figure_26" title2="– How a cosine similarity comparison would look" no2="5.4">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – How a cosine similarity comparison would look</p>
			<p>In terms of embeddings, a small angle (or a high cosine similarity score, close to 1) indicates that the content they represent is similar. This method is particularly useful in text analysis because it is less affected by the length of the documents and focuses more on their direction<a id="_idIndexMarker390"></a> or orientation<a id="_idIndexMarker391"></a> in the <span class="No-Break">vector space.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Cosine similarity is also the default method used by LlamaIndex for calculating similarity <span class="No-Break">between embeddings.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Dot product" no2="5.3.3.2">5.3.3.2. Dot product</h4>
			<p>Also called the <strong class="bold">scalar product</strong>, because it is represented<a id="_idIndexMarker392"></a> by a single value, this<a id="_idIndexMarker393"></a> is another method<a id="_idIndexMarker394"></a> of calculating how well two vectors align with each other. To calculate the scalar product of two vectors, the algorithm multiplies the corresponding elements of the vectors and then sums <span class="No-Break">these products.</span></p>
			<p>Let’s take a simple example of <em class="italic">vector A</em>: [2,3] and <em class="italic">vector B</em>: [4,1]. The <strong class="bold">dot product</strong> is calculated by multiplying their corresponding elements: (2×4) + (3×1), which gives us 8 + 3 = 11. Thus, the dot product of these two vectors <span class="No-Break">is 11.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.5</em> exemplifies <span class="No-Break">this concept:</span></p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B21861_05_5.jpg" alt="Figure 5.5 – Calculating similarity using the dot product method" width="1235" height="989" data-type="figure" id="untitled_figure_27" title2="– Calculating similarity using the dot product method" no2="5.5">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – Calculating similarity using the dot product method</p>
			<p>In the preceding diagram, the dot product is visualized by projecting one vector onto the other. This projection illustrates the geometric interpretation of the dot product. It’s calculated by projecting the components of one vector in the direction of the other and then multiplying these projected components by the corresponding components of the second vector. The sum of these products gives us the dot product. This visualization helps us understand that the dot product is not just a measure of how vectors point in the same direction; it also incorporates <span class="No-Break">their lengths.</span></p>
			<p>Higher values of the dot product mean higher similarities between vectors. In contrast with the cosine method, the dot<a id="_idIndexMarker395"></a> product is sensitive both to the length<a id="_idIndexMarker396"></a> of the two vectors compared and their relative direction. Unlike the dot product, cosine similarity normalizes the dot product by the magnitudes of the vectors. This normalization makes cosine similarity solely a measure of the directional alignment between vectors, independent of <span class="No-Break">their lengths.</span></p>
			<p>The longer the vectors, the higher the result, and this is an important thing to consider in a RAG scenario. Longer vectors, which might represent longer documents or more detailed information, could dominate the retrieved results due to their inherently larger dot product values. This could bias the system toward retrieving longer<a id="_idIndexMarker397"></a> documents, even if they<a id="_idIndexMarker398"></a> are not the <span class="No-Break">most relevant.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Euclidean distance" no2="5.3.3.3">5.3.3.3. Euclidean distance</h4>
			<p>This method is different<a id="_idIndexMarker399"></a> from the dot product and cosine similarity<a id="_idIndexMarker400"></a> methods. While those methods look at the angle or alignment between vectors, <strong class="bold">Euclidean distance</strong> is all about how close the actual values of the vectors are to each other. This can be especially useful when the values in the vectors represent actual counts or measurements, especially where the vector dimensions have real-world <span class="No-Break">physical interpretations.</span></p>
			<p>Take a look at <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.6</em> for a visual representation of <span class="No-Break">Euclidean distance:</span></p>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/B21861_05_6.jpg" alt="Figure 5.6 – The Euclidean distance between two vectors" width="1080" height="865" data-type="figure" id="untitled_figure_28" title2="– The Euclidean distance between two vectors" no2="5.6">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6 – The Euclidean distance between two vectors</p>
			<p>You should now have a foundational understanding of embeddings, how vector similarity works, and, in particular, how they are implemented in LlamaIndex. If you want to familiarize yourself better with this concept, you can find more information on <span class="No-Break">the web.</span></p>
			<p>Here are some<a id="_idIndexMarker401"></a> suggested<a id="_idIndexMarker402"></a> additional reading resources<a id="_idIndexMarker403"></a> you could start <span class="No-Break">with: </span><a href="https://developers.google.com/machine-learning/clustering/similarity/measuring-similarity" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://developers.google.com/machine-learning/clustering/similarity/measuring-similarity</span></a></p>
			<h3 id="f_8__idParaDest-114" data-type="sect2" class="sect2" title2="OK, but how does LlamaIndex generate these embeddings?" no2="5.3.4"><a id="_idTextAnchor113"></a>5.3.4. OK, but how does LlamaIndex generate these embeddings?</h3>
			<p>The short answer is, <em class="italic">however, you prefer</em>. By default, the framework<a id="_idIndexMarker404"></a> is configured to rely on OpenAI’s <strong class="source-inline">text-embedding-ada-002</strong> model. This model has been trained to produce embeddings that effectively capture semantic meanings of the text, enabling applications such as semantic search, topic clustering, anomaly detection, and others. It provides a very good balance between quality, performance, and cost. LlamaIndex uses this model by default to embed documents during Index construction as well as for <span class="No-Break">query embeddings.</span></p>
			<p>Sometimes, though, when you may want to index large volumes of data, the cost associated with a hosted model such as this one may be too high for your budget. In other instances, you might be concerned about the privacy of your proprietary data and prefer to use a local model instead. Or maybe, in some cases, you may want to use more specialized models for a particular topic or <span class="No-Break">technical domain.</span></p>
			<p>The great news is that LlamaIndex also supports a variety of other embedding models. For example, if you wish to use local models, you can set the service context to use a local embedding, which uses a well-balanced<a id="_idIndexMarker405"></a> default model provided by <em class="italic">Hugging Face</em> (<a href="https://huggingface.co/BAAI/bge-small-en-v1.5" target="_blank" rel="noopener noreferrer">https://huggingface.co/BAAI/bge-small-en-v1.5</a>). This can be particularly useful if you aim to reduce costs or have requirements to process <span class="No-Break">data locally.</span></p>
			<h4 data-type="sect3" class="sect3" title2="A brief introduction to Hugging Face" no2="5.3.4.1">5.3.4.1. A brief introduction to Hugging Face</h4>
			<p><strong class="bold">Hugging Face</strong> is a very important resource<a id="_idIndexMarker406"></a> in the AI field, primarily known<a id="_idIndexMarker407"></a> for its extensive collection<a id="_idIndexMarker408"></a> of pre-trained machine learning models, especially in <strong class="bold">natural</strong> <strong class="bold">language processing</strong> (<strong class="bold">NLP</strong>). Its importance lies in democratizing access to state-of-the-art AI models, tools, and techniques, enabling developers and researchers to implement advanced AI functionalities<a id="_idIndexMarker409"></a> with relative ease. Similar to GitHub, Hugging Face embraces<a id="_idIndexMarker410"></a> a community-driven approach, where users can share, collaborate on, and improve AI models, much like developers share and contribute to code repositories on GitHub. This community-centric model accelerates innovation and dissemination of <span class="No-Break">AI advancements.</span></p>
			<p>Before running the next sample, make sure you install the <span class="No-Break">necessary integration:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_93" title2="(no caption)" no2="">pip install llama-index-embeddings-huggingface</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This example will show you how to set up a local <span class="No-Break">embedding model:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_94" title2="(no caption)" no2="">from llama_index.embeddings.huggingface import HuggingFaceEmbedding
embedding_model = HuggingFaceEmbedding(
&nbsp;&nbsp;&nbsp;&nbsp;model_name="WhereIsAI/UAE-Large-V1"
)
embeddings = embedding_model.get_text_embedding(
&nbsp;&nbsp;&nbsp;&nbsp;"The quick brown fox jumps over the lazy cat!"
)
print(embeddings[:15])</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>On the first run, the code will download the <strong class="source-inline">Universal AnglE Embedding</strong> model from Hugging Face. This is one of the best-performing embedding models at the moment, offering great overall performance and <span class="No-Break">quality balance.</span></p>
			<p>More info is available <span class="No-Break">here: </span><a href="https://huggingface.co/WhereIsAI/UAE-Large-V1" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://huggingface.co/WhereIsAI/UAE-Large-V1</span></a><span class="No-Break">.</span></p>
			<p>After downloading and initializing the embedding model, the script calculates the embeddings for the sentence and displays the first 15 values of <span class="No-Break">the vector.</span></p>
			<p>For advanced users or specific applications, LlamaIndex makes it easy to integrate custom embedding models. You can simply extend the <strong class="source-inline">BaseEmbedding</strong> class provided by LlamaIndex and implement your own logic for <span class="No-Break">generating embeddings.</span></p>
			<p>Here, you can find<a id="_idIndexMarker411"></a> an example of how to define your custom embedding <span class="No-Break">class: </span><a href="https://docs.llamaindex.ai/en/stable/examples/embeddings/custom_embeddings.html" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/embeddings/custom_embeddings.html</span></a><span class="No-Break">.</span></p>
			<p>Apart from OpenAI and local<a id="_idIndexMarker412"></a> models, there are integrations with Langchain, enabling<a id="_idIndexMarker413"></a> you to use any embedding model they offer. You also have the option to use embedding models from Azure, CohereAI, and other providers through additional integrations offered by LlamaIndex. This great flexibility ensures that no matter your needs or constraints, you can configure LlamaIndex to use an embedding model that is suitable for <span class="No-Break">your application.</span></p>
			<h3 id="f_8__idParaDest-115" data-type="sect2" class="sect2" title2="How do I decide which embedding model I should use?" no2="5.3.5"><a id="_idTextAnchor114"></a>5.3.5. How do I decide which embedding model I should use?</h3>
			<p>The choice of embedding model<a id="_idIndexMarker414"></a> can significantly affect the performance, quality, and cost of your RAG app. Here are some key points to consider when choosing a <span class="No-Break">particular model:</span></p>
			<ul>
				<li><strong class="bold">Qualitative performance</strong>: Different embedding models may encode the semantics of the text in different ways. While embeddings of models such as OpenAI’s Ada are designed to have a broad understanding of text, other models might be fine-tuned on specific domains or tasks and would outperform in those scenarios. Domain-specific models could lead to more accurate representations of <span class="No-Break">specialized topics</span></li>
				<li><strong class="bold">Quantitative performance</strong>: This includes factors such as how well the model captures semantic similarity, its performance on benchmarks, and generalization to unseen data. This can vary considerably between different models and domains of application. For a general benchmark<a id="_idIndexMarker415"></a> of the most popular models, you can consult the <strong class="bold">Massive Text Embedding Benchmark</strong><em class="italic"> </em>(<strong class="bold">MTEB</strong>) Leaderboard (<a href="https://huggingface.co/spaces/mteb/leaderboard" target="_blank" rel="noopener noreferrer">https://huggingface.co/spaces/mteb/leaderboard</a>) on the Hugging <span class="No-Break">Face website.</span></li>
				<li><strong class="bold">Latency and throughput</strong>: For applications with real-time constraints or large volumes of data, the speed of the embedding model could be a deciding factor. Also consider the maximum input chunk sizes that models can handle, which impacts how text is divided for embedding. Keep in mind that your Nodes will have embeddings computed during ingestion, so that will not affect your overall application performance. However, during retrieval, each query will have to be embedded in real time so that similarity can be measured and the relevant nodes can be retrieved. This is where latency and throughput <span class="No-Break">become important.</span><p class="list-inset">To get an idea of how different embedding models may perform, have a look at this <span class="No-Break">article: </span><a href="https://blog.getzep.com/text-embedding-latency-a-semi-scientific-look/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://blog.getzep.com/text-embedding-latency-a-semi-scientific-look/</span></a><span class="No-Break">.</span></p></li>
				<li><strong class="bold">Multilingual support</strong>: Embedding models can be multilingual or trained for a specific language. Depending on your use case, this can also become an important decision factor. For example, smaller models such as <strong class="source-inline">Mistral</strong> could provide excellent results on par with hosted models such as GPT 3.5 for English data, but their performance in other languages is <span class="No-Break">clearly inferior</span></li>
				<li><strong class="bold">Resource requirements</strong>: Embedding models can vary greatly in size and computational expense. Large models might provide more accurate embeddings but may require substantially more computational resources and thus lead to <span class="No-Break">higher costs.</span></li>
				<li><strong class="bold">Availability</strong>: Some embedding models may only be available through certain APIs or require specific software to be installed, which could affect ease of integration and usage. Fortunately, you have a high degree of customization available <span class="No-Break">in LlamaIndex.</span></li>
				<li><strong class="bold">On-device or local usage</strong>: You may prefer to use a local model when data privacy is a concern or when operating in an environment with limited or no <span class="No-Break">internet access.</span></li>
				<li><strong class="bold">Usage cost</strong>: Consider the cost associated with API calls for cloud-based, hosted embedding models <a id="_idIndexMarker416"></a>versus the computational and storage costs of local <span class="No-Break">embedding models.</span></li>
			</ul>
			<p>The good news is that LlamaIndex supports many out-of-the-box embedding models and provides flexibility to use <span class="No-Break">various embeddings.</span></p>
			<p>By the way, a complete list<a id="_idIndexMarker417"></a> of supported models can be found <span class="No-Break">here: </span><a href="https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#list-of-supported-embeddings" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#list-of-supported-embeddings</span></a></p>
			<p>For most use cases, though, OpenAI’s default embedding model – <strong class="source-inline">text-embedding-ada-002</strong> – will provide you with a good balance between all the parameters we’ve discussed. However, if you have specific needs or constraints, you might benefit from exploring and benchmarking different models to see which provides the best outcomes for your <span class="No-Break">particular application.</span></p>
			<p>Now that we know about embeddings, let us shift our focus to how to store and <span class="No-Break">reuse them.</span></p>
			<h2 id="f_8__idParaDest-116" data-type="sect1" class="sect1" title2="Persisting and reusing Indexes" no2="5.4"><a id="_idTextAnchor115"></a>5.4. Persisting and reusing Indexes</h2>
			<p>An important question<a id="_idIndexMarker418"></a> arises – where exactly<a id="_idIndexMarker419"></a> can we store the vector embeddings generated during the <span class="No-Break">indexing process?</span></p>
			<p>Storing them is important<a id="_idIndexMarker420"></a> for <span class="No-Break">multiple reasons:</span></p>
			<ul>
				<li>Avoid the computational cost of re-embedding documents and rebuilding Indexes in every session. Generating high-quality embeddings for large document collections requires significant processing that can become costly over time. Persisting Indexes preserves these <span class="No-Break">precomputed artifacts</span></li>
				<li>Enable low-latency processing. Avoiding runtime embedding and indexing by loading the already computed embeddings allows applications to get up and running <span class="No-Break">much faster</span></li>
				<li>Maintain query consistency and accuracy. Reloading an Index guarantees we reuse the exact vectors and structure used in the previous sessions. This promises consistent and accurate <span class="No-Break">query execution</span></li>
			</ul>
			<p>If we want to avoid regenerating them on each run, these vector embeddings need to reside somewhere – a repository, if you will – that allows for efficient storage <span class="No-Break">and retrieval.</span></p>
			<p>This is the job of a vector store <span class="No-Break">within LlamaIndex.</span></p>
			<p>By default, LlamaIndex uses an in-memory vector store, but for persistence, it offers a straightforward approach using the <strong class="source-inline">.persist()</strong>method available for any type of Index. This method writes all data to disk at a specified location, <span class="No-Break">ensuring persistence.</span></p>
			<p>Let’s see how we can persist and then load the vector embeddings. First, we create our Index, which handles the embedding <span class="No-Break">of documents:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_95" title2="(no caption)" no2="">from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
documents = SimpleDirectoryReader("data").load_data()
index = VectorStoreIndex.from_documents(documents)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>To persist this data, we use <span class="No-Break">the </span><span class="No-Break"><strong class="source-inline">persist()</strong></span><span class="No-Break">method:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_96" title2="(no caption)" no2="">index.storage_context.persist(persist_dir="index_cache")
print("Index persisted to disk.")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This saves the entire Index data to disk. In future sessions, we can easily reload <span class="No-Break">the data:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_97" title2="(no caption)" no2="">from llama_index.core import StorageContext, load_index_from_storage
storage_context = StorageContext.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;persist_dir="index_cache")
index = load_index_from_storage(storage_context)
print("Index loaded successfully!")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>By rebuilding a <strong class="source-inline">StorageContext</strong> from<a id="_idIndexMarker421"></a> the persisted<a id="_idIndexMarker422"></a> directory and using <strong class="source-inline">load_index_from_storage</strong>, we can effectively reconstitute our Index without needing to re-index <span class="No-Break">our data.</span></p>
			<h3 id="f_8__idParaDest-117" data-type="sect2" class="sect2" title2="Understanding the StorageContext" no2="5.4.1"><a id="_idTextAnchor116"></a>5.4.1. Understanding the StorageContext</h3>
			<p>The <strong class="source-inline">StorageContext</strong> serves as the unifying custodian<a id="_idIndexMarker423"></a> over configurable storage components<a id="_idIndexMarker424"></a> used during indexing and querying. Its key components are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>The <strong class="bold">Document store</strong> (<strong class="source-inline">docstore</strong>): This manages the storage<a id="_idIndexMarker425"></a> of documents. The data is locally stored in a file <span class="No-Break">named </span><span class="No-Break"><strong class="source-inline">docstore.json</strong></span><span class="No-Break">.</span></li>
				<li>The <strong class="bold">Index Store</strong> (<strong class="source-inline">index_store</strong>): This manages the storage<a id="_idIndexMarker426"></a> of Index structures. Indexes are stored locally in a file <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">index_store.json</strong></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Vector Stores</strong> (<strong class="source-inline">vector_stores</strong>): This is a dictionary managing<a id="_idIndexMarker427"></a> multiple vector stores, each potentially serving a different purpose. The vector stores are stored locally <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">vector_store.json</strong></span><span class="No-Break">.</span></li>
				<li>The <strong class="bold">Graph Store</strong> (<strong class="source-inline">graph_store</strong>): This manages the storage<a id="_idIndexMarker428"></a> of graph data structures. A file named <strong class="source-inline">graph_store.json</strong> is automatically created by LlamaIndex for storing <span class="No-Break">the graphs.</span></li>
			</ul>
			<p>The <strong class="source-inline">StorageContext</strong> class encapsulates document, vector, index, and graph data stores under one umbrella. The files mentioned in the previous list for locally storing the data are automatically created by LlamaIndex when we invoke the <strong class="source-inline">persist()</strong> method. If we prefer not to save them in the current folder, we can provide a specific persistence location from where we can load them in <span class="No-Break">future sessions.</span></p>
			<p>Out-of-the-box, LlamaIndex offers basic local stores, but we can swap them with more capable persistence solutions such as <em class="italic">AWS S3,</em> <em class="italic">Pinecone</em>, <em class="italic">MongoDB</em>, <span class="No-Break">and others.</span></p>
			<p>As an example, let’s explore customizing vector storage using ChromaDB, an efficient open source <span class="No-Break">vector engine.</span></p>
			<p>First, make sure<a id="_idIndexMarker429"></a> you install <strong class="source-inline">chromadb</strong> <span class="No-Break">using pip:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_98" title2="(no caption)" no2="">pip install chromadb</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The first part of the code takes care of the <span class="No-Break">necessary imports:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_99" title2="(no caption)" no2="">import chromadb
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import (
&nbsp;&nbsp;&nbsp;&nbsp;VectorStoreIndex, SimpleDirectoryReader, StorageContext)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>We then continue by initializing the Chroma client and creating a collection within Chroma to store <span class="No-Break">our data:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_100" title2="(no caption)" no2="">db = chromadb.PersistentClient(path="chroma_database")
chroma_collection = db.get_or_create_collection(
&nbsp;&nbsp;&nbsp;&nbsp;"my_chroma_store"
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In ChromaDB, we create <strong class="bold">collections</strong> to store data. These are similar<a id="_idIndexMarker430"></a> to <em class="italic">tables</em> in relational databases. The <strong class="source-inline">my_chroma_store</strong> collection will hold <span class="No-Break">our embeddings.</span></p>
			<p>Next, we initialize a tailored vector store using <strong class="source-inline">ChromaVectorStore</strong> and wire it into <span class="No-Break">the </span><span class="No-Break"><strong class="source-inline">StorageContext</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_101" title2="(no caption)" no2="">vector_store = ChromaVectorStore(
&nbsp;&nbsp;&nbsp;&nbsp;chroma_collection=chroma_collection
)
storage_context = StorageContext.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;vector_store=vector_store
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>We’re now ready to ingest our documents and build <span class="No-Break">the Index:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_102" title2="(no caption)" no2="">documents = SimpleDirectoryReader("files").load_data()
index = VectorStoreIndex.from_documents(
&nbsp;&nbsp;&nbsp;&nbsp;documents=documents,
&nbsp;&nbsp;&nbsp;&nbsp;storage_context=storage_context
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>We can now use the <strong class="source-inline">get()</strong> method to display the entire contents of the <span class="No-Break">Chroma collection:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_103" title2="(no caption)" no2="">results = chroma_collection.get()
print(results)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Subsequently, restoring this Index in future sessions is also <span class="No-Break">very simple:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_104" title2="(no caption)" no2="">index = VectorStoreIndex.from_vector_store(
&nbsp;&nbsp;&nbsp;&nbsp;vector_store=vector_store,
&nbsp;&nbsp;&nbsp;&nbsp;storage_context=storage_context
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>We just rebuilt our <span class="No-Break">original Index.</span></p>
			<p>By wrapping <strong class="bold">vector databases</strong> such as ChromaDB, LlamaIndex<a id="_idIndexMarker431"></a> makes enterprise-scale vector storage accessible through a simple storage abstraction. The complexity is concealed, enabling you to focus on your application logic while still leveraging industrial-strength <span class="No-Break">data infrastructure.</span></p>
			<p>In summary, LlamaIndex<a id="_idIndexMarker432"></a> provides flexibility in vector storage – from a simple in-memory store for testing to cloud-hosted databases for large, real-world deployments. And through storage<a id="_idIndexMarker433"></a> integrations, swapping any component is <span class="No-Break">a breeze!</span></p>
			<h3 id="f_8__idParaDest-118" data-type="sect2" class="sect2" title2="The difference between vector stores and vector databases" no2="5.4.2"><a id="_idTextAnchor117"></a>5.4.2. The difference between vector stores and vector databases</h3>
			<p>The terms vector store and vector database<a id="_idIndexMarker434"></a> are often used in the context of managing and querying large sets of vectors, which are commonly used in machine learning, particularly in applications involving NLP, image recognition, and similar tasks. You may have already noticed that I’m using them quite often in this chapter, sometimes implying they are similar concepts. However, there is a subtle distinction between <span class="No-Break">the two:</span></p>
			<ul>
				<li><strong class="bold">Vector store</strong>: This generally refers to a storage system or repository where vectors are stored. The vectors are high-dimensional and represent complex data such as text, images, or audio in a format that can be processed by machine learning models. A vector store focuses primarily on the efficient storage of these vectors. It might not have advanced capabilities for querying or analyzing the data and its main purpose is to maintain a large repository of vectors that can be retrieved and used for various machine <span class="No-Break">learning tasks</span></li>
				<li><strong class="bold">Vector database</strong>: A vector database, on the other hand, is a more sophisticated system that not only stores vectors but also provides advanced functionalities for querying and analyzing them. This includes the ability to perform similarity searches and other complex operations that are useful in machine learning and data analysis. A vector database is designed to handle the nuances of vector data, such as their high dimensionality and the need for specialized indexing techniques to enable efficient search and retrieval. In <span class="No-Break">a nutshell</span></li>
			</ul>
			<p>While a vector store is more about<a id="_idIndexMarker435"></a> the storage aspect, a vector database encompasses both storage and the complex querying capabilities required for vector data. This makes vector databases particularly important in applications where it’s necessary to search through large volumes of vectorized data quickly <span class="No-Break">and accurately.</span></p>
			<p>One distinguishing feature usually representative of a vector database and less often provided<a id="_idIndexMarker436"></a> by vector stores is the support for <strong class="source-inline">CRUD</strong> (<strong class="source-inline">create</strong><strong class="bold">, </strong><strong class="source-inline">read</strong><strong class="bold">, </strong><strong class="source-inline">update</strong><strong class="bold">, </strong><strong class="source-inline">delete</strong>) functions. Whether or not a vector store offers <strong class="source-inline">CRUD</strong> functions can vary depending on the specific implementation and design of the store. However, in general, a vector store, especially if it’s a simplified or basic form of storage for vector data, might not support all the <strong class="source-inline">CRUD</strong> operations in the same way a traditional database system would. Let’s break down the <span class="No-Break">typical operations:</span></p>
			<ul>
				<li><strong class="bold">Create</strong>: The ability to add new vectors<a id="_idIndexMarker437"></a> to the store is usually a fundamental feature. This is essential for building up the <span class="No-Break">vector repository.</span></li>
				<li><strong class="bold">Read</strong>: Reading or retrieving vectors based on some form of identifier or criterion is also a common feature. In a basic vector store, this might be limited to simple retrieval rather than <span class="No-Break">complex queries.</span></li>
				<li><strong class="bold">Update</strong>: Updating existing vectors in a vector store might not be as straightforward or as commonly supported as in traditional databases. This is because vector data, often used in machine learning and similar applications, is usually generated in a fixed form and not <span class="No-Break">frequently updated.</span></li>
				<li><strong class="bold">Delete</strong>: The capability to delete vectors may be supported, but like updating, it may not be a primary feature, depending on the use case of the <span class="No-Break">vector store.</span></li>
			</ul>
			<p>In many machine learning and AI applications, once vectors are created and stored, they are not frequently updated or deleted, which is why some vector stores might focus more on efficient storage and retrieval (create and read operations) rather than full <span class="No-Break">CRUD functionality.</span></p>
			<p>In contrast to a simple vector store, a vector database, which is more sophisticated, is more likely to offer complete CRUD<a id="_idIndexMarker438"></a> capabilities, allowing for more dynamic and flexible management of the <span class="No-Break">vector data.</span></p>
			<p>Here’s a good starting point in your journey<a id="_idIndexMarker439"></a> toward a better understanding of vector <span class="No-Break">databases: </span><a href="https://learn.microsoft.com/en-us/semantic-kernel/memories/vector-db" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://learn.microsoft.com/en-us/semantic-kernel/memories/vector-db</span></a><span class="No-Break">.</span></p>
			<h2 class="sect1" id="f_8__idParaDest-119" data-type="sect1" title2="Exploring other index types in LlamaIndex" no2="5.5"><a id="_idTextAnchor118"></a>5.5. Exploring other index types in LlamaIndex</h2>
<p style="font-style: italic;">As this ebook edition doesn't have fixed pagination, the page numbers below are hyperlinked for reference only, based on the printed edition of this book.</p>
			<p>While the <strong class="source-inline">VectorStoreIndex</strong> may be the star<a id="_idIndexMarker440"></a> of the show in most of our RAG scenarios, LlamaIndex provides many other useful indexing tools. They all have specific features and use cases and the following section will explore them in <span class="No-Break">more detail.</span></p>
			<h3 id="f_8__idParaDest-120" data-type="sect2" class="sect2" title2="The SummaryIndex" no2="5.5.1"><a id="_idTextAnchor119"></a>5.5.1. The SummaryIndex</h3>
			<p>The <strong class="source-inline">SummaryIndex</strong> offers<a id="_idIndexMarker441"></a> a straightforward<a id="_idIndexMarker442"></a> yet powerful way of indexing data for retrieval purposes. Unlike the <strong class="source-inline">VectorStoreIndex</strong>, which focuses on embeddings within a vector store, the <strong class="source-inline">SummaryIndex</strong> is based on a simple data structure where nodes are stored in a sequence. You’ll find a simple depiction of the structure of the <strong class="source-inline">SummaryIndex</strong> in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/B21861_05_7.jpg" alt="Figure 5.7 – The structure of a SummaryIndex" width="1650" height="394" data-type="figure" id="untitled_figure_29" title2="– The structure of a SummaryIndex" no2="5.7">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.7 – The structure of a SummaryIndex</p>
			<p>When building the Index, it ingests a collection of documents, splits them into smaller chunks, and then compiles these chunks into a sequential list. Everything runs locally, without involving an LLM<a id="_idIndexMarker443"></a> or any <span class="No-Break">embedding</span><span class="No-Break"><a id="_idIndexMarker444"></a></span><span class="No-Break"> model.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Practical use case" no2="5.5.1.1">5.5.1.1. Practical use case</h4>
			<p>Imagine we would create<a id="_idIndexMarker445"></a> a documentation search tool within a software development project. Often, software projects accumulate extensive documentation over time, including technical specifications, API documentation, user guides, and developer notes. Keeping track of this information can become challenging, especially when the team needs to quickly reference specific details. Implementing a <strong class="source-inline">SummaryIndex</strong> for the project’s documentation repository allows developers to perform quick searches across all documents. For example, a developer could query <em class="italic">What are the error handling procedures for the payment gateway API?</em> The <strong class="source-inline">SummaryIndex</strong> would scan through the indexed documentation to retrieve relevant sections where error handling is discussed, without the need for complex embedding models or intensive computational resources. This Index would be particularly useful in environments where maintaining an extensive vector store would not be viable due to resource constraints or where simplicity and speed <span class="No-Break">are prioritized.</span></p>
			<p>The <strong class="source-inline">SummaryIndex</strong> is particularly effective for applications where a linear scan through data is sufficient or where complex embedding-based retrieval is not required. It’s a more basic form of indexing but still versatile enough for various use cases, especially in scenarios<a id="_idIndexMarker446"></a> where you need a simple way to index <span class="No-Break">your data.</span></p>
			<h4 data-type="sect3" class="sect3" title2="A simple usage model for the SummaryIndex" no2="5.5.1.2">5.5.1.2. A simple usage model for the SummaryIndex</h4>
			<p>Creating a <strong class="source-inline">SummaryIndex</strong> is a <span class="No-Break">straightforward</span><span class="No-Break"><a id="_idIndexMarker447"></a></span><span class="No-Break"> process:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_105" title2="(no caption)" no2="">from llama_index.core import SummaryIndex, SimpleDirectoryReader
documents = SimpleDirectoryReader("files").load_data()
index = SummaryIndex.from_documents(documents)
query_engine = index.as_query_engine()
response = query_engine.query("How many documents have you loaded?")
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Here, Nodes are created from our sample files and the <strong class="source-inline">SummaryIndex</strong> is instantiated with these Nodes. This simple model enables quick setup without the complexity of embedding or using <span class="No-Break">vector storage.</span></p>
			<p>If you have correctly cloned the structure of our book’s GitHub repository and have a <strong class="source-inline">files</strong> subfolder containing two text<a id="_idIndexMarker448"></a> files, the output of the previous code snippet should be <span class="No-Break">the following:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_106" title2="(no caption)" no2="">I have loaded two documents.</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<h4 data-type="sect3" class="sect3" title2="Understanding the inner workings of the SummaryIndex" no2="5.5.1.3">5.5.1.3. Understanding the inner workings of the SummaryIndex</h4>
			<p>Internally, the <strong class="source-inline">SummaryIndex</strong> operates by storing<a id="_idIndexMarker449"></a> each node in a list-like structure. When a query is executed, the Index iterates through this list to find relevant nodes. While this process is less complex than embedding-based searches in <strong class="source-inline">VectorStoreIndex</strong>, it’s still effective for <span class="No-Break">many applications.</span></p>
			<p>The Index can be used with various retrievers such as <strong class="source-inline">SummaryIndexRetriever</strong>, <strong class="source-inline">SummaryIndexEmbeddingRetriever</strong>, and <strong class="source-inline">SummaryIndexLLMRetriever</strong>, each providing different mechanisms for searching and retrieving data. During queries, the <strong class="source-inline">SummaryIndex</strong> employs a <em class="italic">create and refine</em> approach to formulate responses. Initially, it assembles a preliminary answer based on the first chunk of text. This initial response is subsequently refined by incorporating additional text chunks as contextual information. The refinement process involves either maintaining the initial answer, slightly modifying it, or entirely<a id="_idIndexMarker450"></a> rephrasing the original response. We’ll cover the retrieval part in detail during <a href="#_idTextAnchor131"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Querying Our Data, Part 1 – </em><span class="No-Break"><em class="italic">Context Retrieval</em></span><span class="No-Break">.</span></p>
			<h3 id="f_8__idParaDest-121" data-type="sect2" class="sect2" title2="The DocumentSummaryIndex" no2="5.5.2"><a id="_idTextAnchor120"></a>5.5.2. The DocumentSummaryIndex</h3>
			<p>LlamaIndex’s arsenal<a id="_idIndexMarker451"></a> of indexing tools<a id="_idIndexMarker452"></a> extends beyond its well-regarded <strong class="source-inline">VectorStoreIndex</strong>, encompassing a variety of specialized Indexes designed for diverse applications. Among these, the <strong class="source-inline">DocumentSummaryIndex</strong> stands out for its unique approach to document management <span class="No-Break">and retrieval.</span></p>
			<p>At its core, the <strong class="source-inline">DocumentSummaryIndex</strong> is designed to optimize information retrieval by summarizing Documents and mapping these summaries to their corresponding Nodes within the Index. This process facilitates efficient data retrieval, using the summaries to quickly identify <span class="No-Break">relevant Documents.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.8</em> provides a visual representation of <span class="No-Break">this mechanism:</span></p>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/B21861_05_8.jpg" alt="Figure 5.8 – The DocumentSummaryIndex" width="1650" height="651" data-type="figure" id="untitled_figure_30" title2="– The DocumentSummaryIndex" no2="5.8">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.8 – The DocumentSummaryIndex</p>
			<p>This Index operates by first creating a summary for each ingested Document. These summaries are then linked to the Document’s Nodes, forming a structured Index that enables fast and accurate <span class="No-Break">data retrieval.</span></p>
			<p>The <strong class="source-inline">DocumentSummaryIndex</strong> is particularly useful for handling queries where a succinct overview of the document content can significantly narrow down the search space, making it a great tool for applications requiring quick access to specific Documents in a large and <span class="No-Break">diverse dataset.</span></p>
			<p>For example, a practical<a id="_idIndexMarker453"></a> use case for the <strong class="source-inline">DocumentSummaryIndex</strong> is in the development of a knowledge <a id="_idIndexMarker454"></a>management system within a large organization. In such an environment, employees often need quick access to a vast array of documents, including reports, research papers, policy documents, and technical manuals. These documents are typically stored across different departments and may be extensive in length, making it challenging to quickly find specific information relevant to a user’s query. In addition, multiple documents may contain similar chunks of text, making a simple embedding-based retrieval impractical over the <span class="No-Break">entire dataset.</span></p>
			<p>Several parameters<a id="_idIndexMarker455"></a> can be customized for this <span class="No-Break">particular Index:</span></p>
			<ul>
				<li><strong class="source-inline">response_synthesizer</strong>: This parameter allows you to specify a response synthesizer that is responsible for generating summaries. By customizing this parameter, you can control the summarization process, adjusting it to fit specific needs or preferences in how summaries <span class="No-Break">are generated.</span></li>
				<li><strong class="source-inline">summary_query</strong>: This parameter is used to define the query that guides the summarization process. Essentially, it tells the response synthesizer what kind of summary to generate for each Document. The default query asks for a summary that describes what the Document is about and what questions it can answer. Adjusting this query allows you to tailor the focus and style of the summaries, making them more relevant to the specific use cases of <span class="No-Break">the Index.</span></li>
				<li><strong class="source-inline">show_progress</strong>: This Boolean parameter determines whether to display progress bars during operations that can take a significant amount of time. Setting this to <strong class="source-inline">True</strong> provides visual feedback on the progress of <span class="No-Break">these operations.</span></li>
				<li><strong class="source-inline">embed_summaries</strong>: When set to <strong class="source-inline">True</strong> – which is the default – this parameter indicates that the summaries should be embedded. Embedded summaries can then be used for similarity comparisons and retrieval in an embedding-based search. This is particularly useful for scenarios where you want to retrieve Nodes based on the similarity between the Document summary content and the user<a id="_idIndexMarker456"></a> query. We’ll cover this in more detail during <a href="#_idTextAnchor131"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Querying Our Data, Part 1 – </em><span class="No-Break"><em class="italic">Context Retrieval</em></span><span class="No-Break"><em class="italic">.</em></span></li>
			</ul>
			<p>Let’s now see how to use <span class="No-Break">the </span><span class="No-Break"><strong class="source-inline">DocumentSummaryIndex</strong></span><span class="No-Break">.</span></p>
			<h4 data-type="sect3" class="sect3" title2="A simple usage model for the DocumentSummaryIndex" no2="5.5.2.1">5.5.2.1. A simple usage model for the DocumentSummaryIndex</h4>
			<p>Creating a <strong class="source-inline">DocumentSummaryIndex</strong> involves a series<a id="_idIndexMarker457"></a> of steps, starting with the aggregation of Documents and their subsequent summarization. The following code snippet demonstrates the basic setup for creating <span class="No-Break">this Index:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_107" title2="(no caption)" no2="">from llama_index.core import (
&nbsp;&nbsp;&nbsp;&nbsp;DocumentSummaryIndex, SimpleDirectoryReader)
documents = SimpleDirectoryReader("files").load_data()
index = DocumentSummaryIndex.from_documents(
&nbsp;&nbsp;&nbsp;&nbsp;documents,
&nbsp;&nbsp;&nbsp;&nbsp;show_progress=True
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This process involves reading documents from a directory, parsing them into Nodes, summarizing the Documents, and then associating the corresponding Nodes with these summaries for quick retrieval. Next, let’s observe the summaries that were generated in <span class="No-Break">the process:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_108" title2="(no caption)" no2="">summary1 = index.get_document_summary(documents[0].doc_id)
summary2 = index.get_document_summary(documents[1].doc_id)
print("\n Summary of the first document: " + summary1)
print("\n Summary of the second document: " + summary2)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The second part of the code sample displays the summaries that were generated for each Document. These summaries were associated with the underlying Nodes for each Document. During retrieval, this association will allow extracting only the relevant Nodes, based on the user query and the summary of <span class="No-Break">each Document.</span></p>
			<p>Internally, the <strong class="source-inline">DocumentSummaryIndex</strong> supports both embedding-based and LLM-based retrievers, allowing <a id="_idIndexMarker458"></a>for flexible retrieval mechanisms that cater to different needs. By default, the Index also generates embeddings for each summary in order to facilitate embedding-based retrieval, which is particularly useful for <span class="No-Break">similarity searches.</span></p>
			<h3 id="f_8__idParaDest-122" data-type="sect2" class="sect2" title2="The KeywordTableIndex" no2="5.5.3"><a id="_idTextAnchor121"></a>5.5.3. The KeywordTableIndex</h3>
			<p>The <strong class="source-inline">KeywordTableIndex</strong> in LlamaIndex implements<a id="_idIndexMarker459"></a> a clever <a id="_idIndexMarker460"></a>architecture – similar to a glossary of terms – for rapidly matching queries to relevant nodes based on important terms. Unlike complex embedding spaces, this structure relies on a straightforward keyword table, yet proves highly effective for targeted factual lookup. This Index extracts keywords from documents and constructs a keyword-to-node mapping, offering a highly efficient <span class="No-Break">search mechanism.</span></p>
			<p>It’s particularly useful in scenarios where precise keyword matching is vital for retrieving relevant information. These keywords become the reference keys in a central lookup table, each one pointing to associated nodes such as a glossary definition. During retrieval, just like scanning a glossary for entries of interest, relevant nodes containing a particular keyword are identified<a id="_idIndexMarker461"></a> and returned. See <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.9</em> for a <span class="No-Break">visual representation:</span></p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B21861_05_9.jpg" alt="Figure 5.9 – The structure of a KeywordTableIndex" width="1650" height="678" data-type="figure" id="untitled_figure_31" title2="– The structure of a KeywordTableIndex" no2="5.9">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.9 – The structure of a KeywordTableIndex</p>
			<p>The customizable<a id="_idIndexMarker462"></a> parameters for the <strong class="source-inline">KeywordTableIndex</strong> are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="source-inline">keyword_extract_template</strong>: This is an optional prompt template used for keyword extraction. Custom prompts can be specified to change how keywords are extracted from text, allowing for tailored keyword extraction strategies. We’ll talk more about prompt customization during <a href="#_idTextAnchor214"><span class="No-Break"><em class="italic">Chapter 10</em></span></a><span class="No-Break"><em class="italic">.</em></span></li>
				<li><strong class="source-inline">max_keywords_per_chunk</strong>: This sets the maximum number of keywords to extract from each text chunk. By using this parameter, we can make sure the keyword table remains manageable and focused on the most relevant keywords. The default value <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">10</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">use_async</strong>: This determines whether to use asynchronous calls. This can improve performance, especially when handling large datasets or complex operations. Its default<a id="_idIndexMarker463"></a> setting <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">False</strong></span><span class="No-Break">.</span></li>
			</ul>
			<p>Next up, we will create <span class="No-Break">the </span><span class="No-Break"><strong class="source-inline">KeywordTableIndex</strong></span><span class="No-Break">.</span></p>
			<h4 data-type="sect3" class="sect3" title2="A simple usage model for the KeywordTableIndex" no2="5.5.3.1">5.5.3.1. A simple usage model for the KeywordTableIndex</h4>
			<p>Creating a <strong class="source-inline">KeywordTableIndex</strong> is <span class="No-Break">very</span><span class="No-Break"><a id="_idIndexMarker464"></a></span><span class="No-Break"> straightforward:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_109" title2="(no caption)" no2="">from llama_index.core import KeywordTableIndex, SimpleDirectoryReader
documents = SimpleDirectoryReader("files").load_data()
index = KeywordTableIndex.from_documents(documents)
query_engine = index.as_query_engine()
response = query_engine.query("
&nbsp;&nbsp;&nbsp;&nbsp;What famous buildings were in ancient Rome?")
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Here, the Index automatically extracts keywords from your data and sets up a keyword table, streamlining the process of setting up a keyword-based <span class="No-Break">retrieval system.</span></p>
			<p>Just like in the previous example, if you have correctly cloned the structure of our GitHub repository and have a <strong class="source-inline">files</strong> subfolder containing two text files, the output of the previous code snippet should be something along the lines of <em class="italic">The Colosseum and the Pantheon were famous buildings in </em><span class="No-Break"><em class="italic">ancient Rome</em></span><span class="No-Break">.</span></p>
			<h4 data-type="sect3" class="sect3" title2="How does the KeywordTableIndex operate?" no2="5.5.3.2">5.5.3.2. How does the KeywordTableIndex operate?</h4>
			<p>The <strong class="source-inline">KeywordTableIndex</strong> builds and operates<a id="_idIndexMarker465"></a> a keyword table, akin to a glossary, where each keyword is linked to relevant nodes. The Index initially processes a collection of documents, breaking them down into smaller chunks. For each chunk, the Index uses the LLM with a specially designed prompt to identify and extract relevant keywords. These keywords, which may range from simple terms to short phrases, are subsequently cataloged in the keyword table. Each keyword in this table is directly linked to the chunk of text from which it <span class="No-Break">was derived.</span></p>
			<p>Upon receiving a query, the Index identifies keywords within it and matches them with the table entries, enabling rapid and accurate retrieval of related chunks containing those keywords. It supports various retrieval modes, including simple keyword matching and advanced<a id="_idIndexMarker466"></a> techniques such as <strong class="bold">RAKE</strong> or LLM-based keyword extraction and matching. We’ll talk more about these retrieval modes during <a href="#_idTextAnchor131"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Querying Our Data, Part 1 – </em><span class="No-Break"><em class="italic">Context Retrieval</em></span><span class="No-Break">.</span></p>
			<p class="callout-heading">Quick note on the RAKE extraction method</p>
			<p class="callout">This method is particularly effective<a id="_idIndexMarker467"></a> in identifying phrases or keywords that are significant within a body of text. The key idea behind RAKE is that keywords often consist of multiple words but rarely include punctuation, stop words, or words with minimal lexical meaning. The <strong class="source-inline">KeywordTableIndex</strong> has two similar alternatives that are designed to operate without the assistance of an LLM: <strong class="source-inline">SimpleKeywordTableIndex</strong>, which uses a simple regex extractor, and <strong class="source-inline">RAKEKeywordTableIndex</strong>, which relies on a RAKE keyword extractor based on the <strong class="source-inline">rake_nltk</strong> (Natural Language Toolkit) <span class="No-Break">Python package.</span></p>
			<p>You should know that, just like the <strong class="source-inline">SummaryIndex</strong>, the <strong class="source-inline">KeywordTableIndex</strong> also uses a <em class="italic">create and refine</em> approach when synthesizing<a id="_idIndexMarker468"></a> the final response. The adaptability of the <strong class="source-inline">KeywordTableIndex</strong> makes it a versatile tool for diverse applications where keyword precision <span class="No-Break">is key.</span></p>
			<h3 id="f_8__idParaDest-123" data-type="sect2" class="sect2" title2="The TreeIndex" no2="5.5.4"><a id="_idTextAnchor122"></a>5.5.4. The TreeIndex</h3>
			<p>The <strong class="source-inline">TreeIndex</strong> introduces a<a id="_idIndexMarker469"></a> hierarchical approach<a id="_idIndexMarker470"></a> to information organization and retrieval. Unlike a simple list, this structure organizes data in a hierarchical <span class="No-Break">tree format.</span></p>
			<p>Have a look at <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.10</em> for a diagram depicting the structure of <span class="No-Break">the </span><span class="No-Break"><strong class="source-inline">TreeIndex</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B21861_05_10.jpg" alt="Figure 5.10 – The structure of a TreeIndex" width="1297" height="812" data-type="figure" id="untitled_figure_32" title2="– The structure of a TreeIndex" no2="5.10">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.10 – The structure of a TreeIndex</p>
			<p>Each node in this tree can represent a piece of data or information, similar to a branch or leaf on a real tree. This structural formation allows for efficient handling and querying of data. The <strong class="source-inline">TreeIndex</strong> first takes in a set of documents as input. It then builds up a tree in a bottom-up fashion; each parent node is able to summarize the child nodes using a general summarization prompt, and each intermediate node contains text summarizing the components below it. This summary is generated using an LLM based on a prompt template that can be customized with the <strong class="source-inline">summary_prompt</strong> parameter. <strong class="source-inline">TreeIndex</strong> acts like an organizer<a id="_idIndexMarker471"></a> and summarizer, taking lots of individual pieces of data, grouping them together, and creating a summary<a id="_idIndexMarker472"></a> that captures <span class="No-Break">their essence.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Customizable parameters for the TreeIndex" no2="5.5.4.1">5.5.4.1. Customizable parameters for the TreeIndex</h4>
			<p>Apart from the general customization<a id="_idIndexMarker473"></a> inherited from the <strong class="source-inline">BaseIndex</strong> class, the <strong class="source-inline">TreeIndex</strong> provides the <span class="No-Break">following parameters:</span></p>
			<ul>
				<li><strong class="source-inline">summary_template</strong>: This is a prompt for summarization, used during Index construction. This prompt can be customized for better control of the <span class="No-Break">summarization process.</span></li>
				<li><strong class="source-inline">insert_prompt</strong>: This is a prompt used by the Index for tree insertion, facilitating Index construction. This prompt facilitates the insertion of nodes into the tree. It guides how new information is integrated into the existing tree structure. We’ll cover details about prompt customization during <a href="#_idTextAnchor214"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Prompt Engineering Guidelines and </em><span class="No-Break"><em class="italic">Best Practices.</em></span></li>
				<li><strong class="source-inline">num_children</strong>: This defines the maximum number of child nodes each node should have. This parameter controls the breadth of the tree, impacting its level of detail at each node. By default, this is set <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">10</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">build_tree</strong>: This is a Boolean indicating whether to build the tree during Index construction. If we don’t use the default value – which is <strong class="source-inline">True</strong> – the Index will build its tree during query time instead of building it during the Index construction. Setting the <strong class="source-inline">build_tree</strong> parameter to <strong class="source-inline">False</strong> could be useful in scenarios where you might want to manually control the tree-building process or modify the tree structure after <span class="No-Break">initial construction.</span></li>
				<li><strong class="source-inline">use_async</strong>: This determines whether asynchronous<a id="_idIndexMarker474"></a> operation mode should <span class="No-Break">be used.</span></li>
			</ul>
			<p>Next, let’s create a <span class="No-Break">simple </span><span class="No-Break"><strong class="source-inline">TreeIndex</strong></span><span class="No-Break">.</span></p>
			<h4 data-type="sect3" class="sect3" title2="A simple usage model for the TreeIndex" no2="5.5.4.2">5.5.4.2. A simple usage model for the TreeIndex</h4>
			<p>To implement a <strong class="source-inline">TreeIndex</strong>, you can<a id="_idIndexMarker475"></a> follow this <span class="No-Break">simple example:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_110" title2="(no caption)" no2="">from llama_index.core import TreeIndex, SimpleDirectoryReader
documents = SimpleDirectoryReader("files").load_data()
index = TreeIndex.from_documents(documents)
query_engine = index.as_query_engine()
response = query_engine.query("Tell me about dogs")
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This process involves the <strong class="source-inline">TreeIndex</strong> taking in documents, structuring them hierarchically, and then allowing for queries<a id="_idIndexMarker476"></a> that leverage this structure for efficient <span class="No-Break">data retrieval.</span></p>
			<h4 data-type="sect3" class="sect3" title2="The inner mechanics of the TreeIndex" no2="5.5.4.3">5.5.4.3. The inner mechanics of the TreeIndex</h4>
			<p>The index-building process<a id="_idIndexMarker477"></a> is recursive. After the first level of parent nodes is created, the builder can repeat the process, summarizing these parent nodes into higher-level nodes, and so on. This creates multiple levels in the tree, with each level abstracting and summarizing the information from the level below it. Also, for large datasets, the Index can handle data asynchronously with <strong class="source-inline">use_async</strong>. This means it can process multiple parts of the data simultaneously, making the building process faster and <span class="No-Break">more efficient.</span></p>
			<p>By using LLMs for summaries, the <strong class="source-inline">TreeIndex</strong> can encapsulate a nuanced understanding of the data. This is particularly useful for complex datasets where relationships and <span class="No-Break">context matter.</span></p>
			<h4 data-type="sect3" class="sect3" title2="For example, in organizations" no2="5.5.4.4">5.5.4.4. For example, in organizations</h4>
			<p>In organizations with complex hierarchical data<a id="_idIndexMarker478"></a> such as reports, memos, and research papers, a <strong class="source-inline">TreeIndex</strong> can organize this information efficiently, allowing for quick retrieval of specific data points within their knowledge <span class="No-Break">management systems.</span></p>
			<p><strong class="source-inline">TreeIndex</strong> operates by building a tree where each node is a summarized representation of its children, offering a clear and organized view of <span class="No-Break">the data.</span></p>
			<p>This Index supports<a id="_idIndexMarker479"></a> several <span class="No-Break">retrieval modes:</span></p>
			<ul>
				<li><strong class="source-inline">TreeSelectLeafRetriever</strong>: This traverses the tree to find leaf nodes that can best answer a query. It involves choosing a specific number of child nodes at each level <span class="No-Break">for traversal.</span></li>
				<li><strong class="source-inline">TreeSelectLeafEmbeddingRetriever</strong>: This utilizes embedding similarity between the query and node text to traverse the tree, selecting leaf nodes based on <span class="No-Break">this similarity.</span></li>
				<li><strong class="source-inline">TreeRootRetriever</strong>: This directly retrieves answers from the root nodes of the tree. This method assumes the graph already stores the answer, so it doesn’t parse information down <span class="No-Break">the tree.</span></li>
				<li><strong class="source-inline">TreeAllLeafRetriever</strong>: This builds a query-specific tree from all leaf nodes to return a response. It rebuilds the tree for each query, making it suitable for scenarios<a id="_idIndexMarker480"></a> where the tree structure doesn’t need to be built <span class="No-Break">during initialization.</span></li>
			</ul>
			<p>During query time, the tree Index<a id="_idIndexMarker481"></a> operates in the <span class="No-Break">following way:</span></p>
			<ol>
				<li>First, the provided query string is processed to extract <span class="No-Break">relevant keywords</span></li>
				<li>Beginning from the root Node, the Index navigates through the <span class="No-Break">tree structure</span></li>
				<li>At each Node, it determines whether the keywords are found in the <span class="No-Break">Node’s summary</span></li>
				<li>If keywords are found, the Index proceeds to explore the Node’s <span class="No-Break">child Nodes</span></li>
				<li>If the keywords are absent, the Index advances to the <span class="No-Break">subsequent Node</span></li>
				<li>This process persists until a leaf Node is encountered or all Nodes in the tree have <span class="No-Break">been examined</span></li>
			</ol>
			<p>The reached leaf Nodes represent the context with the highest likelihood of relevance to the <span class="No-Break">given query.</span></p>
			<p>We’ll cover the retrievers in more detail during <a href="#_idTextAnchor131"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Querying Our Data, Part 1 – </em><span class="No-Break"><em class="italic">Context Retrieval</em></span><span class="No-Break">.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Some potential drawbacks of using the TreeIndex" no2="5.5.4.5">5.5.4.5. Some potential drawbacks of using the TreeIndex</h4>
			<p>Using a <strong class="source-inline">TreeIndex</strong> in our RAG workflow<a id="_idIndexMarker482"></a> can potentially be less advantageous compared to simpler retrieval methods. Here are a few <span class="No-Break">reasons why:</span></p>
			<ul>
				<li><em class="italic">Increased computation</em>: Building and maintaining a <strong class="source-inline">TreeIndex</strong> requires additional computational resources. During the Index construction phase, the tree structure needs to be created by recursively summarizing and organizing the Nodes. This process involves applying summarization using LLM calls and constructing the hierarchical structure, which can be computationally intensive, especially for <span class="No-Break">large datasets.</span></li>
				<li><em class="italic">Recursive retrieval</em>: When querying the Index, the retrieval process involves traversing the tree structure from the root nodes down to the relevant leaf nodes. This recursive traversal can require multiple steps and computations, especially if the tree is deep or if multiple branches need to be explored. Each step in the traversal may involve comparing the query with the Node summaries and making decisions on which branches to follow. This recursive process can be more computationally expensive compared to retrieving from a <span class="No-Break">flat Index.</span></li>
				<li><em class="italic">Summarization overhead</em>: This Index relies on summarizing the content of each node to provide a concise representation of its child Nodes. The summarization process needs to be performed during Index construction and potentially during updates or insertions, adding to the overall <span class="No-Break">computational overhead.</span></li>
				<li><em class="italic">Storage requirements</em>: Storing a <strong class="source-inline">TreeIndex</strong> requires additional storage compared to a flat Index. The Index needs to store the tree structure, Node summaries, and metadata associated with each Node. This extra storage overhead can increase storage costs, especially for <span class="No-Break">large-scale datasets.</span></li>
				<li><em class="italic">Maintenance and updates</em>: Maintaining a <strong class="source-inline">TreeIndex</strong> requires regular updates and re-organization as new data is added or existing data is modified. Inserting new nodes or updating existing nodes in the tree structure may trigger a cascading effect, requiring updates to the parent nodes and their summaries. This maintenance process can be more complex and time-consuming compared to <span class="No-Break">other Indexes.</span></li>
			</ul>
			<p>However, it’s important to note that the higher costs associated with using a <strong class="source-inline">TreeIndex</strong> can be justified in certain scenarios. If the RAG application deals with a large-scale dataset and requires efficient and context-aware retrieval, the benefits of using this type of Index may outweigh the additional costs. Its hierarchical structure and summarization capabilities can lead to improved retrieval performance, reduced search space, and better response generation quality. By traversing the tree from the root Nodes and selectively exploring relevant branches, the model can quickly narrow down the search to the most promising Nodes. This can lead to faster retrieval times and improved efficiency compared to searching through a flat <span class="No-Break">Index structure.</span></p>
			<p>The key is to assess<a id="_idIndexMarker483"></a> the specific requirements, scale, and constraints of the RAG scenario to determine whether the benefits of using a <strong class="source-inline">TreeIndex</strong> justify the potential increase in costs. Careful evaluation and benchmarking can help in making an informed decision based on the trade-offs between retrieval efficiency, generation quality, and computational and <span class="No-Break">storage costs.</span></p>
			<h3 id="f_8__idParaDest-124" data-type="sect2" class="sect2" title2="The KnowledgeGraphIndex" no2="5.5.5"><a id="_idTextAnchor123"></a>5.5.5. The KnowledgeGraphIndex</h3>
			<p>The <strong class="source-inline">KnowledgeGraphIndex</strong> enhances query<a id="_idIndexMarker484"></a> processing<a id="_idIndexMarker485"></a> by constructing a <strong class="bold">knowledge graph</strong> (<strong class="bold">KG</strong>) from extracted <strong class="bold">triplets</strong>. This type of Index primarily<a id="_idIndexMarker486"></a> relies on the LLM <a id="_idIndexMarker487"></a>to extract triplets from text, but it also provides flexibility to use custom extraction functions <span class="No-Break">if needed.</span></p>
			<p>KG Indexes excel in scenarios where understanding complex, interlinked relationships and contextual information is important. They are very good at capturing intricate connections between entities and concepts, thus offering better insights and context-aware responses to queries. Among other use cases, KGs are ideal for answering multifaceted questions that require an understanding of the relationships between different entities. <em class="italic">Yes, I’m talking about our tutor project, </em><span class="No-Break"><em class="italic">PITS, here.</em></span></p>
			<p>Let’s get a visual of how KGs work in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.11</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B21861_05_11.jpg" alt="Figure 5.11 – The structure of a KnowledgeGraphIndex" width="1617" height="1012" data-type="figure" id="untitled_figure_33" title2="– The structure of a KnowledgeGraphIndex" no2="5.11">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.11 – The structure of a KnowledgeGraphIndex</p>
			<h4 data-type="sect3" class="sect3" title2="Practical use case" no2="5.5.5.1">5.5.5.1. Practical use case</h4>
			<p>An interesting use case<a id="_idIndexMarker488"></a> for a KG could be, for example, a news aggregation app, where large volumes of text are ingested every day from various sources such as newspapers, blogs, and social media platforms. In such a scenario, KGs could be used to represent entities such as people, organizations, locations, and so on, and their relationships over time. This would allow users to explore historical trends, breaking news events, and related entities based on the graph structure and <span class="No-Break">traversal algorithms.</span></p>
			<p>Sounds good, right? We will now take a look at how you can work <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">KnowledgeGraphIndex</strong></span><span class="No-Break">.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Customizable parameters for the KnowledgeGraphIndex" no2="5.5.5.2">5.5.5.2. Customizable parameters for the KnowledgeGraphIndex</h4>
			<p>You can customize<a id="_idIndexMarker489"></a> the <span class="No-Break">following parameters:</span></p>
			<ul>
				<li><strong class="source-inline">kg_triple_extract_template</strong>: This is a prompt template for extracting triplets. It can be customized to change how triplets (subject-predicate-object) are identified, enabling tailored extraction strategies based on specific <span class="No-Break">use cases.</span></li>
				<li><strong class="source-inline">max_triplets_per_chunk</strong>: This limits the number of triplets extracted per text chunk. Setting this value helps manage the size and complexity of the KG. The default value <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">10</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">graph_store</strong>: This defines the storage type for the graph. Different storage types can be used to optimize performance and scalability based on the <span class="No-Break">application’s requirements.</span></li>
				<li><strong class="source-inline">include_embeddings</strong>: This decides whether to include embeddings in the Index. This is useful for scenarios where embeddings can enhance the retrieval process, such as similarity searches or advanced <span class="No-Break">query understanding.</span></li>
				<li><strong class="source-inline">max_object_length</strong>: This sets the maximum length – in characters – for the object in a triplet. It prevents overly long or complex objects that could complicate the graph’s structure and the retrieval process. Its default value <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">128</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">kg_triplet_extract_fn</strong>: A custom function for triplet extraction can be provided, offering the flexibility to use specialized or proprietary methods for extracting triplets<a id="_idIndexMarker490"></a> <span class="No-Break">from text.</span></li>
			</ul>
			<p>Let’s create a simple <span class="No-Break">KG next.</span></p>
			<h4 data-type="sect3" class="sect3" title2="A basic usage model for KnowledgeGraphIndex" no2="5.5.5.3">5.5.5.3. A basic usage model for KnowledgeGraphIndex</h4>
			<p>Here’s a simple way<a id="_idIndexMarker491"></a> of constructing and querying <span class="No-Break">a KG:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_111" title2="(no caption)" no2="">from llama_index.core import (
&nbsp;&nbsp;&nbsp;&nbsp;KnowledgeGraphIndex, SimpleDirectoryReader)
documents = SimpleDirectoryReader("files").load_data()
index = KnowledgeGraphIndex.from_documents(
&nbsp;&nbsp;&nbsp;&nbsp;documents, max_triplets_per_chunk=2, use_async=True)
query_engine = index.as_query_engine()
response = query_engine.query("Tell me about dogs.")
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In this setup, the Index builds a KG by extracting triplets from documents, enabling complex relationship queries. Notice that we configured the Index to run the build process in asynchronous mode by setting <strong class="source-inline">use_async</strong> to <strong class="source-inline">True</strong>. Of course, for the two small documents that we’re using as an example in our case, this won’t make too much difference in the total execution time. However, when working with large datasets, enabling asynchronous operation<a id="_idIndexMarker492"></a> for this Index may provide an important <span class="No-Break">performance boost.</span></p>
			<h4 data-type="sect3" class="sect3" title2="How does the KnowledgeGraphIndex build its structure?" no2="5.5.5.4">5.5.5.4. How does the KnowledgeGraphIndex build its structure?</h4>
			<p><strong class="source-inline">KnowledgeGraphIndex</strong> operates by extracting subject-predicate-object triplets from text data, forming <span class="No-Break">a KG.</span></p>
			<p>There are two main ways in which<a id="_idIndexMarker493"></a> this Index can build <span class="No-Break">its structure:</span></p>
			<ul>
				<li><em class="italic">The default, built-in approach</em>: In its default implementation, the Index uses an internal method to extract triplets from text. This method takes the text content of each Node and passes it through a pre-defined prompt template – <strong class="source-inline">DEFAULT_KG_TRIPLET_EXTRACT_PROMPT</strong> or a custom template provided during initialization through the <strong class="source-inline">kg_triple_extract_template</strong> argument. The prompt template is designed to instruct the LLM to extract knowledge triplets from the given text. The LLM’s response is then parsed by a specialized internal method to extract the subject, predicate, and object of each triplet. This method extracts knowledge triplets in the format of <em class="italic">subject, predicate, object</em>. It applies various checks and string manipulations to ensure the validity and consistency of the extracted triplets. Finally, the method returns a list of cleaned and well-formatted triplets that can be added to the <span class="No-Break">KG Index.</span></li>
				<li><em class="italic">The second approach involves a custom triplet extraction function</em>: If a custom <strong class="source-inline">kg_triplet_extract_fn</strong> function is provided during initialization, it will be used instead of the LLM-based method. This allows us to define our own function to extract triplets from text based on their specific requirements or <span class="No-Break">domain knowledge.</span></li>
			</ul>
			<p>Regardless of whether we’re using the first or the second approach to generate the triplets, the inner components of the Index are responsible for building the actual KG from the given Nodes. They iterate over each Node, extract triplets using either the LLM-based method or the custom extraction function and add the triplets to the <span class="No-Break">Index structure.</span></p>
			<p>If the <strong class="source-inline">include_embeddings</strong> flag is set to <strong class="source-inline">True</strong>, the Index will also generate embeddings for each triplet using the specified embedding model. These embeddings are stored in the <strong class="source-inline">embedding_dict</strong> of the <span class="No-Break">Index structure.</span></p>
			<p>The <strong class="source-inline">upsert_triplet()</strong> method allows the manual insertion of triplets into the KG. It adds the triplet to the graph store and also optionally generates embeddings for the triplet if <strong class="source-inline">include_embeddings</strong> are set <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">True</strong></span><span class="No-Break">.</span></p>
			<p>During querying, the Index leverages the KG to retrieve relevant data and help provide context-rich responses. There are three distinct retrievers available for this Index: <strong class="source-inline">KGTableRetriever</strong> for keyword-focused queries, <strong class="source-inline">KnowledgeGraphRAGRetriever</strong> for retrieving sub-graphs based on extracted entities and synonyms, and a hybrid mode that combines both keyword and embedding strategies for a comprehensive<a id="_idIndexMarker494"></a> approach. More details about these retrieval capabilities will be explored during <a href="#_idTextAnchor131"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Querying Our Data, Part 1 – </em><span class="No-Break"><em class="italic">Context Retrieval</em></span><span class="No-Break">.</span></p>
			<h2 id="f_8__idParaDest-125" data-type="sect1" class="sect1" title2="Building Indexes on top of other Indexes with ComposableGraph" no2="5.6"><a id="_idTextAnchor124"></a>5.6. Building Indexes on top of other Indexes with ComposableGraph</h2>
			<p>The <strong class="source-inline">ComposableGraph</strong> in LlamaIndex represents<a id="_idIndexMarker495"></a> a sophisticated way<a id="_idIndexMarker496"></a> to structure information by <strong class="bold">stacking Indexes</strong> on top of <span class="No-Break">each other.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.12</em> provides an overview of <span class="No-Break">a </span><span class="No-Break"><strong class="source-inline">ComposableGraph</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B21861_05_12.jpg" alt="Figure 5.12 – The structure of a ComposableGraph" width="1517" height="1012" data-type="figure" id="untitled_figure_34" title2="– The structure of a ComposableGraph" no2="5.12">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.12 – The structure of a ComposableGraph</p>
			<p>This approach allows for the construction<a id="_idIndexMarker497"></a> of Indexes within individual documents – lower-level Indexes – and the aggregation of these Indexes into higher-order ones over a collection of documents. For example, you can build a <strong class="source-inline">TreeIndex</strong> for the text within each document and a <strong class="source-inline">SummaryIndex</strong> that encompasses each <strong class="source-inline">TreeIndex</strong> in <span class="No-Break">a collection.</span></p>
			<h3 id="f_8__idParaDest-126" data-type="sect2" class="sect2" title2="How to use the ComposableGraph" no2="5.6.1"><a id="_idTextAnchor125"></a>5.6.1. How to use the ComposableGraph</h3>
			<p>Here’s a simple code example demonstrating<a id="_idIndexMarker498"></a> the usage <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">ComposableGraph</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_112" title2="(no caption)" no2="">from llama_index.core import (
&nbsp;&nbsp;&nbsp;&nbsp;ComposableGraph, SimpleDirectoryReader, 
&nbsp;&nbsp;&nbsp;&nbsp;TreeIndex, SummaryIndex)
documents = SimpleDirectoryReader("files").load_data()
index1 = TreeIndex.from_documents([documents[0]])
index2 = TreeIndex.from_documents([documents[1]])
summary1 = "A short introduction to ancient Rome"
summary2 = "Some facts about dogs"
graph = ComposableGraph.from_indices(
&nbsp;&nbsp;&nbsp;&nbsp;SummaryIndex, [index1, index2],
&nbsp;&nbsp;&nbsp;&nbsp;index_summaries=[summary1, summary2]
)
query_engine = graph.as_query_engine()
response = query_engine.query("What can you tell me?")
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In this example, the <strong class="source-inline">ComposableGraph</strong> facilitates the organization of detailed information within Documents and the summarization <span class="No-Break">across Documents.</span></p>
			<p>We first load our two test Documents: one related to ancient Rome and the other one describing dogs. We then create a <strong class="source-inline">TreeIndex</strong> for <span class="No-Break">each Document.</span></p>
			<p>We also define the summaries<a id="_idIndexMarker499"></a> of the <span class="No-Break">two Documents.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Pro tip" no2="5.6.1.1">5.6.1.1. Pro tip</h4>
			<p>As an alternative to manually<a id="_idIndexMarker500"></a> defining the summaries, we could have also queried each individual Index to automatically generate the content summary or used <strong class="source-inline">SummaryExtractor</strong> to accomplish the <span class="No-Break">same purpose.</span></p>
			<p>In the next step, we build a <strong class="source-inline">ComposableGraph</strong> containing the two tree Indexes along with their summaries. For this example, the output of the code should be something similar to the following: <em class="italic">I can tell you about the ancient Roman civilization and dogs and their various breeds, traits, </em><span class="No-Break"><em class="italic">and personalities.</em></span></p>
			<p>Once the <strong class="source-inline">ComposableGraph</strong> has been built, the root <strong class="source-inline">SummaryIndex</strong> will have an overview of the contents of the individual Indexes for <span class="No-Break">each document.</span></p>
			<h3 id="f_8__idParaDest-127" data-type="sect2" class="sect2" title2="A more detailed description of this concept" no2="5.6.2"><a id="_idTextAnchor126"></a>5.6.2. A more detailed description of this concept</h3>
			<p>Under the hood, a <strong class="source-inline">ComposableGraph</strong> enables the creation<a id="_idIndexMarker501"></a> of hierarchical structures by stacking Indexes on top of each other. This allows for the organization of detailed information within individual Documents using lower-level Indexes and the aggregation of these Indexes into higher-order ones over a collection <span class="No-Break">of Documents.</span></p>
			<p>The process begins by creating individual Indexes for each Document to capture the detailed information within the Documents. Additionally, summaries are defined for <span class="No-Break">each Document.</span></p>
			<p>The <strong class="source-inline">ComposableGraph</strong> is then constructed<a id="_idIndexMarker502"></a> using the <strong class="source-inline">from_indices()</strong> class method. It takes the root Index class (in our example, the <strong class="source-inline">SummaryIndex</strong>), the child Indexes (in our example, the two <strong class="source-inline">TreeIndex</strong> instances), and their corresponding summaries as input. The method creates <strong class="source-inline">IndexNodes</strong> instances for each child Index, associating the summary with the respective Index. These <strong class="source-inline">IndexNodes</strong> instances are then used to construct the <span class="No-Break">root Index.</span></p>
			<p>During a query, the <strong class="source-inline">ComposableGraph</strong> starts with the top-level summary Index, where each Node corresponds to an underlying lower-level Index. The query is executed recursively, starting from the root Index, and traversing through the sub-Indexes. The <strong class="source-inline">ComposableGraphQueryEngine</strong> is responsible for this recursive <span class="No-Break">querying process.</span></p>
			<p>The query engine retrieves relevant Nodes from the root Index based on the query. For each relevant Node, it identifies the corresponding child Index using the <strong class="source-inline">index_id</strong> stored in the Node’s relationships. It then queries the child Index with the original query to obtain more detailed information. This process continues recursively until all relevant sub-Indexes have <span class="No-Break">been queried.</span></p>
			<p>Custom query engines can be configured for each Index within the <strong class="source-inline">ComposableGraph</strong>, allowing for tailored retrieval strategies at different levels of the hierarchy. This enables a deep, hierarchical understanding of complex datasets by seamlessly integrating information from various levels <span class="No-Break">of Indexes.</span></p>
			<p>Overall, the <strong class="source-inline">ComposableGraph</strong> allows for the efficient retrieval of relevant information from both high-level summaries and detailed, low-level Indexes, enabling a comprehensive understanding<a id="_idIndexMarker503"></a> of the <span class="No-Break">underlying data.</span></p>
			<p>Now that we have covered the Indexes available for our RAG implementation, it’s time to address the elephant in the room – <span class="No-Break"><strong class="bold">cost</strong></span><span class="No-Break">.</span></p>
			<h2 id="f_8__idParaDest-128" data-type="sect1" class="sect1" title2="Estimating the potential cost of building and querying Indexes" no2="5.7"><a id="_idTextAnchor127"></a>5.7. Estimating the potential cost of building and querying Indexes</h2>
			<p>In a similar manner to metadata<a id="_idIndexMarker504"></a> extractors, Indexes pose issues related to costs and data privacy. That is because, as we have seen in this chapter, most Indexes rely on LLMs to some extent – during building <span class="No-Break">and/or querying.</span></p>
			<p>Repeatedly calling LLMs<a id="_idIndexMarker505"></a> to process large volumes of text can quickly break your budget if you’re not paying attention to your potential costs. For example, if you are building a <strong class="source-inline">TreeIndex</strong> or <strong class="source-inline">KeywordTableIndex</strong> from thousands of documents, those constant LLM invocations during Index construction will carry a significant cost. Embeddings can also rely on calls to external models; therefore, the <strong class="source-inline">VectorStoreIndex</strong> is another important source of costs. In my experience, prevention and prediction are the best ways to avoid nasty surprises and keep your <span class="No-Break">expenses low.</span></p>
			<p>Just like with metadata extraction, I’d start first by observing and applying some <span class="No-Break">best practices:</span></p>
			<ul>
				<li>Use Indexes with no LLM calls during building where possible, such as <strong class="source-inline">SummaryIndex</strong> or <strong class="source-inline">SimpleKeywordTableIndex</strong>. This eliminates Index <span class="No-Break">building costs.</span></li>
				<li>Use cheaper LLM models. If full accuracy isn’t critical, cheaper LLM models with lower computational demands can be used but be aware of possible <span class="No-Break">quality trade-offs.</span></li>
				<li>Cache and reuse Indexes. Avoid rebuilding Indexes by caching and reusing previously <span class="No-Break">constructed ones.</span></li>
				<li>Optimize query parameters to minimize LLM calls during your search. For example, reducing <strong class="source-inline">similarity_top_k</strong> in <strong class="source-inline">VectorStoreIndex</strong> will reduce your <span class="No-Break">query cost.</span></li>
				<li>Use local models. To further manage costs and maintain data privacy when using Indexes in LlamaIndex, consider utilizing local LLM and embedding models instead of relying on hosted services. This approach not only offers more control over data privacy but also helps in reducing the dependency on external services, which can be costly. Using local models can significantly cut down on expenses, particularly when handling large volumes of data or when operating within strict <span class="No-Break">budget constraints.</span></li>
			</ul>
			<p class="callout-heading">Important side note regarding local AI models</p>
			<p class="callout">Always remember that RAG<a id="_idIndexMarker506"></a> introduces additional knowledge and contextual information into the model’s processing, effectively bridging the gap caused by a smaller training dataset. So, even for models that haven’t been trained on extensive or diverse data, RAG allows them to access a broader range of information beyond their initial training set, thus enhancing their performance and <span class="No-Break">output quality.</span></p>
			<p>These guidelines will definitely<a id="_idIndexMarker507"></a> help you reduce costs, but it’s still a good idea to estimate before indexing <span class="No-Break">larger datasets.</span></p>
			<p>Here is a basic example of how we can estimate the LLM costs of building a <strong class="source-inline">TreeIndex</strong> using <span class="No-Break">a </span><span class="No-Break"><strong class="source-inline">MockLLM</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_113" title2="(no caption)" no2="">import tiktoken
from llama_index.core import (
&nbsp;&nbsp;&nbsp;&nbsp;TreeIndex, SimpleDirectoryReader, Settings)
from llama_index.core.llms.mock import MockLLM
from llama_index.core.callbacks import (
&nbsp;&nbsp;&nbsp;&nbsp;CallbackManager, TokenCountingHandler)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In the previous part, we first took care of the necessary imports. If you’re unfamiliar with the reasons to use <strong class="source-inline">tiktoken</strong> as a tokenizer here, head back to <a href="#_idTextAnchor068"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Ingesting Data into Our RAG Workflow</em> where we discussed estimating the potential cost of using metadata extractors. Let’s set up the <span class="No-Break"><strong class="source-inline">MockLLM</strong></span><span class="No-Break"> next:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_114" title2="(no caption)" no2="">llm = MockLLM(max_tokens=256)
token_counter = TokenCountingHandler(
&nbsp;&nbsp;&nbsp;&nbsp;tokenizer=tiktoken.encoding_for_model("gpt-3.5-turbo").encode
)
callback_manager = CallbackManager([token_counter])
Settings.callback_manager=callback_manager
Settings.llm=llm</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>We just created a <strong class="source-inline">MockLLM</strong> instance with a specified maximum token limit acting as a worst-case maximal cost. We then initialized <strong class="source-inline">TokenCountingHandler</strong> with a tokenizer that matches our real LLM model using <span class="No-Break">the following:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_115" title2="(no caption)" no2="">tiktoken.encoding_for_model("gpt-3.5-turbo").encode).</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This handler will track token usage. This construct simulates an LLM without actually calling the <span class="No-Break"><strong class="source-inline">gpt-3.5-turbo</strong></span><span class="No-Break"> API:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_116" title2="(no caption)" no2="">documents = SimpleDirectoryReader(
&nbsp;&nbsp;&nbsp;&nbsp;"cost_prediction_samples").load_data()</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>We’ve loaded our documents and are now ready to build <span class="No-Break">the </span><span class="No-Break"><strong class="source-inline">TreeIndex</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_117" title2="(no caption)" no2="">index = TreeIndex.from_documents(
&nbsp;&nbsp;&nbsp;&nbsp;documents=documents,
&nbsp;&nbsp;&nbsp;&nbsp;num_children=2,
&nbsp;&nbsp;&nbsp;&nbsp;show_progress=True)
print("Total LLM Token Count:", token_counter.total_llm_token_count)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>After building the Index, the script<a id="_idIndexMarker508"></a> displays the <strong class="source-inline">total_llm_token_count</strong> value stored in <span class="No-Break">the </span><span class="No-Break"><strong class="source-inline">TokenCountingHandler</strong></span><span class="No-Break">.</span></p>
			<p>In this example, we’re only using the <strong class="source-inline">MockLLM</strong> class because there are no embeddings used for building the <strong class="source-inline">TreeIndex</strong>. This allows us to estimate the worst-case LLM token cost before actually building the Index and invoking the real LLM. The same method can be applied to estimate <span class="No-Break">query costs.</span></p>
			<p class="callout-heading">The main lesson here?</p>
			<p class="callout">While Indexes unlock many capabilities, overuse without optimization can greatly impact costs. Always estimate token usage before indexing <span class="No-Break">larger datasets.</span></p>
			<p>Here is a second example. It’s similar to the previous one, but this time, we’re first estimating the embedding costs of building a <strong class="source-inline">VectorStoreIndex</strong> and after that, the total cost of querying <span class="No-Break">the Index:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_118" title2="(no caption)" no2="">import tiktoken
from llama_index.core import (
&nbsp;&nbsp;&nbsp;&nbsp;MockEmbedding, VectorStoreIndex,
&nbsp;&nbsp;&nbsp;&nbsp;SimpleDirectoryReader, Settings)
from llama_index.core.callbacks import (
&nbsp;&nbsp;&nbsp;&nbsp;CallbackManager, TokenCountingHandler)
from llama_index.core.llms.mock import MockLLM</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The first part took care<a id="_idIndexMarker509"></a> of the imports. Next, we set up the <strong class="source-inline">MockEmbedding</strong> and <span class="No-Break"><strong class="source-inline">MockLLM</strong></span><span class="No-Break"> objects:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_119" title2="(no caption)" no2="">embed_model = MockEmbedding(embed_dim=1536)
llm = MockLLM(max_tokens=256)
token_counter = TokenCountingHandler(
&nbsp;&nbsp;&nbsp;&nbsp;tokenizer=tiktoken.encoding_for_model("gpt-3.5-turbo").encode
)
callback_manager = CallbackManager([token_counter])
Settings.embed_model=embed_model
Settings.llm=llm
Settings.callback_manager=callback_manager</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>After initializing the <strong class="source-inline">MockEmbedding</strong> and <strong class="source-inline">MockLLM</strong> objects, we defined a <strong class="source-inline">TokenCountingHandler</strong> and a <strong class="source-inline">CallbackManager</strong> and wrapped them into the custom <strong class="source-inline">Settings</strong>. It’s now time to load our sample documents and build the <strong class="source-inline">VectorStoreIndex</strong> using the <span class="No-Break">custom </span><span class="No-Break"><strong class="source-inline">Settings</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_120" title2="(no caption)" no2="">documents = SimpleDirectoryReader(
&nbsp;&nbsp;&nbsp;&nbsp;"cost_prediction_samples").load_data()
index = VectorStoreIndex.from_documents(
&nbsp;&nbsp;&nbsp;&nbsp;documents=documents,
&nbsp;&nbsp;&nbsp;&nbsp;show_progress=True)
print("Embedding Token Count:", 
&nbsp;&nbsp;&nbsp;&nbsp;token_counter.total_embedding_token_count)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>If you have successfully cloned the book’s GitHub repo, the <strong class="source-inline">cost_prediction_samples</strong> subfolder in the <strong class="source-inline">ch5</strong> folder should contain a file with a fictional story about <strong class="source-inline">Fluffy the cat</strong>. The <strong class="source-inline">VectorStoreIndex</strong> uses an embedding model to encode document text into vectors during indexing. In our second example, we estimated the token costs of those embedding calls by using <strong class="source-inline">MockEmbedding</strong> and <strong class="source-inline">TokenCountingHandler</strong>. The embedding token count provides an indication of how expensive it will be to build this Index per document based on the <span class="No-Break">text lengths.</span></p>
			<p>To have a complete view, we can take this a step further and also estimate <span class="No-Break">search costs:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_121" title2="(no caption)" no2="">query_engine = index.as_query_engine(service_context=service_context)
response = query_engine.query("What's the cat's name?")
print("Query LLM Token Count:", token_counter.total_llm_token_count)
print("Query Embedding Token Count:",
&nbsp;&nbsp;&nbsp;&nbsp;token_counter.total_embedding_token_count)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This shows the potential search fees as well by counting tokens for embedding lookups and synthesizing the response. We also had to use <strong class="source-inline">MockLLM</strong> to catch the LLM tokens hypothetically consumed during <span class="No-Break">response synthesis.</span></p>
			<p>So, in summary, follow preventive<a id="_idIndexMarker510"></a> best practices and always forecast your Index build and query expenses before unleashing them on your full <span class="No-Break">document collection!</span></p>
			<p>It’s time to make some progress with our project. Let’s revisit our <span class="No-Break">PITS project.</span></p>
			<h2 id="f_8__idParaDest-129" data-type="sect1" class="sect1" title2="Indexing our PITS study materials – hands-on" no2="5.8"><a id="_idTextAnchor128"></a>5.8. Indexing our PITS study materials – hands-on</h2>
			<p>With a solid understanding<a id="_idIndexMarker511"></a> of how indexing works in LlamaIndex, we’re now ready to implement the indexing logic in our <span class="No-Break">tutoring application.</span></p>
			<p>Let’s create the <strong class="source-inline">index_builder.py</strong> module. This module takes care of Index creation. In the current implementation, it creates two Indexes: a <strong class="source-inline">VectorStoreIndex</strong> and a <strong class="source-inline">TreeIndex</strong>. As you can see, this is a very basic implementation and there is definitely<a id="_idIndexMarker512"></a> room for improvement. Let’s handle the <span class="No-Break">imports first:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_122" title2="(no caption)" no2="">from llama_index.core import (
&nbsp;&nbsp;&nbsp;&nbsp;VectorStoreIndex, TreeIndex, load_index_from_storage)
from llama_index.core import StorageContext
from global_settings import INDEX_STORAGE
from document_uploader import ingest_documents</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Next, we’ll implement our Index <span class="No-Break">building function:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_123" title2="(no caption)" no2="">def build_indexes(nodes):
&nbsp;&nbsp;&nbsp;&nbsp;try:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;storage_context = StorageContext.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persist_dir=INDEX_STORAGE
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;vector_index = load_index_from_storage(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;storage_context, index_id="vector"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tree_index = load_index_from_storage(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;storage_context, index_id="tree"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("All indices loaded from storage.")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>We first check to see whether the Indexes have already been persisted to disk. If affirmative, then we leverage persistence to avoid the additional cost of <span class="No-Break">rebuilding them.</span></p>
			<p class="callout-heading">Note on: Notice the usage of index_id</p>
			<p class="callout">Because we have persisted<a id="_idIndexMarker513"></a> more than one Index in the same storage folder – <strong class="source-inline">INDEX_STORAGE</strong> – when using <strong class="source-inline">load_index_from_storage</strong>, we need to specify their individual IDs so that LlamaIndex can identify the <span class="No-Break">correct Index.</span></p>
			<p>If we cannot find them in the <strong class="source-inline">INDEX_STORAGE</strong> folder, we proceed to build them from the nodes. We also set an ID for each Index using <strong class="source-inline">set_index_id</strong> so that we can load them correctly<a id="_idIndexMarker514"></a> in <span class="No-Break">future sessions:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_124" title2="(no caption)" no2="">&nbsp;&nbsp;&nbsp;&nbsp;except Exception as e:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"Error occurred while loading indices: {e}")
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;storage_context = StorageContext.from_defaults()
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;vector_index = VectorStoreIndex(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nodes, storage_context=storage_context
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;vector_index.set_index_id("vector")
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tree_index = TreeIndex(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nodes, storage_context=storage_context
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tree_index.set_index_id("tree")
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;storage_context.persist(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persist_dir=INDEX_STORAGE
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("New indexes created and persisted.")
&nbsp;&nbsp;&nbsp;&nbsp;return vector_index, tree_index</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The <strong class="source-inline">build_indexes</strong> function returns the two Index objects that we’ll use later in <span class="No-Break">our application.</span></p>
			<p>That’s it for now. We’ll take<a id="_idIndexMarker515"></a> the next steps during <a href="#_idTextAnchor131"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Querying Our Data, Part 1 – </em><span class="No-Break"><em class="italic">Context Retrieval</em></span><span class="No-Break">.</span></p>
			<h2 id="f_8__idParaDest-130" data-type="sect1" class="sect1" title2="Summary" no2="5.9"><a id="_idTextAnchor129"></a>5.9. Summary</h2>
			<p>In this chapter, we explored various indexing strategies and architectures within LlamaIndex. Indexes provide essential capabilities for building performant <span class="No-Break">RAG systems.</span></p>
			<p>Throughout the chapter, we looked at the <strong class="source-inline">VectorStoreIndex</strong>, which is the most commonly used Index type. We also gained an understanding of embeddings, vector stores, similarity search, and storage contexts. These are key concepts related to <span class="No-Break">the </span><span class="No-Break"><strong class="source-inline">VectorStoreIndex</strong></span><span class="No-Break">.</span></p>
			<p>We also covered other Index types such as <strong class="source-inline">SummaryIndex</strong> for simple linear scans, <strong class="source-inline">KeywordTableIndex</strong> for keyword search, <strong class="source-inline">TreeIndex</strong> for hierarchical data, and <strong class="source-inline">KnowledgeGraphIndex</strong> for relationship-based queries. <strong class="source-inline">ComposableGraph</strong> was introduced as a tool for building multi-level Indexes, and cost estimation techniques were discussed together with <span class="No-Break">best practices.</span></p>
			<p>Overall, this chapter provided an overview of indexing capabilities in LlamaIndex, laying the foundation for building sophisticated and efficient <span class="No-Break">RAG applications.</span></p>
			<p>See you in <a href="#_idTextAnchor131"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, where we’ll discuss methods for querying our data <span class="No-Break">in LlamaIndex.</span></p>
		</div>
<div id="f_9__idContainer053" class="part" data-type="part" file="B21861_Part_3_xhtml" title2="Retrieving and Working with Indexed Data" no2="3">
			<h1 id="f_9__idParaDest-131" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor130"></a>Part 3: Retrieving and Working with Indexed Data</h1>
		</div>
<div id="f_10__idContainer071" data-type="chapter" class="chapter" file="B21861_06_xhtml" title2="Querying Our Data, Part 1 – Context Retrieval" no2="6">
			<h1 id="f_10__idParaDest-132" class="chapter-number"><a id="_idTextAnchor131"></a>6</h1>
			<h1 id="f_10__idParaDest-133"><a id="_idTextAnchor132"></a>Querying Our Data, Part 1 – Context Retrieval</h1>
			<p>The focus of this chapter will be on understanding the querying capabilities of LlamaIndex in an RAG workflow. We’ll be covering the overall working of the querying system, mostly focusing on the retrieval capabilities of <span class="No-Break">the framework.</span></p>
			<p>Here are the main sections that will be covered in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Learning about query mechanics – <span class="No-Break">an overview</span></li>
				<li>Understanding the <span class="No-Break">basic retrievers</span></li>
				<li>Building more advanced <span class="No-Break">retrieval mechanisms</span></li>
				<li>Increasing efficiency with <span class="No-Break">asynchronous retrieval</span></li>
				<li>Working with metadata filters, tools, <span class="No-Break">and selectors</span></li>
				<li>Transforming queries and <span class="No-Break">generating sub-queries</span></li>
				<li>Understanding the concepts of dense and <span class="No-Break">sparse retrieval</span></li>
			</ul>
			<h2 id="f_10__idParaDest-134" data-type="sect1" class="sect1" title2="Technical requirements" no2="6.1"><a id="_idTextAnchor133"></a>6.1. Technical requirements</h2>
			<p>For this chapter, you will need to install the <strong class="source-inline">Rank-BM25</strong> package in your environment. You can find it <span class="No-Break">at </span><a href="https://pypi.org/project/rank-bm25/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pypi.org/project/rank-bm25/</span></a><span class="No-Break">.</span></p>
			<p>Two additional integration packages are required to run the <span class="No-Break">sample code:</span></p>
			<ul>
				<li><em class="italic">OpenAI Question </em><span class="No-Break"><em class="italic">Generator</em></span><span class="No-Break">: </span><a href="https://pypi.org/project/llama-index-question-gen-openai/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pypi.org/project/llama-index-question-gen-openai/</span></a></li>
				<li><em class="italic">BM25 </em><span class="No-Break"><em class="italic">Retriever</em></span><span class="No-Break">: </span><a href="https://pypi.org/project/llama-index-retrievers-bm25/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pypi.org/project/llama-index-retrievers-bm25/</span></a></li>
				<li>All the code samples for this chapter can be found in the ch6 subfolder of this book’s GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex</span></a><span class="No-Break">.</span></li>
			</ul>
			<h2 id="f_10__idParaDest-135" data-type="sect1" class="sect1" title2="Learning about query mechanics – an overview" no2="6.2"><a id="_idTextAnchor134"></a>6.2. Learning about query mechanics – an overview</h2>
			<p>In this chapter, we will finally begin to reap the fruits of our work so far. Document ingestion, parsing and segmenting, metadata extraction, and index building were all just preparatory steps for what we are about to<a id="_idIndexMarker516"></a> discuss: <strong class="bold">querying</strong>. At the heart of any RAG workflow is the idea of being able to bring relevant context into the prompt we use in the LLM query. So far, we have been concerned with constructing and organizing this context, but now, it is time to <a id="_idIndexMarker517"></a>use it and extract the best possible answers from our interactions with LLMs. In the following sections, we will discuss various techniques that LlamaIndex provides us for the query part. As usual, we will start with the simplest<a id="_idIndexMarker518"></a> query methods – called <em class="italic">naive</em> methods in jargon – and then discuss more advanced <span class="No-Break">query variants.</span></p>
			<p>First, we need to <a id="_idIndexMarker519"></a>understand the<a id="_idIndexMarker520"></a> typical steps in the query<a id="_idIndexMarker521"></a> process: <strong class="bold">retrieval</strong>, <strong class="bold">postprocessing</strong>, and <span class="No-Break"><strong class="bold">response synthesis</strong></span><span class="No-Break">.</span></p>
			<p>In <a href="#_idTextAnchor045"><span class="No-Break"><em class="italic">Chapter 3</em></span></a><em class="italic">,</em> <em class="italic">Kickstarting Your Journey with LlamaIndex</em>, in the <em class="italic">Indexes</em> section, we discussed the simplest way to go through the three steps – using <strong class="source-inline">QueryEngine</strong> but built very simply by running <strong class="source-inline">index.as_query_engine()</strong>. This is very simple but not necessarily always effective as this <em class="italic">naive</em> way of querying an index is just the tip of the iceberg. We will now explore the three mechanisms individually and understand how they work and the customizable options <span class="No-Break">they offer.</span></p>
			<p>First, we’ll focus <span class="No-Break">on </span><span class="No-Break"><strong class="bold">retrievers</strong></span><span class="No-Break">.</span></p>
			<h2 id="f_10__idParaDest-136" data-type="sect1" class="sect1" title2="Understanding the basic retrievers" no2="6.3"><a id="_idTextAnchor135"></a>6.3. Understanding the basic retrievers</h2>
			<p><strong class="bold">Retrieval mechanisms</strong> are a central<a id="_idIndexMarker522"></a> element in any RAG system. Although they work in different ways, all<a id="_idIndexMarker523"></a> types of retrievers are based on the same principle: they browse an index and select the relevant nodes to build the necessary context. Each index type offers several retrieval modes, each providing different features and customization options. Regardless of the retriever type, the result that will be returned is in the form of a <strong class="source-inline">NodeWithScore</strong> object – a structure that combines a node with an associated score. The score can be useful further in the RAG flow because it allows us to sort the returned nodes according to their relevance. However, keep in mind that while all retrievers return <strong class="source-inline">NodeWithScore</strong>, not all of them<a id="_idIndexMarker524"></a> associate a specific <span class="No-Break">node score.</span></p>
			<p>As usual, LlamaIndex offers multiple alternatives to accomplish a task, so a retriever can be constructed in several <a id="_idIndexMarker525"></a>ways. The simplest path is direct construction from an <strong class="source-inline">Index</strong> object. Assuming that we have already dealt with document ingestion, the following code builds an index and then builds a retriever based on the structure of <span class="No-Break">the index:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_125" title2="(no caption)" no2="">from llama_index.core import SummaryIndex, SimpleDirectoryReader
documents = SimpleDirectoryReader("files").load_data()
summary_index = SummaryIndex.from_documents(documents)
retriever = summary_index.as_retriever(
&nbsp;&nbsp;&nbsp;&nbsp;retriever_mode='embedding'
)
result = retriever.retrieve("Tell me about ancient Rome")
print(result[0].text)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In the previous example, the generated retriever is of the <strong class="source-inline">SummaryIndexRetriever</strong> type. This is the default retriever for <span class="No-Break">this index.</span></p>
			<p>The second option is direct instantiation, as shown in the <span class="No-Break">following example:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_126" title2="(no caption)" no2="">from llama_index.core import SummaryIndex, SimpleDirectoryReader
from llama_index.core.retrievers import SummaryIndexEmbeddingRetriever
documents = SimpleDirectoryReader("files").load_data()
summary_index = SummaryIndex.from_documents(documents)
retriever = SummaryIndexEmbeddingRetriever(
&nbsp;&nbsp;&nbsp;&nbsp;index=summary_index
)
result = retriever.retrieve("Tell me about ancient Rome")
print(result[0].text)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In the next section, we’ll go through a list of retrieval options that are available for each index type. Next to each retriever type, I’ve specified how it can be instantiated from the corresponding index. I warn you <a id="_idIndexMarker526"></a>now that a lot of information has been condensed in the next section. However, it is useful information that you can bookmark and come back to later when you start<a id="_idIndexMarker527"></a> building real applications with the <span class="No-Break">LlamaIndex framework.</span></p>
			<p>So, here’s the list of retrievers for each type <span class="No-Break">of index.</span></p>
			<h3 id="f_10__idParaDest-137" data-type="sect2" class="sect2" title2="The VectorStoreIndex retrievers" no2="6.3.1"><a id="_idTextAnchor136"></a>6.3.1. The VectorStoreIndex retrievers</h3>
			<p>We have two retriever<a id="_idIndexMarker528"></a> options available for this index. Let’s have a look at how <a id="_idIndexMarker529"></a>they work and how to customize them for different <span class="No-Break">use cases.</span></p>
			<h4 data-type="sect3" class="sect3" title2="VectorIndexRetriever" no2="6.3.1.1">6.3.1.1. VectorIndexRetriever</h4>
			<p>The default retriever that’s used by <strong class="source-inline">VectorStoreIndex</strong> is <strong class="source-inline">VectorIndexRetriever</strong>. It can easily be <a id="_idIndexMarker530"></a>constructed using the <span class="No-Break">following command:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_127" title2="(no caption)" no2="">VectorStoreIndex.as_retriever()</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>As expected, since <strong class="source-inline">VectorStoreIndex</strong> is one of the most sophisticated and widely used indexes, this retriever is <span class="No-Break">also complex.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.1</em> exemplifies its <span class="No-Break">operating mode:</span></p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B21861_06_1.jpg" alt="Figure 6.1 – Node retrieval using VectorIndexRetriever" width="1650" height="780" data-type="figure" id="untitled_figure_35" title2="– Node retrieval using VectorIndexRetriever" no2="6.1">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Node retrieval using VectorIndexRetriever</p>
			<p>This retriever operates by converting queries into vectors and then performing <em class="italic">similarity-based</em> searches in the vector <a id="_idIndexMarker531"></a>space. Several parameters can be customized for different <span class="No-Break">use cases:</span></p>
			<ul>
				<li><strong class="source-inline">similarity_top_k</strong>: This defines the number of <em class="italic">top (k)</em> results returned by the retriever. This determines how many of the most similar results are returned for each query. For example, if we want a broader search, we can change the default value, which <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">2</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">vector_store_query_mode</strong>: This sets the query mode of the vector store. Different variants of external vector stores, such as <em class="italic">Pinecone</em> (<a href="https://www.pinecone.io/" target="_blank" rel="noopener noreferrer">https://www.pinecone.io/</a>), <em class="italic">OpenSearch</em> (<a href="https://opensearch.org/" target="_blank" rel="noopener noreferrer">https://opensearch.org/</a>), and others, support different query modes. This is the mechanism by which we can make best use of their <span class="No-Break">search capabilities.</span></li>
				<li><strong class="source-inline">filters</strong>: Remember that in <span class="No-Break"><em class="italic">Chapter 3</em></span>, in the <em class="italic">Nodes</em> section, we saw how to add metadata to our nodes? Well, we can use this metadata to narrow down the search scope of the retriever. We will see a practical example of this in this chapter, where we will use metadata filters to implement a simple system for filtering nodes returned by <span class="No-Break">an index.</span></li>
				<li><strong class="source-inline">alpha</strong>: This one is useful when using a hybrid search mode (a combination of sparse and <strong class="bold">dense search</strong>). We will <a id="_idIndexMarker532"></a>discuss the difference between sparse and dense search in more detail later in <span class="No-Break">this chapter.</span></li>
				<li><strong class="source-inline">sparse_top_k</strong>: The number of top <a id="_idIndexMarker533"></a>results for the <strong class="bold">sparse search</strong>. This is relevant in hybrid search modes. The previous mention applies <span class="No-Break">here also.</span></li>
				<li><strong class="source-inline">doc_ids</strong>: Similar to metadata filters, but slightly coarser, <strong class="source-inline">doc_ids</strong> can be used to restrict the search to a specific subset of documents. For example, suppose the organization uses a<a id="_idIndexMarker534"></a> common knowledge base that is shared by all departments. At the same time, however, the organization has a clear naming convention for documents. If the department’s name or code is found in the document name, we could use this parameter to limit a user’s query to documents in their <span class="No-Break">department only.</span></li>
				<li><strong class="source-inline">node_ids</strong>: This parameter is similar to <strong class="source-inline">doc_ids</strong> but refers to node IDs within the index. This can give us even more granular control over the information that’s returned by <span class="No-Break">the retriever.</span></li>
				<li><strong class="source-inline">vector_store_kwargs</strong>: This parameter can pass additional arguments that are specific to each vector store so that they can be sent at <span class="No-Break">query time.</span></li>
			</ul>
			<p>As a secure design principle, security should be implemented as early as possible in the life cycle of an application. This is also true for an RAG application. For example, if we want to better control access to information, we should filter the information that’s processed by the application as early as possible. In an RAG flow, which means from the moment it is retrieved – if not earlier. There are ways to filter the information later in the query engine – for example, in post-processing or even in response synthesis – but it is much easier not to introduce risks in the first place by introducing information into the flow that is outside the user’s security context. There is also a cost issue. Since much of the processing in an RAG flow is based on LLM ingestion, the less information we process, the lower <span class="No-Break">the cost.</span></p>
			<h4 data-type="sect3" class="sect3" title2="VectorIndexAutoRetriever" no2="6.3.1.2">6.3.1.2. VectorIndexAutoRetriever</h4>
			<p>All the parameters we discussed earlier regarding <strong class="source-inline">VectorIndexRetriever</strong> are very useful when we know exactly what we are looking for and understand the structure of the data very well. Unfortunately, in some situations, we will be dealing with complex structures or ambiguities in the <span class="No-Break">indexed data.</span></p>
			<p><strong class="source-inline">VectorIndexAutoRetriever</strong> is a more advanced form of retriever that can use an LLM to automatically set query parameters in a vector store based on a natural language description of the content and supporting metadata. This is particularly useful when users are unfamiliar with the<a id="_idIndexMarker535"></a> structure of the data or do not know how to formulate an effective query. In these situations, this retriever can transform vague or unclear queries into more structured queries and better leverage the capabilities of the vector store, thus increasing the chances of finding relevant results. Since a detailed discussion of this mechanism would take several pages and I am probably digressing too much from the main topic, if you want to learn more about how it works, I suggest that you consult the official documentation <span class="No-Break">at </span><a href="https://docs.llamaindex.ai/en/stable/examples/vector_stores/elasticsearch_auto_retriever.html" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/vector_stores/elasticsearch_auto_retriever.html</span></a><span class="No-Break">.</span></p>
			<h4 data-type="sect3" class="sect3" title2="The SummaryIndex retrievers" no2="6.3.1.3">6.3.1.3. The SummaryIndex retrievers</h4>
			<p>There are three retriever options available for this<a id="_idIndexMarker536"></a> index. Let’s take <span class="No-Break">a look.</span></p>
			<h4 data-type="sect3" class="sect3" title2="SummaryIndexRetriever" no2="6.3.1.4">6.3.1.4. SummaryIndexRetriever</h4>
			<p>This retriever can be built using the<a id="_idIndexMarker537"></a> <span class="No-Break">following command:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_128" title2="(no caption)" no2="">SummaryIndex.as_retriever(retriever_mode = 'default')</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This is the default retriever for <strong class="source-inline">SummaryIndex</strong>. As seen in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.2</em>, it has a very simple approach – it returns all nodes in the index without applying any filtering <span class="No-Break">or sorting:</span></p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B21861_06_2.jpg" alt="Figure 6.2 – Retrieving nodes using SummaryIndexRetriever" width="1650" height="663" data-type="figure" id="untitled_figure_36" title2="– Retrieving nodes using SummaryIndexRetriever" no2="6.2">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Retrieving nodes using SummaryIndexRetriever</p>
			<p>This is useful when we want to <a id="_idIndexMarker538"></a>get a complete view of the data in the index, without having to filter or sort the results. No relevance score is returned for <span class="No-Break">the nodes.</span></p>
			<h4 data-type="sect3" class="sect3" title2="SummaryIndexEmbeddingRetriever" no2="6.3.1.5">6.3.1.5. SummaryIndexEmbeddingRetriever</h4>
			<p>We can build this one with the<a id="_idIndexMarker539"></a> <span class="No-Break">following command:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_129" title2="(no caption)" no2="">SummaryIndex.as_retriever(retriever_mode='embedding')</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This retriever relies on embeddings to retrieve nodes from <strong class="source-inline">SummaryIndex</strong>. While <strong class="source-inline">SummaryIndex</strong> itself stores nodes in plain text, this retriever uses an embedding model to convert these plain text nodes into embeddings when a query is made. Have a look at <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.3</em> to get a better view of its <span class="No-Break">operating mode:</span></p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/B21861_06_3.jpg" alt="Figure 6.3 – Inner workings of SummaryIndexEmbeddingRetriever" width="1650" height="642" data-type="figure" id="untitled_figure_37" title2="– Inner workings of SummaryIndexEmbeddingRetriever" no2="6.3">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – Inner workings of SummaryIndexEmbeddingRetriever</p>
			<p>The embeddings are created dynamically as needed for retrieval, rather than being stored persistently with the index. The <strong class="source-inline">similarity_top_k</strong> parameter determines the number of nodes <a id="_idIndexMarker540"></a>to return, based on their similarity to the query. This retriever is useful for finding the most relevant nodes concerning a given query by using <span class="No-Break">similarity computation.</span></p>
			<p>For each selected node, the retriever calculates a similarity score – based on embeddings – which is then returned alongside the node as <strong class="source-inline">NodeWithScore</strong>. This score is a reflection of the extent to which each node corresponds to <span class="No-Break">the query.</span></p>
			<h4 data-type="sect3" class="sect3" title2="SummaryIndexLLMRetriever" no2="6.3.1.6">6.3.1.6. SummaryIndexLLMRetriever</h4>
			<p>This retriever can be built using the<a id="_idIndexMarker541"></a> <span class="No-Break">following command:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_130" title2="(no caption)" no2="">SummaryIndex.as_retriever(retriever_mode='llm')</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>As its name suggests, this retriever uses an LLM to retrieve nodes from <strong class="source-inline">SummaryIndex</strong>. It uses a prompt to select the most relevant nodes. Check out <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.4</em> for an overview of <span class="No-Break">its approach:</span></p>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/B21861_06_4.jpg" alt="Figure 6.4 – SummaryIndexLLMRetriever in action" width="1650" height="663" data-type="figure" id="untitled_figure_38" title2="– SummaryIndexLLMRetriever in action" no2="6.4">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – SummaryIndexLLMRetriever in action</p>
			<p>If we wish, we can override the default prompt using the <strong class="source-inline">choice_select_prompt</strong> parameter. Queries are processed in<a id="_idIndexMarker542"></a> batches; the size of each batch is determined by the <strong class="source-inline">choice_batch_size</strong> parameter. Optionally, we can also provide the <strong class="source-inline">format_node_batch_fn</strong> and <strong class="source-inline">parse_choice_select_answer_fn</strong> functions as parameters. These are used to format the batch of nodes and parse the LLM responses. The <strong class="source-inline">parse_choice_select_answer_fn</strong> function is also responsible for calculating node-specific relevance scores. The scores are determined by parsing the LLM responses. These scores are then associated with the corresponding nodes and returned as <strong class="source-inline">NodeWithScore</strong>. If we don’t want to use the default LLM, that’s not a problem: the retriever accepts <strong class="source-inline">service_context</strong> as a parameter. In <span class="No-Break">Chapter 3</span>, we saw how to customize the default LLM <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">ServiceContext</strong></span><span class="No-Break">.</span></p>
			<p>This type of retriever is useful in complex search systems where LLMs can provide contextual and detailed answers <span class="No-Break">to queries.</span></p>
			<p>Next, we’ll talk about retrievers <span class="No-Break">for </span><span class="No-Break"><strong class="source-inline">DocumentSummaryIndex</strong></span><span class="No-Break">.</span></p>
			<h3 id="f_10__idParaDest-138" data-type="sect2" class="sect2" title2="The DocumentSummaryIndex retrievers" no2="6.3.2"><a id="_idTextAnchor137"></a>6.3.2. The DocumentSummaryIndex retrievers</h3>
			<p>For this index, we only have <a id="_idIndexMarker543"></a>two retrieval options. Let’s<a id="_idIndexMarker544"></a> take <span class="No-Break">a look.</span></p>
			<h4 data-type="sect3" class="sect3" title2="DocumentSummaryIndexLLMRetriever" no2="6.3.2.1">6.3.2.1. DocumentSummaryIndexLLMRetriever</h4>
			<p>We can build this with the <span class="No-Break">following command:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_131" title2="(no caption)" no2="">DocumentSummaryIndex.as_retriever(retriever_mode='llm')</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This retriever uses an<a id="_idIndexMarker545"></a> LLM to select relevant summaries from an index of document summaries. You can get a better understanding of how it works by looking at <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B21861_06_5.jpg" alt="Figure 6.5 – How DocumentSummaryIndexLLMRetriever works" width="1650" height="620" data-type="figure" id="untitled_figure_39" title2="– How DocumentSummaryIndexLLMRetriever works" no2="6.5">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – How DocumentSummaryIndexLLMRetriever works</p>
			<p>This retriever processes queries in batches, with each batch containing a specified number of nodes to send to the LLM for evaluation. The <strong class="source-inline">choice_batch_size</strong> parameter can be used to specify the size of a batch. The retriever can use a custom prompt provided via the <strong class="source-inline">choice_select_prompt</strong> parameter to determine the relevance of the abstracts to the query. Results are sorted by relevance and returned according to the number specified by <strong class="source-inline">choice_top_k</strong>. The <strong class="source-inline">format_node_batch_fn</strong> and <strong class="source-inline">parse_choice_select_answer_fn</strong> functions can also be specified as parameters. The first function, <strong class="source-inline">format_node_batch_fn</strong>, prepares the information from nodes in a format suitable for the LLM. This may include combining text from multiple nodes, structuring the information in a particular way, or adding contextual elements to help the LLM understand and evaluate the content. The second function, <strong class="source-inline">parse_choice_select_answer_fn</strong>, can, for example, determine which nodes are most relevant to the query and extract relevance scores or other metrics associated with each node. By analyzing the LLM response, this function allows the retriever to decide which nodes are most relevant to the user’s query. To summarize, <strong class="source-inline">DocumentSummaryIndexLLMRetriever</strong> is useful for retrieving useful data from a large number of documents using the natural language processing power of LLMs. As a useful side note, it is good to know that this retriever also returns the relevance score that is <a id="_idIndexMarker546"></a>associated with each of <span class="No-Break">the nodes.</span></p>
			<p class="callout-heading">Additional observation</p>
			<p class="callout">During my experimentation with this type of retriever, I noticed that the relevance scores that are assigned to each node by the LLM were consistently high, often reaching the maximum value of 10 (tested using GPT3.5-Turbo). For applications where nuanced differentiation between degrees of relevance is crucial, it might be beneficial to adjust the prompt or apply post-processing to the LLM’s responses to achieve a more balanced and nuanced distribution of relevance scores. This issue also underscores the importance of tailoring LLM prompts and response handling to suit the specific needs and contexts of different applications. We’ll talk more about prompt customization in <a href="#_idTextAnchor214"><span class="No-Break"><em class="italic">Chapter 10</em></span></a><span class="No-Break">.</span></p>
			<h4 data-type="sect3" class="sect3" title2="DocumentSummaryIndexEmbeddingRetriever" no2="6.3.2.2">6.3.2.2. DocumentSummaryIndexEmbeddingRetriever</h4>
			<p>To build this retriever, we can use<a id="_idIndexMarker547"></a> the <span class="No-Break">following code:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_132" title2="(no caption)" no2="">DocumentSummaryIndex.as_retriever(
&nbsp;&nbsp;&nbsp;&nbsp;retriever_mode='embedding'
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This retriever relies on embeddings to retrieve summary nodes from the index. <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.6</em> exemplifies <span class="No-Break">its operation:</span></p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B21861_06_5.jpg" alt="Figure 6.6 – DocumentSummaryIndexEmbeddingRetriever" width="1650" height="620" data-type="figure" id="untitled_figure_40" title2="– DocumentSummaryIndexEmbeddingRetriever" no2="6.6">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – DocumentSummaryIndexEmbeddingRetriever</p>
			<p>It computes the <a id="_idIndexMarker548"></a>embeddings for the query and then finds the summaries with the highest similarity to the query. For this method to work, the index should have been built with the <strong class="source-inline">embed_summaries</strong> parameters set to <strong class="source-inline">True</strong>. The <strong class="source-inline">similarity_top_k</strong> parameter specifies the number of summary nodes to return based on similarity. The retriever does not return a relevance score associated with <span class="No-Break">each node.</span></p>
			<p>It is effective for finding the most relevant summaries relative to a given query, using similarity calculation techniques based <span class="No-Break">on embeddings.</span></p>
			<h3 id="f_10__idParaDest-139" data-type="sect2" class="sect2" title2="The TreeIndex retrievers" no2="6.3.3"><a id="_idTextAnchor138"></a>6.3.3. The TreeIndex retrievers</h3>
			<p>This is a more complex<a id="_idIndexMarker549"></a> index type that constructs a tree graph of nodes, as<a id="_idIndexMarker550"></a> we saw in <a href="#_idTextAnchor104"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><em class="italic">,  Indexing with LlamaIndex</em>, in the <em class="italic">Other index types in </em><span class="No-Break"><em class="italic">LlamaIndex</em></span><span class="No-Break"> section.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout"><strong class="source-inline">TreeIndex</strong>, by its very nature, is designed to reflect hierarchical relationships within data, making it a great tool for scenarios where data is naturally organized in a tree-like structure, such as filesystems, organizational charts, or product categories. That being said, the LlamaIndex implementation of this structure is a tree of summaries about the data. Regardless of any existing structure in the initial document, this index builds a parallel hierarchical structure by chunking it down and creating summaries at each level of the tree. Because of the recursive nature of <strong class="source-inline">TreeSelectLeafRetriever</strong> and <strong class="source-inline">TreeSelectLeafEmbeddingRetriever</strong>, navigating this structure at query time could be more computationally expensive than with other types of indexes. This recursive process adds computational overhead, especially for deep trees or <span class="No-Break">large datasets.</span></p>
			<p>That being said, we have<a id="_idIndexMarker551"></a> several ways to <span class="No-Break">query </span><span class="No-Break"><strong class="source-inline">TreeIndex</strong></span><span class="No-Break">.</span></p>
			<h4 data-type="sect3" class="sect3" title2="TreeSelectLeafRetriever" no2="6.3.3.1">6.3.3.1. TreeSelectLeafRetriever</h4>
			<p>We can construct this retriever <span class="No-Break">like this:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_133" title2="(no caption)" no2="">TreeIndex.as_retriever(retriever_mode='select_leaf').</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This is also the default retriever<a id="_idIndexMarker552"></a> that’s used by <strong class="source-inline">TreeIndex</strong>. Its purpose is to recursively navigate the index structure and identify the leaf nodes that are most relevant to the query being formulated. This can be seen in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/B21861_06_7.jpg" alt="Figure 6.7 – TreeSelectLeafRetriever configured with a child_branch_factor argument value of 1" width="1649" height="648" data-type="figure" id="untitled_figure_41" title2="– TreeSelectLeafRetriever configured with a child_branch_factor argument value of 1" no2="6.7">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – TreeSelectLeafRetriever configured with a child_branch_factor argument value of 1</p>
			<p>The <strong class="source-inline">child_branch_factor</strong> argument specifies the number of child nodes to be considered at each level of the tree. Setting a higher value can result in a more exhaustive search and increase the chance of finding the most relevant nodes. However, this has the disadvantage of increasing the computational cost and processing time. If no value is specified, the retriever defaults to a value of <strong class="source-inline">1</strong>. Another very useful parameter is <strong class="source-inline">Verbose</strong>, which, when set to <strong class="source-inline">True</strong>, causes the detailed selection process to be displayed. This is a very good way to understand how the retriever works or troubleshoot possible execution problems. The nodes that are returned by this retriever do not contain an associated <a id="_idIndexMarker553"></a>relevance score. As this retriever uses an LLM for node selection, several parameters can be used to customize <span class="No-Break">the prompts:</span></p>
			<ul>
				<li><strong class="source-inline">query_template</strong>: This is a prompt template that we can use to customize queries for <span class="No-Break">the LLM</span></li>
				<li><strong class="source-inline">text_qa_template</strong>: This is another template that’s used for text-based Q&amp;A queries. It is used to get specific answers from <span class="No-Break">text nodes</span></li>
				<li><strong class="source-inline">refine_template</strong>: This template is used to refine or enhance the initial answers that are obtained from the LLM. It can be used to add additional context or <span class="No-Break">clarify answers</span></li>
				<li><strong class="source-inline">query_template_multiple</strong>: An alternative prompt template that allows queries to be formulated for multiple nodes simultaneously. It is useful when using a <strong class="source-inline">child_branch_factor</strong> argument that’s higher <span class="No-Break">than 1</span></li>
			</ul>
			<p>We’ll talk <span class="No-Break">about </span><span class="No-Break"><strong class="source-inline">TreeSelectEmbeddingRetriever</strong></span><span class="No-Break">.</span></p>
			<h4 data-type="sect3" class="sect3" title2="TreeSelectLeafEmbeddingRetriever" no2="6.3.3.2">6.3.3.2. TreeSelectLeafEmbeddingRetriever</h4>
			<p>This particular kind of retriever can be built using the <span class="No-Break">following code:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_134" title2="(no caption)" no2="">TreeIndex.as_retriever(
&nbsp;&nbsp;&nbsp;&nbsp;retriever_mode='select_leaf_embedding'
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>As its name suggests, this retriever <a id="_idIndexMarker554"></a>navigates the index by using the similarity of the embeddings between the query and the node text to select the <span class="No-Break">relevant nodes.</span></p>
			<p>This process is recursive, navigating all levels of the tree. It works almost identically to <strong class="source-inline">TreeSelectLeafRetriever</strong>, with the only difference being that it uses embeddings for <span class="No-Break">node selection.</span></p>
			<p>The parameters we discussed earlier are also valid here, but there is an additional parameter: <strong class="source-inline">embed_model</strong>. This <a id="_idIndexMarker555"></a>can be used to specify a preferred embedding model. As with the previous retriever, the nodes that are returned by this retriever do not contain an associated <span class="No-Break">relevance score.</span></p>
			<h4 data-type="sect3" class="sect3" title2="TreeAllLeafRetriever" no2="6.3.3.3">6.3.3.3. TreeAllLeafRetriever</h4>
			<p>Here’s the fastest way to <a id="_idIndexMarker556"></a>construct <span class="No-Break">this retriever:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_135" title2="(no caption)" no2="">TreeIndex.as_retriever(retriever_mode='all_leaf')</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>You can find an explanatory diagram in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.8</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/B21861_06_8.jpg" alt="Figure 6.8 – Retrieving all nodes by using TreeAllLeafRetriever" width="1649" height="614" data-type="figure" id="untitled_figure_42" title2="– Retrieving all nodes by using TreeAllLeafRetriever" no2="6.8">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 – Retrieving all nodes by using TreeAllLeafRetriever</p>
			<p>This retriever is useful for its ability to analyze a large amount of data, ensuring that no potentially relevant information is missed in the response generation process. In a similar way to <strong class="source-inline">SummaryIndexRetriever</strong>, this retriever extracts all nodes from the index and sorts them, regardless of their position in the hierarchy. This is akin to a bulk retrieval but without it returning any <span class="No-Break">relevance score.</span></p>
			<h4 data-type="sect3" class="sect3" title2="TreeRootRetriever" no2="6.3.3.4">6.3.3.4. TreeRootRetriever</h4>
			<p>We can build this with the <span class="No-Break">following command:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_136" title2="(no caption)" no2="">TreeIndex.as_retriever(retriever_mode='root')</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Unlike <strong class="source-inline">TreeAllLeafRetriever</strong>, this retriever focuses on retrieving responses directly from the root nodes of the tree. It <a id="_idIndexMarker557"></a>assumes that the index tree already stores the response. Unlike other methods that might parse information down the tree to extract relevant nodes, <strong class="source-inline">TreeRootRetriever</strong> relies on the fact that the answer is already at the root level. <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.9</em> provides a <span class="No-Break">visual explanation:</span></p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/B21861_06_9.jpg" alt="Figure 6.9 – Retrieving from the root of the tree" width="1650" height="697" data-type="figure" id="untitled_figure_43" title2="– Retrieving from the root of the tree" no2="6.9">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.9 – Retrieving from the root of the tree</p>
			<p>It is effective in cases where essential information is aggregated or synthesized at the top level of the data structure, such as data summaries, general conclusions, or answers to frequently asked questions. This retriever also does not return relevance scores associated <span class="No-Break">with nodes.</span></p>
			<p class="callout-heading">Practical use case</p>
			<p class="callout">A practical example would be a <strong class="bold">clinical decision support system</strong> (<strong class="bold">CDSS</strong>) in the medical field. Imagine such a <a id="_idIndexMarker558"></a>system having a <strong class="source-inline">TreeIndex</strong> retriever in which each root node represents a specific medical question and the corresponding answers or clinical advice are pre-computed and stored in these root nodes. For example, the root nodes may store a pre-computed answer such as <em class="italic">Common symptoms of COVID-19 include fever, dry cough, tiredness, and so on</em>. In this scenario, when a doctor or patient interrogates the system with the <em class="italic">Symptoms of a COVID-19 infection</em> query, this retriever will look at the appropriate root node and return the pre-computed answer without any additional <a id="_idIndexMarker559"></a>processing or having to traverse the tree to <span class="No-Break">find information.</span></p>
			<h4 data-type="sect3" class="sect3" title2="The KeywordTableIndex retrievers" no2="6.3.3.5">6.3.3.5. The KeywordTableIndex retrievers</h4>
			<p>The retrieval process from <strong class="source-inline">KeywordTableIndex</strong> starts by extracting the relevant keywords from the query given to the <a id="_idIndexMarker560"></a>retriever. Extraction can be done in several ways, depending on the retriever being used. Once the keywords have been extracted, the retriever counts their frequency in the different indexed. All retrievers that are available for this index operate as described in  <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.10</em>. The only difference is the method that’s used to extract <span class="No-Break">the keywords:</span></p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/B21861_06_10.jpg" alt="Figure 6.10 – KeywordTableIndex" width="1649" height="511" data-type="figure" id="untitled_figure_44" title2="– KeywordTableIndex" no2="6.10">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.10 – KeywordTableIndex</p>
			<p>The nodes are sorted by the number of matching keywords, usually in descending order of relevance, and returned as a <span class="No-Break"><strong class="source-inline">NodeWithScore</strong></span><span class="No-Break"> response.</span></p>
			<p>It’s worth noting that queries against this type of index do not return a relevance score associated with <span class="No-Break">the nodes.</span></p>
			<p>Let’s have a look at the available retrievers for <span class="No-Break">this Index.</span></p>
			<h4 data-type="sect3" class="sect3" title2="KeywordTableGPTRetriever" no2="6.3.3.6">6.3.3.6. KeywordTableGPTRetriever</h4>
			<p>We can build this type of retriever <a id="_idIndexMarker561"></a>with the <span class="No-Break">following command:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_137" title2="(no caption)" no2="">KeywordTableIndex.as_retriever(retriever_mode='default')</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>It uses an LLM query to identify relevant keywords in a query and then returns the nodes associated with <span class="No-Break">those keywords.</span></p>
			<h4 data-type="sect3" class="sect3" title2="KeywordTableSimpleRetriever" no2="6.3.3.7">6.3.3.7. KeywordTableSimpleRetriever</h4>
			<p>This retriever can be built <span class="No-Break">as follows:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_138" title2="(no caption)" no2="">KeywordTableIndex.as_retriever(retriever_mode='simple')</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This is a simpler method that does<a id="_idIndexMarker562"></a> not use the LLM and is faster. However, it may be less efficient at identifying complex or contextual keywords. It uses a regular expression-based <span class="No-Break">keyword extractor.</span></p>
			<h4 data-type="sect3" class="sect3" title2="KeywordTableRAKERetriever" no2="6.3.3.8">6.3.3.8. KeywordTableRAKERetriever</h4>
			<p>To define this, we can use the <a id="_idIndexMarker563"></a><span class="No-Break">following command:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_139" title2="(no caption)" no2="">KeywordTableIndex.as_retriever(retriever_mode='rake')</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Similar to the previous retriever, this one uses the <em class="italic">RAKE method</em> to efficiently extract relevant keywords. We discussed the RAKE method in <a href="#_idTextAnchor104"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Indexing with LlamaIndex</em>, i<em class="italic">n the A simple usage model for </em><span class="No-Break"><em class="italic">KeywordTableIndex</em></span><span class="No-Break"> section.</span></p>
			<p>There are also several common arguments that we can use to set up the retrievers <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">KeywordTableIndex</strong></span><span class="No-Break">:</span></p>
			<ul>
				<li><strong class="source-inline">query_keyword_extract_template</strong>: This is used to change the default prompt that’s used to extract keywords from the text of a query. This can only be applied to the <span class="No-Break">default mode.</span></li>
				<li><strong class="source-inline">max_keywords_per_query</strong>: This specifies the maximum number of keywords that can be extracted from a single query. This parameter is important to control query complexity and to avoid overloading the system with too <span class="No-Break">many keywords.</span></li>
				<li><strong class="source-inline">num_chunks_per_query</strong>: This specifies the maximum number of chunks that can be retrieved in a query. This parameter helps limit the amount of data that can be processed in a single query, optimizing system performance <span class="No-Break">and efficiency.</span></li>
			</ul>
			<p>Next, we’ll talk about how to<a id="_idIndexMarker564"></a> retrieve data from <span class="No-Break">knowledge graphs.</span></p>
			<h3 id="f_10__idParaDest-140" data-type="sect2" class="sect2" title2="The KnowledgeGraphIndex retrievers" no2="6.3.4"><a id="_idTextAnchor139"></a>6.3.4. The KnowledgeGraphIndex retrievers</h3>
			<p>As discussed in the previous chapter, this<a id="_idIndexMarker565"></a> type of Index constructs a <a id="_idIndexMarker566"></a>graph made up of <em class="italic">triplets</em>. Each <strong class="bold">triplet</strong> consists of a subject, a predicate, and an object. The <strong class="bold">subject</strong> is the entity or concept about which a statement is being made. The <strong class="bold">predicate</strong> is the relationship or verb that links the subject to the object, describing<a id="_idIndexMarker567"></a> how the two are related, and the object is the entity or concept that is linked to the subject by the predicate. At the core of this index, there are two retrievers, <strong class="source-inline">KGTableRetriever</strong> and <strong class="source-inline">KnowledgeGraphRAGRetriever</strong>, both of which extract relevant nodes from a knowledge graph based <span class="No-Break">on queries.</span></p>
			<p><strong class="source-inline">KGTableRetriever</strong> is the default retriever for <strong class="source-inline">KnowledgeGraphIndex</strong> and can be configured in three retrieval modes: using keywords only, using embeddings only, or a combination of both – in hybrid mode. All modes operate as described in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.11</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/B21861_06_11.jpg" alt="Figure 6.11 – The inner workings of KGTableRetriever" width="1650" height="693" data-type="figure" id="untitled_figure_45" title2="– The inner workings of KGTableRetriever" no2="6.11">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.11 – The inner workings of KGTableRetriever</p>
			<p>Let’s look at how they work under <span class="No-Break">the hood.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Keyword mode" no2="6.3.4.1">6.3.4.1. Keyword mode</h4>
			<p>The retriever can be built in this<a id="_idIndexMarker568"></a> mode using the <span class="No-Break">following command:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_140" title2="(no caption)" no2="">KnowledgeGraphIndex.as_retriever(retriever_mode='keyword')</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>When configured in keyword mode, the retriever uses keywords extracted from the query to find relevant nodes containing <span class="No-Break">those keywords.</span></p>
			<p>Keywords are evaluated in case-sensitive mode. This means that on a hypothetical index, a query of the form <em class="italic">where is the Colosseum?</em> will return a correct result, while <em class="italic">where is the colosseum?</em> will return <span class="No-Break">no nodes.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Embedding mode" no2="6.3.4.2">6.3.4.2. Embedding mode</h4>
			<p>We can set it to this mode using<a id="_idIndexMarker569"></a> the <span class="No-Break">following code:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_141" title2="(no caption)" no2="">KnowledgeGraphIndex.as_retriever(
&nbsp;&nbsp;&nbsp;&nbsp;retriever_mode='embedding'
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In this mode, the retriever turns the query into an embedding and the system finds nodes in the graph whose vector representation is similar to the embedding of the query, even if the same keywords are <span class="No-Break">not used.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Hybrid mode" no2="6.3.4.3">6.3.4.3. Hybrid mode</h4>
			<p>This mode can be configured using the <span class="No-Break">following command:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_142" title2="(no caption)" no2="">KnowledgeGraphIndex.as_retriever(retriever_mode='hybrid')</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In hybrid mode, the retriever uses<a id="_idIndexMarker570"></a> both the keywords extracted from the query and the embeddings to find a set of relevant Nodes. It combines the results from both the keyword-based and embedding-based retrieval steps and removes any duplicated results. This approach combines the precision of keyword-based search with the semantic understanding of <span class="No-Break">the embeddings.</span></p>
			<p>There are several customizable parameters for this type of retriever. For example, <strong class="source-inline">query_keyword_extract_template</strong>, <strong class="source-inline">refine_template</strong>, and <strong class="source-inline">text_qa_template</strong> can be used to change the default prompt for keyword extraction, the default prompt for query refinement, and the default prompt for text queries and answers, respectively. Here are some other <span class="No-Break">useful parameters:</span></p>
			<ul>
				<li><strong class="source-inline">max_keywords_per_query</strong>: This limits the number of keywords to avoid overloading the search process. The default value <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">10</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">num_chunks_per_query</strong>: This determines how many text fragments can be parsed in a single query. The default is <strong class="source-inline">10</strong> and any change must take into account the performance impact and limitations of the <span class="No-Break">LLM used.</span></li>
				<li><strong class="source-inline">include_text</strong>: The default value is <strong class="source-inline">True</strong>. This argument indicates whether the text of the source document <a id="_idIndexMarker571"></a>should be used in queries in each relevant triplet. This can enrich the query with additional context but inevitably increases the <span class="No-Break">computational cost.</span></li>
				<li><strong class="source-inline">similarity_top_k</strong>: When the retriever is configured in embedding or hybrid mode, this parameter specifies the number of similar embeddings to be considered in the retrieval process. The default value <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">2</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">graph_store_query_depth</strong>: This parameter controls how deep into the graph structure to search for relevant information. The default value <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">2</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">use_global_node_triplets</strong>: When set to <strong class="source-inline">True</strong>, the retriever will not limit itself to keywords extracted directly from the user query; instead, it will search for other keywords or entities in the text fragments that have already been identified as relevant to the initial keywords. This process helps bring an additional layer of knowledge to the query. By exploring the relationships and connections between different nodes in the graph, the retriever can access richer and more contextual information than would be possible by limiting itself to the original keywords. However, this approach is more costly in terms of computing resources and search time as it involves analyzing a greater number of nodes and relationships in the graph. For this reason, the option is disabled by default – that is, it’s set <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">False</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">max_knowledge_sequence</strong>: This parameter provides a balance between the quality and quantity of information presented. For example, if a query can theoretically generate 100 sequences of relevant knowledge, but <strong class="source-inline">max_knowledge_sequence</strong> is set to 30, only the most relevant 30 sequences will be presented as answers. This is also the default. Setting a limit ensures that the answer does not become too long or difficult to interpret, while still retaining <a id="_idIndexMarker572"></a>enough information to <span class="No-Break">be useful</span></li>
			</ul>
			<p>Although they return <strong class="source-inline">NodeWithScore</strong> objects, the knowledge graph retrievers do not provide any score for the actual nodes. Instead, they simply return a default value of <strong class="source-inline">1000</strong> for each <span class="No-Break">retrieved node.</span></p>
			<p>If the retrievers do not find any nodes in the index based on the configured mode and search parameters, they will first try to identify nodes based on the provided keywords only. If they do not find any relevant nodes, they will return a single placeholder node with the text <em class="italic">No relationships found </em>and a score <span class="No-Break">of 1.</span></p>
			<h4 data-type="sect3" class="sect3" title2="KnowledgeGraphRAGRetriever" no2="6.3.4.4">6.3.4.4. KnowledgeGraphRAGRetriever</h4>
			<p>This additional retriever is a bit more<a id="_idIndexMarker573"></a> special in that it operates by identifying key entities within a query and leveraging these to navigate the knowledge graph. It utilizes functions and templates for entity extraction (<strong class="source-inline">extraction entity_extract_fn</strong> and <strong class="source-inline">entity_extract_template</strong>) and synonym expansion (<strong class="source-inline">synonym_expand_fn</strong> and <strong class="source-inline">synonym_expand_template</strong>) to enrich the query with a broader context of related terms and concepts. The retriever traverses the graph to a specified depth – <strong class="source-inline">graph_traversal_depth</strong> – based on these entities and their synonyms, constructing a knowledge sequence relevant to <span class="No-Break">the query.</span></p>
			<p>This retriever can operate in various modes and can be configured by setting <strong class="source-inline">retriever_mode</strong>, allowing for flexibility in its approach to finding <span class="No-Break">relevant nodes.</span></p>
			<p>Just like <strong class="source-inline">KGTableRetriever</strong>, this retriever has three operating modes: <strong class="source-inline">keyword</strong>, <strong class="source-inline">embedding</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">keyword_embedding</strong></span><span class="No-Break">.</span></p>
			<p class="callout-heading">A note regarding retrieval modes</p>
			<p class="callout">As of January 2024, in LlamaIndex v0.9.25, only the keyword <a id="_idIndexMarker574"></a>retrieval mode <span class="No-Break">was implemented.</span></p>
			<p>In addition, the retriever features the <strong class="source-inline">with_nl2graphquery</strong> option, which, when enabled, combines <strong class="bold">Natural Language to Graph Query</strong> (<strong class="bold">NL2GraphQuery</strong>)  capabilities, enhancing its ability to interpret and <a id="_idIndexMarker575"></a>respond to complex queries. NL2GraphQuery is a process that converts natural language queries into graph-based query languages. This is achieved via a combination of entity extraction, synonym expansion, and graph<a id="_idIndexMarker576"></a> traversal techniques. This parameter is set to <strong class="source-inline">False</strong> <span class="No-Break">by default.</span></p>
			<p>Here are some other parameters that we may wish <span class="No-Break">to customize:</span></p>
			<ul>
				<li><strong class="source-inline">max_knowledge_sequence</strong>: Sets a limit on the number of knowledge sequences included in the response, balancing detail <span class="No-Break">with clarity</span></li>
				<li><strong class="source-inline">max_entities</strong>: Specifies the maximum number of entities to extract from the query, defaulting <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">5</strong></span></li>
				<li><strong class="source-inline">max_synonyms</strong>: Determines the maximum number of synonyms to expand for each entity, with a default value <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">5</strong></span></li>
				<li><strong class="source-inline">synonym_expand_policy</strong>: Controls the policy for synonym expansion, either <em class="italic">union</em> or <em class="italic">intersection</em>, with <em class="italic">union</em> as <span class="No-Break">the default</span></li>
				<li><strong class="source-inline">entity_extract_policy</strong>: Sets the policy for entity extraction, also either <em class="italic">union</em> or <em class="italic">intersection</em>, defaulting <span class="No-Break">to </span><span class="No-Break"><em class="italic">union</em></span></li>
				<li><strong class="source-inline">verbose</strong>: As usual, this is used to enable or disable the printing of debug information, aiding in the understanding of the <span class="No-Break">Retriever’s operation</span></li>
				<li><strong class="source-inline">graph_traversal_depth</strong>: Determines the depth of the traversal within the knowledge graph. By default, this is set <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">2</strong></span></li>
			</ul>
			<p class="callout-heading">A quick note</p>
			<p class="callout">There’s something important to highlight for all retrievers that use LLMs and accept parameters for customization prompts: All of these parameters are of the <strong class="source-inline">BasePromptTemplate</strong> type. We will talk more about the structure of this class and how to use it in <a href="#_idTextAnchor214"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Prompt Engineering Guidelines and </em><span class="No-Break"><em class="italic">Best Practices</em></span><span class="No-Break">.</span></p>
			<p>With that, we’ve covered the differences between each type of retriever. Now, let’s see what they all have <span class="No-Break">in </span><span class="No-Break"><a id="_idIndexMarker577"></a></span><span class="No-Break">common.</span></p>
			<h3 id="f_10__idParaDest-141" data-type="sect2" class="sect2" title2="Common characteristics shared by all retrievers" no2="6.3.5"><a id="_idTextAnchor140"></a>6.3.5. Common characteristics shared by all retrievers</h3>
			<p>All retrievers accept either<a id="_idIndexMarker578"></a> a query directly or a <strong class="source-inline">QueryBundle</strong> object as a parameter. <strong class="source-inline">QueryBundle</strong> is a universal mechanism that can be used for more advanced use cases, such as searching based on embeddings or searching for images and/or text in a <span class="No-Break">multimodal scenario.</span></p>
			<p>In addition, all retrievers accept the <strong class="source-inline">callback_manager</strong> argument. We will discuss this mechanism in more detail in <a href="#_idTextAnchor214"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Prompt Engineering Guidelines and </em><span class="No-Break"><em class="italic">Best Practices</em></span><span class="No-Break">.</span></p>
			<p>These are the basic building blocks for the retrieval logic of our RAG applications. If we want a generic and easy-to-build solution, we can use them directly. However, for more complex cases, there are several advanced retrieval modules in LlamaIndex that either combine the functionality of the basic retrievers or add new features to the mix. We will discuss some of them later in <span class="No-Break">this chapter.</span></p>
			<p>As we have seen, some retrievers use either embedding models or LLM queries to identify the most relevant nodes. However, at their core, all of the retriever types listed here are subclasses of <strong class="source-inline">BaseRetriever</strong>. This means that they all inherit the main <strong class="source-inline">retrieve()</strong> method, as well as <strong class="source-inline">aretrieve()</strong>, for <span class="No-Break">asynchronous operation.</span></p>
			<p>We will discuss the asynchronous <span class="No-Break">operation next.</span></p>
			<h3 id="f_10__idParaDest-142" data-type="sect2" class="sect2" title2="Efficient use of retrieval mechanisms – asynchronous operation" no2="6.3.6"><a id="_idTextAnchor141"></a>6.3.6. Efficient use of retrieval mechanisms – asynchronous operation</h3>
			<p>For the sake of simplicity, all the code <a id="_idIndexMarker579"></a>examples we have discussed so <a id="_idIndexMarker580"></a>far have used <strong class="bold">synchronous methods</strong>. Although the synchronous – or <strong class="bold">serialized</strong> – mode of operation is linear, easy to understand, and predictable, in modern applications, performance and low latency are very important to provide a great <span class="No-Break">user experience.</span></p>
			<p>The good news is that LlamaIndex already <a id="_idIndexMarker581"></a>offers – in most cases – <strong class="bold">asynchronous execution</strong> alternatives. Here’s a simple example of asynchronous execution for two Retrievers defined <span class="No-Break">over </span><span class="No-Break"><strong class="source-inline">KeywordTableIndex</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_143" title2="(no caption)" no2="">import asyncio
from llama_index.core import KeywordTableIndex
from llama_index.core import SimpleDirectoryReader
async def retrieve(retriever, query, label):
&nbsp;&nbsp;&nbsp;&nbsp;response = await retriever.aretrieve(query)
&nbsp;&nbsp;&nbsp;&nbsp;print(f"{label} retrieved {str(len(response))} nodes")
async def main():
&nbsp;&nbsp;&nbsp;&nbsp;reader = SimpleDirectoryReader('files')
&nbsp;&nbsp;&nbsp;&nbsp;documents = reader.load_data()
&nbsp;&nbsp;&nbsp;&nbsp;index = KeywordTableIndex.from_documents(documents)
&nbsp;&nbsp;&nbsp;&nbsp;retriever1 = index.as_retriever(
retriever_mode='default'
)
&nbsp;&nbsp;&nbsp;&nbsp;retriever2 = index.as_retriever(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;retriever_mode='simple'
)
&nbsp;&nbsp;&nbsp;&nbsp;query = "Where is the Colosseum?"
&nbsp;&nbsp;&nbsp;&nbsp;await asyncio.gather(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;retrieve(retriever1, query, '&lt;llm&gt;'),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;retrieve(retriever2, query, '&lt;simple&gt;')
&nbsp;&nbsp;&nbsp;&nbsp;)
asyncio.run(main())</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The preceding code executes<a id="_idIndexMarker582"></a> the two retrievals in parallel. Of<a id="_idIndexMarker583"></a> course, being a trivial example with a very small dataset, the performance benefits of <strong class="bold">asynchronous operation</strong> will not be significant in <span class="No-Break">this case.</span></p>
			<p>However, in the context of a commercial application that frequently calls retrievers and operates numerous complex queries over many indexed nodes, the benefits will be substantial. Asynchronous operation improves performance, uses resources more efficiently, reduces latency, and generally provides a more natural user experience by reducing <span class="No-Break">waiting times.</span></p>
			<p>Now, it’s time to talk<a id="_idIndexMarker584"></a> about the more advanced <span class="No-Break">retrieval methods.</span></p>
			<h2 id="f_10__idParaDest-143" data-type="sect1" class="sect1" title2="Building more advanced retrieval mechanisms" no2="6.4"><a id="_idTextAnchor142"></a>6.4. Building more advanced retrieval mechanisms</h2>
			<p>Now we understand the basic components offered by LlamaIndex, we can build increasingly sophisticated solutions. On one hand, the retrievers we have discussed already provide efficient solutions for knowledge base querying and context enhancement in an RAG flow. On the other hand, we’ll see that there are many more advanced retrieval methods that either use specific techniques or ingeniously combine the retrievers <span class="No-Break">already discussed.</span></p>
			<h3 id="f_10__idParaDest-144" data-type="sect2" class="sect2" title2="The naive retrieval method" no2="6.4.1"><a id="_idTextAnchor143"></a>6.4.1. The naive retrieval method</h3>
			<p>LlamaIndex provides fast query <a id="_idIndexMarker585"></a>methods by default. As we have seen, in just a few lines of code, we can ingest documents, create nodes and, for example, build a <strong class="source-inline">VectorStoreIndex</strong> retriever, which we can then just as easily query to return the most relevant parts using a retriever that uses similarity <span class="No-Break">measurement techniques.</span></p>
			<p>The method is very simple and easy to implement. However, it is not an ideal method in all situations. More often than not, the <strong class="bold">naive method</strong>, as it is <a id="_idIndexMarker586"></a>usually called, produces<a id="_idIndexMarker587"></a> mediocre rather than <strong class="bold">state-of-the-art</strong> (<span class="No-Break"><strong class="bold">SOTA</strong></span><span class="No-Break">) solutions.</span></p>
			<p class="callout-heading">To use an analogy…</p>
			<p class="callout">It’s pretty much like using a hammer for all kinds of repairs in a house. The hammer is an essential and easy-to-use tool, but it is not always the best solution for every problem. Similarly, using a simplified method of questioning may be effective for basic situations but will not be as effective for more complex situations or specific needs that require a greater degree of finesse <span class="No-Break">and adaptation.</span></p>
			<p>In these more complex cases, it is necessary to explore more advanced and tailored solutions, which may involve adapting the retrieval algorithms or combining them in <span class="No-Break">different ways.</span></p>
			<p>Also, for large datasets, naive methods can be inefficient, either returning too many irrelevant results or missing important information. They can also underperform in terms of response time and <span class="No-Break">resource consumption.</span></p>
			<p>In addition, in a real-world<a id="_idIndexMarker588"></a> situation, data can vary significantly in terms of quality, structure, and format. Simple methods are not always able to manage this diversity and extract <span class="No-Break">valuable information.</span></p>
			<p>For example, if the specific information we are looking for is scattered in small chunks that are randomly distributed throughout the document, the results will be below expectations. In the next few sections, we’ll discuss some more advanced retrieval methods that can provide much better results in various <span class="No-Break">specific situations.</span></p>
			<h3 id="f_10__idParaDest-145" data-type="sect2" class="sect2" title2="Implementing metadata filters" no2="6.4.2"><a id="_idTextAnchor144"></a>6.4.2. Implementing metadata filters</h3>
			<p>A very simple but also effective<a id="_idIndexMarker589"></a> retrieval mechanism is filtering<a id="_idIndexMarker590"></a> the retrieved nodes by <strong class="bold">metadata</strong>. We’ll tackle a practical problem that’s usually encountered in an organization and for which the retrieval functions in LlamaIndex can provide <span class="No-Break">a solution.</span></p>
			<p>We will see how to implement a retrieval system that filters the returned nodes according to the user’s department. Similar to the concept of polymorphism in object-oriented programming, it often happens that the same concept has different definitions, depending on the area <span class="No-Break">of use.</span></p>
			<p>In our example, the user is looking for the definition of an incident in an organizational knowledge base. However, the term <em class="italic">incident</em> may have a different definition for those who deal with information security than for those who deal with IT service operations. Let’s have a look at how we can implement a form of polymorphism in a <span class="No-Break">retrieval mechanism.</span></p>
			<ol>
				<li>First, we must take care of the necessary imports and define a mapping of users <span class="No-Break">to departments:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_144" title2="(no caption)" no2="">from llama_index.core.vector_stores.types import MetadataFilter, MetadataFilters
from llama_index.core import VectorStoreIndex
from llama_index.core.schema import TextNode
user_departments = {"Alice": "Security", "Bob": "IT"}</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>				<li>Then, we must define two nodes that both store the definition of the concept of incident. The difference is in <a id="_idIndexMarker591"></a>the metadata, which specifies the department where the <span class="No-Break">definition applies:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_145" title2="(no caption)" no2="">nodes = [
&nbsp;&nbsp;&nbsp;&nbsp;TextNode(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text=(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"An incident is an accidental or malicious event that has the potential to cause unwanted effects on the security of our IT assets."),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata={"department": "Security"},
&nbsp;&nbsp;&nbsp;&nbsp;),
&nbsp;&nbsp;&nbsp;&nbsp;TextNode(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text=("An incident is an unexpected interruption or
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;degradation of an IT service."),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata={"department": "IT"},
&nbsp;&nbsp;&nbsp;&nbsp;)
]
Next, we must define the function that's responsible for filtering and retrieval:
def show_report(index, user, query):
&nbsp;&nbsp;&nbsp;&nbsp;user_department = user_departments[user]
&nbsp;&nbsp;&nbsp;&nbsp;filters = MetadataFilters(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;filters=[
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MetadataFilter(key="department", 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value=user_department)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;]
&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;retriever = index.as_retriever(filters=filters)
&nbsp;&nbsp;&nbsp;&nbsp;response = retriever.retrieve(query)
&nbsp;&nbsp;&nbsp;&nbsp;print(f"Response for {user}: {response[0].node.text}")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>				<li>Now, if we run the same<a id="_idIndexMarker592"></a> query in the context of each user, we will get different answers, depending on the department each user <span class="No-Break">belongs to:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_146" title2="(no caption)" no2="">index = VectorStoreIndex(nodes)
query = "What is an incident?"
show_report(index, "Alice", query)
show_report(index, "Bob", query)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div><p class="list-inset">The output will look <span class="No-Break">like this:</span></p><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_147" title2="(no caption)" no2="">Response for Alice: An incident is an accidental or malicious event that has the potential to cause unwanted effects on the security of our IT assets.
Response for Bob: An incident is an unexpected interruption or degradation of an IT service.</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>			</ol>
			<p>See how simple that was? The same mechanism can be used, for example, to control access to information and define <span class="No-Break">security rules.</span></p>
			<p>For example, in a knowledge base system shared by several clients on a multi-tenancy model, we can restrict access by <span class="No-Break">implementing </span><span class="No-Break"><strong class="source-inline">MetadataFilters</strong></span><span class="No-Break">.</span></p>
			<p>The code you saw earlier only does simple filtering: it restricts the search to nodes for which the value of the <strong class="source-inline">department</strong> key is equal to the user’s department. But there are also more complex filtering variants that use operators based on the <strong class="source-inline">FilterOperator</strong> class. Unfortunately, the default vector store in LlamaIndex only supports the <strong class="source-inline">EQ</strong> (equal) operator – that is, it can only apply filters where the value of a key is equal to a certain parameter. If we use a more sophisticated version of vector store (such as Pinecone or <a id="_idIndexMarker593"></a>ChromaDB), we can use the full range of operators available in <strong class="source-inline">FilterOperator</strong>, as listed in the <span class="No-Break">following table:</span></p>
			<table id="table001-1" class="No-Table-Style _idGenTablePara-1" data-type="table" title2="– A complete list of operators available for FilterOperator" no2="6.1"><colgroup><col><col><col></colgroup><thead><tr class="No-Table-Style"><th class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Symbolic Operator</strong></span></p>
						</th><th class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Programming Equivalent</strong></span></p>
						</th><th class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Description</strong></span></p>
						</th></tr></thead><tbody><tr class="No-Table-Style"><th class="No-Table-Style">
							<p><span class="No-Break">EQ</span></p>
						</th><th class="No-Table-Style">
							<p>==</p>
						</th><th class="No-Table-Style">
							<p><span class="No-Break">Equal (default)</span></p>
						</th></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break">GT</span></p>
						</td><td class="No-Table-Style">
							<p>&gt;</p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">Greater than</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break">LT</span></p>
						</td><td class="No-Table-Style">
							<p>&lt;</p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">Less than</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break">NE</span></p>
						</td><td class="No-Table-Style">
							<p>!=</p>
						</td><td class="No-Table-Style">
							<p>Not <span class="No-Break">equal to</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break">GTE</span></p>
						</td><td class="No-Table-Style">
							<p>&gt;=</p>
						</td><td class="No-Table-Style">
							<p>Greater than or <span class="No-Break">equal to</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break">LTE</span></p>
						</td><td class="No-Table-Style">
							<p>&lt;=</p>
						</td><td class="No-Table-Style">
							<p>Less than or <span class="No-Break">equal to</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break">IN</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">in</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">In array</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break">NIN</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">nin</span></p>
						</td><td class="No-Table-Style">
							<p>Not <span class="No-Break">in array</span></p>
						</td></tr></tbody></table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.1 – A complete list of operators available for FilterOperator</p>
			<p>Here is an example where we use filter operators and filter aggregation conditions to implement more <span class="No-Break">complex scenarios:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_148" title2="(no caption)" no2="">from llama_index.core.vector_stores.types import (
&nbsp;&nbsp;&nbsp;&nbsp;FilterOperator, FilterCondition)
filters = MetadataFilters(
&nbsp;&nbsp;&nbsp;&nbsp;filters=[
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MetadataFilter(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;key="department",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value="Procurement"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MetadataFilter(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;key="security_classification",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value=&lt;user_clearance_level&gt;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;operator=FilterOperator.LTE
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),
&nbsp;&nbsp;&nbsp;&nbsp;],
&nbsp;&nbsp;&nbsp;&nbsp;condition=FilterCondition.AND
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In the previous example, we implemented a very simple access control mechanism based on clearance level and security classification. Only nodes that belong to a particular department and<a id="_idIndexMarker594"></a> have a classification level less than or equal to the user’s access level will be returned. We’ll talk about another <span class="No-Break">method next.</span></p>
			<h3 id="f_10__idParaDest-146" data-type="sect2" class="sect2" title2="Using selectors for more advanced decision logic" no2="6.4.3"><a id="_idTextAnchor145"></a>6.4.3. Using selectors for more advanced decision logic</h3>
			<p>In an advanced user interaction <a id="_idIndexMarker595"></a>system, the user may employ a wide variety of queries. For example, they may ask a very specific question, looking for a precise definition. At other times, the user may be looking for more general information or may be asking the system to summarize or compare <span class="No-Break">two documents.</span></p>
			<p>In these complex situations, which retriever should be used? It becomes clear that the best implementation is based on the combined strength of many retrieval systems. But this implicitly means that the RAG application must have an internal selection mechanism to choose the most appropriate retriever according to the query. This brings us to the topic of this section: the <a id="_idIndexMarker596"></a>use <span class="No-Break">of </span><span class="No-Break"><strong class="bold">selectors</strong></span><span class="No-Break">.</span></p>
			<p>In LlamaIndex, they come in five different flavors: <strong class="source-inline">LLMSingleSelector</strong>, <strong class="source-inline">LLMMultiSelector</strong>, <strong class="source-inline">EmbeddingSingleSelector</strong>, <strong class="source-inline">PydanticSingleSelector</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">PydanticMultiSelector</strong></span><span class="No-Break">.</span></p>
			<p>The way they work is slightly different. As the name suggests, some rely on the decision capabilities of an LLM, others select a particular option from a list of options based on a similarity calculation, and others use Pydantic objects to return a selection. Some return a single option from a list; others may return multiple selections from a list of options. In the end, however, their result is more or less the same: they help us implement advanced conditional logic in the applications <span class="No-Break">we develop.</span></p>
			<p>That is because they<a id="_idIndexMarker597"></a> can evaluate complex conditions and decide which logic branch the application should follow – just like an <em class="italic">IF...THEN</em> decision block, but able to handle more <span class="No-Break">complex scenarios.</span></p>
			<p>The following diagram can help us better understand the role a selector plays in the logic of an RAG application. <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.12</em>, provides a visual representation of how <span class="No-Break"><strong class="source-inline">LLMSingleSelector</strong></span><span class="No-Break"> works:</span></p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/B21861_06_12.jpg" alt="Figure 6.12 – Visualizing LLMSingleSelector" width="1650" height="722" data-type="figure" id="untitled_figure_46" title2="– Visualizing LLMSingleSelector" no2="6.12">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.12 – Visualizing LLMSingleSelector</p>
			<p>Here is a very simple implementation of a selector that uses an LLM to return a single option from a list of <span class="No-Break">predefined options:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_149" title2="(no caption)" no2="">from llama_index.core.selectors.llm_selectors import LLMSingleSelector
options = [
&nbsp;&nbsp;&nbsp;&nbsp;"option 1: this is good for summarization questions",
&nbsp;&nbsp;&nbsp;&nbsp;"option 2: this is useful for precise definitions",
&nbsp;&nbsp;&nbsp;&nbsp;"option 3: this is useful for comparing concepts",
]
selector = LLMSingleSelector.from_defaults()</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In the first part of the code, we<a id="_idIndexMarker598"></a> defined the options as a list of strings to be sent to the LLM via the <strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">select()</strong></span><span class="No-Break"> method:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_150" title2="(no caption)" no2="">decision = selector.select(
&nbsp;&nbsp;&nbsp;&nbsp;options,
&nbsp;&nbsp;&nbsp;&nbsp;query="What's the definition of space?"
).selections[0]
print(decision.index+1)
print(decision.reason)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The <strong class="source-inline">.select()</strong> method takes the defined options and the user query as arguments. Under the hood, the selector uses a specially constructed prompt to ask the LLM to choose the best option from the list based on <span class="No-Break">the query.</span></p>
			<p>As a response, the selector returns a <strong class="source-inline">SingleSelection</strong> object containing the number of the selected option and a justification for the selection made. As you can see, the selector is not something specific to retrievers. We haven’t even defined a retriever in <span class="No-Break">this example.</span></p>
			<p>This is because I wanted to show that the mechanism is generic and can be used for absolutely any conditional logic we want to implement in the application. The returned option number could help us to choose from a list of parsers, indexes, retrievers, and so on. In this simple version, the selector simply chooses from a list of strings defining the available options. However, there is a more advanced form of selection that involves the use of the <strong class="source-inline">ToolMetadata</strong> class. But to understand this concept, we first need to clarify what a <span class="No-Break"><strong class="bold">tool</strong></span><span class="No-Break"> is.</span></p>
			<h3 id="f_10__idParaDest-147" data-type="sect2" class="sect2" title2="Understanding tools" no2="6.4.4"><a id="_idTextAnchor146"></a>6.4.4. Understanding tools</h3>
			<p>An essential element<a id="_idIndexMarker599"></a> in any <strong class="bold">agentic functionality</strong>, where the application decides which method to use depending on the context, is a generic container. It may contain different functionalities that can be called by<a id="_idIndexMarker600"></a> the application <span class="No-Break">at runtime.</span></p>
			<p>There is a rich collection of tools already developed and available in LlamaHub: <a href="https://llamahub.ai/?tab=tools" target="_blank" rel="noopener noreferrer">https://llamahub.ai/?tab=tools</a>. They can perform various specific functions, from composing and sending emails to querying various APIs or interacting with the computer’s filesystem. We will talk much more about the use of tools in implementing <strong class="bold">agents</strong> in <a href="#_idTextAnchor179"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>,  <em class="italic">Building Chatbots and Agents with LlamaIndex</em>, where we will build our <span class="No-Break">PITS chatbot.</span></p>
			<p>For now, I want to show you how we can encapsulate a retriever in a tool container, and then use selectors to implement an adaptive retrieval mechanism. We will focus on the <strong class="source-inline">RetrieverTool</strong> class, which takes two important arguments: a retriever and a textual description of the retriever. Based on<a id="_idIndexMarker601"></a> the description, the selector decides, for example, whether to use one retriever or another for a particular query. We define a <strong class="source-inline">RouterRetriever</strong> object on top of each retriever we build. This <strong class="source-inline">RouterRetriever</strong> is a complex decision mechanism that uses the selector to decide which retriever to use depending on the situation. The most important arguments to give it are the selector and the options to choose from – in the form of <strong class="source-inline">RetrieverTool</strong> objects. Let’s see how we can implement this <span class="No-Break">in code:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_151" title2="(no caption)" no2="">from llama_index.core.selectors import PydanticMultiSelector
from llama_index.core.retrievers import RouterRetriever
from llama_index.core.tools import RetrieverTool
from llama_index.core import (
&nbsp;&nbsp;&nbsp;&nbsp;VectorStoreIndex, SummaryIndex, SimpleDirectoryReader)
documents = SimpleDirectoryReader("files").load_data()
vector_index = VectorStoreIndex.from_documents([documents[0]])
summary_index = SummaryIndex.from_documents([documents[1]])
vector_retriever = vector_index.as_retriever()
summary_retriever = summary_index.as_retriever()</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>First, we took the two sample files from the <strong class="source-inline">files</strong> subfolder. The first file contains information about ancient Rome and the second is a generic text about dogs. Then, we created an index for<a id="_idIndexMarker602"></a> each file and from each index, we created a retriever. Now, we must define <span class="No-Break">the tools:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_152" title2="(no caption)" no2="">vector_tool = RetrieverTool.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;retriever=vector_retriever,
&nbsp;&nbsp;&nbsp;&nbsp;description="Use this for answering questions about Ancient Rome"
)
summary_tool = RetrieverTool.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;retriever=summary_retriever,
&nbsp;&nbsp;&nbsp;&nbsp;description="Use this for answering questions about dogs"
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>As you can see, we have wrapped each retriever into <strong class="source-inline">RetrieverTool</strong> and added a clear description for the selector to use. Next, we must <span class="No-Break">build </span><span class="No-Break"><strong class="source-inline">RouterRetriever</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_153" title2="(no caption)" no2="">retriever = RouterRetriever(
&nbsp;&nbsp;&nbsp;&nbsp;selector=PydanticMultiSelector.from_defaults(),
&nbsp;&nbsp;&nbsp;&nbsp;retriever_tools=[
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;vector_tool,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;summary_tool
&nbsp;&nbsp;&nbsp;&nbsp;]
)
response = retriever.retrieve(
&nbsp;&nbsp;&nbsp;&nbsp;"What can you tell me about the Ancient Rome?"
)
for r in response:
&nbsp;&nbsp;&nbsp;&nbsp;print(r.text)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>That’s all we need to do. From this point on, every time we query this dynamic retriever, the selector will determine which individual retriever to use to return the context. Here’s <span class="No-Break">an example:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_154" title2="(no caption)" no2="">retriever.retrieve("What can you tell me about the Ancient Rome?")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This will use <strong class="source-inline">vector_tool</strong> for retrieval. Now, take a look at the <span class="No-Break">following code:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_155" title2="(no caption)" no2="">retriever.retrieve("Tell me all you know about dogs")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This will call <strong class="source-inline">summary_tool</strong>. Because we used <strong class="source-inline">PydanticMultiSelector</strong>, we can also handle situations where both retrievers should be used, <span class="No-Break">like so:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_156" title2="(no caption)" no2="">retriever.retrieve("Tell me about dogs in Ancient Rome")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Unlike <strong class="source-inline">PydanticSingleSelector</strong>, <strong class="source-inline">PydanticMultiSelector</strong> can simultaneously select multiple options from the selector list, covering multiple use cases. Similarly, we can also define <a id="_idIndexMarker603"></a>more complex routers at the query engine level by using <strong class="source-inline">RouterQueryEngine</strong>. We will discuss this in more detail in <a href="#_idTextAnchor155"><span class="No-Break"><em class="italic">Chapter 7</em></span></a><span class="No-Break">.</span></p>
			<p>First, we need to cover a few other advanced forms <span class="No-Break">of retrievers.</span></p>
			<h3 id="f_10__idParaDest-148" data-type="sect2" class="sect2" title2="Transforming and rewriting queries" no2="6.4.5"><a id="_idTextAnchor147"></a>6.4.5. Transforming and rewriting queries</h3>
			<p>In the previous section, we saw how <a id="_idIndexMarker604"></a>we can use selectors and the router concept to let the application decide which retriever <span class="No-Break">to use.</span></p>
			<p>Another very powerful tool that our RAG application can use is the <strong class="source-inline">QueryTransform</strong> construct. This allows us to rewrite and modify a query before using it to interrogate the index, as shown in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.13</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/B21861_06_13.jpg" alt="Figure 6.13 – QueryTransform improving the retrieval process" width="1211" height="535" data-type="figure" id="untitled_figure_47" title2="– QueryTransform improving the retrieval process" no2="6.13">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.13 – QueryTransform improving the retrieval process</p>
			<p>Let’s imagine a scenario where we<a id="_idIndexMarker605"></a> might need the functionality provided <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">QueryTransform</strong></span><span class="No-Break">.</span></p>
			<p class="callout-heading">Practical example</p>
			<p class="callout"><strong class="bold">A chatbot designed to provide technical support for complex software</strong>: Users often describe their problems in vague or non-technical terms. <strong class="source-inline">QueryTransform</strong> can interpret these descriptions, break them down into more specific sub-queries, or enrich them with technical terms that better match the documentation. For example, a query of the form <em class="italic">My computer keeps freezing</em> could be transformed into a more specific query, such as <em class="italic">Troubleshooting steps for operating </em><span class="No-Break"><em class="italic">system freezes</em></span><span class="No-Break">.</span></p>
			<p>There are several variations of <strong class="source-inline">QueryTransform</strong> that we can use. Each has its specific role in augmenting the information retrieval process. Let’s look at <span class="No-Break">each one:</span></p>
			<ul>
				<li><strong class="source-inline">IdentityQueryTransform</strong>: This is a basic transform that does not modify the query. It returns the query as it was received, without any transformation. It’s useful for maintaining default or basic behavior where no specific transformations <span class="No-Break">are required</span></li>
				<li><strong class="source-inline">HyDEQueryTransform</strong>: <strong class="bold">Hypothetical Document Embeddings</strong> (<strong class="bold">HyDE</strong>) transforms the query into a<a id="_idIndexMarker606"></a> hypothetical document generated by an LLM. The idea is to generate hypothetical query answers and use them as embedding strings. This can help improve the relevance of the results. This method filters out inaccurate details while<a id="_idIndexMarker607"></a> grounding the generated response in the actual content. You can read more about the benefits of using this technique here: <em class="italic">Gao, Luyu; Ma, Xueguang; Lin, Jimmy; Callan, Jamie (2022). “Precise Zero-Shot Dense Retrieval without Relevance Labels”. arXiv:2212.10496v1 [</em><span class="No-Break"><em class="italic">cs.IR].</em></span><span class="No-Break"> </span><a href="https://arxiv.org/abs/2212.10496" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://arxiv.org/abs/2212.10496</span></a></li>
				<li><strong class="source-inline">DecomposeQueryTransform</strong>: This type of transform decomposes a complex query into a simpler and more focused subquery. This can be useful to make queries easier for the index to process and increase the chances of finding relevant nodes, especially if the index structure is not optimized for complex or <span class="No-Break">ambiguous queries</span></li>
				<li><strong class="source-inline">ImageOutputQueryTransform</strong>: This method adds instructions for formatting results as images, such as generating HTML <em class="italic">&lt;img&gt;</em> tags. It is useful for cases where query results are expected to be displayed as images or when the output is just an intermediate step in more complex logic and has to be further processed in a <span class="No-Break">particular format</span></li>
				<li><strong class="source-inline">StepDecomposeQueryTransform</strong>: This is similar to <strong class="source-inline">DecomposeQueryTransform</strong> but it adds an extra layer by taking previous reasoning or context into account when decomposing the query. This can help to continually refine the query based on feedback or previous results, thus improving <span class="No-Break">retrieval accuracy</span></li>
			</ul>
			<p>Each of these transformations improves a system’s ability to process and respond to queries in a more efficient way that is better tailored to the user’s specific needs or the nature of <span class="No-Break">the data.</span></p>
			<p>Let’s have a look at a practical example to better understand how <span class="No-Break">they work:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_157" title2="(no caption)" no2="">from llama_index.core.indices.query.query_transform.base import DecomposeQueryTransform
decompose = DecomposeQueryTransform()
query_bundle = decompose.run(
&nbsp;&nbsp;&nbsp;&nbsp;"Tell me about buildings in ancient Rome"
)
print(query_bundle.query_str)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Once we run the code, <strong class="source-inline">DecomposeQueryTransform</strong> takes in our original – and otherwise very ambiguous – query. It then uses a specially designed prompt to generate a more precise query using the LLM. In our example, the output should look something <span class="No-Break">like this:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_158" title2="(no caption)" no2="">What were some famous buildings in ancient Rome?</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>You can immediately see<a id="_idIndexMarker608"></a> that the new query is much clearer and greatly increases the chances of the retriever generating a correct context from <span class="No-Break">the index.</span></p>
			<h3 id="f_10__idParaDest-149" data-type="sect2" class="sect2" title2="Creating more specific sub-queries" no2="6.4.6"><a id="_idTextAnchor148"></a>6.4.6. Creating more specific sub-queries</h3>
			<p>Another useful approach to<a id="_idIndexMarker609"></a> augmenting a query is to generate sub-queries. Sometimes, an ambiguous or very complex question becomes much clearer when it is split into several specific questions. LlamaIndex comes to our rescue this time too. <strong class="source-inline">OpenAIQuestionGenerator</strong> is a mechanism that’s designed exactly for this operation. Here is the code we used as an example earlier when we talked about selectors and routers. This time, we will adapt it a bit to demonstrate how <span class="No-Break"><strong class="source-inline">OpenAIQuestionGenerator</strong></span><span class="No-Break"> works:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_159" title2="(no caption)" no2="">from llama_index.question_gen.openai import OpenAIQuestionGenerator
from llama_index.core.tools import RetrieverTool, ToolMetadata
from llama_index.core import (
&nbsp;&nbsp;&nbsp;&nbsp;VectorStoreIndex, SummaryIndex, 
&nbsp;&nbsp;&nbsp;&nbsp;SimpleDirectoryReader, QueryBundle)
documents = SimpleDirectoryReader("files").load_data()
vector_index = VectorStoreIndex.from_documents(
&nbsp;&nbsp;&nbsp;&nbsp;[documents[0]]
)
summary_index = SummaryIndex.from_documents([documents[1]])</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>So far, the code is identical to <a id="_idIndexMarker610"></a>the earlier example. We read the two files from the <strong class="source-inline">files</strong> subfolder and then create an index for <span class="No-Break">each document:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_160" title2="(no caption)" no2="">vector_tool_metadata = ToolMetadata(
&nbsp;&nbsp;&nbsp;&nbsp;name="Vector Tool",
&nbsp;&nbsp;&nbsp;&nbsp;description="Use this for answering questions about Ancient Rome"
)
summary_tool_metadata = ToolMetadata(
&nbsp;&nbsp;&nbsp;&nbsp;name="Summary Tool",
&nbsp;&nbsp;&nbsp;&nbsp;description="Use this for answering questions about dogs"
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>For each index, we define a name and a description in a <strong class="source-inline">ToolMetadata</strong> structure. This information will be used by <strong class="source-inline">OpenAIQuestionGenerator</strong> to <em class="italic">understand</em> what role each retriever has and what type of questions it might answer. Next, we will define the <span class="No-Break">two retrievers:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_161" title2="(no caption)" no2="">vector_tool = RetrieverTool(
&nbsp;&nbsp;&nbsp;&nbsp;retriever=vector_index.as_retriever(),
&nbsp;&nbsp;&nbsp;&nbsp;metadata=vector_tool_metadata
)
summary_tool = RetrieverTool(
&nbsp;&nbsp;&nbsp;&nbsp;retriever=summary_index.as_retriever(),
&nbsp;&nbsp;&nbsp;&nbsp;metadata=summary_tool_metadata
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Now follows the generation of sub-queries. First, we initialize an <strong class="source-inline">OpenAIQuestionGenerator</strong> object with the default settings. Then, we build a <strong class="source-inline">QueryBundle</strong> object that will <a id="_idIndexMarker611"></a>contain the original query received from the user. This <strong class="source-inline">QueryBundle</strong> will be sent as an argument to the <span class="No-Break">question generator:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_162" title2="(no caption)" no2="">question_generator = OpenAIQuestionGenerator.from_defaults()
query_bundle = QueryBundle(
&nbsp;&nbsp;&nbsp;&nbsp;query_str="Tell me about dogs and Ancient Rome")
sub_questions = question_generator.generate(
&nbsp;&nbsp;&nbsp;&nbsp;tools=[vector_tool.metadata, summary_tool.metadata],
&nbsp;&nbsp;&nbsp;&nbsp;query=query_bundle
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>As you can see, the subquery generator takes two arguments – a list of tools at its disposal, and the original query from which it can build more <span class="No-Break">specific queries:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_163" title2="(no caption)" no2="">for sub_question in sub_questions:
&nbsp;&nbsp;&nbsp;&nbsp;print(f"{sub_question.tool_name}: {sub_question.sub_question}")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In the end, the generated questions might look something <span class="No-Break">like this:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_164" title2="(no caption)" no2="">Summary Tool: What are the different breeds of dog?
Summary Tool: What was the role of dogs in ancient Rome?
Vector Tool: What were the most important events in Ancient Rome?
Vector Tool: What were the most famous buildings in ancient Rome?</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p><strong class="source-inline">OpenAIQuestionGenerator</strong> took the initial query and, using the LLM, returned a list of more <span class="No-Break">specific questions.</span></p>
			<p>The answer that’s returned in the <strong class="source-inline">sub_questions</strong> variable is a list of <strong class="source-inline">SubQuestion</strong> items - a simple class with two attributes: <strong class="source-inline">tool_name</strong> and <strong class="source-inline">sub_question</strong>. We can now iterate through all the items in the list and get the tools and questions we are <span class="No-Break">looking for.</span></p>
			<p>In practice, using more specific queries, as in the preceding example, is likely to generate more context with the retriever and therefore likely to get a better-quality answer <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">QueryEngine</strong></span><span class="No-Break">.</span></p>
			<p>As an alternative to <strong class="source-inline">OpenAIQuestionGenerator</strong>, it is good to know that there is also <strong class="source-inline">LLMQuestionGenerator</strong>, which, as its name suggests, allows us to use any LLM. Another difference<a id="_idIndexMarker612"></a> between the two is that <strong class="source-inline">LLMQuestionGenerator</strong> uses a special parser to structure the output, unlike <strong class="source-inline">OpenAIQuestionGenerator</strong>, which relies on the generation of <span class="No-Break">Pydantic objects.</span></p>
			<p>The same collection of question generators also includes <strong class="source-inline">GuidanceQuestionGenerator</strong>. This mechanism uses an LLM to create helper questions to guide the query engine. It can be extremely useful when you’re dealing with complex queries that need to be broken down and processed in a <span class="No-Break">particular order.</span></p>
			<p>Once these sub-queries have been generated, they can be used in a specially constructed query engine. We will discuss this step in more detail in <a href="#_idTextAnchor155"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Querying Our Data, Part 2 – Postprocessing and Response Synthesis</em>, when we talk <span class="No-Break">about </span><span class="No-Break"><strong class="source-inline">SubQuestionQueryEngine</strong></span><span class="No-Break">.</span></p>
			<p>Next, we’ll talk about two important concepts related to <span class="No-Break">information retrieval.</span></p>
			<h2 id="f_10__idParaDest-150" data-type="sect1" class="sect1" title2="Understanding the concepts of dense and sparse retrieval" no2="6.5"><a id="_idTextAnchor149"></a>6.5. Understanding the concepts of dense and sparse retrieval</h2>
			<p>As we have seen, retrieval methods are a critical component of RAG systems. They enable the identification and ranking of relevant content for queries, which is the first step in generating useful answers from an LLM. During your journey into RAG application development, you’re <a id="_idIndexMarker613"></a>likely to encounter two <a id="_idIndexMarker614"></a>dominant retrieval paradigms – <strong class="bold">dense retrieval</strong> and <strong class="bold">sparse retrieval</strong>. Because it is important to understand these concepts, this section will focus on their characteristics, trade-offs, and the benefits of <span class="No-Break">combining them.</span></p>
			<h3 id="f_10__idParaDest-151" data-type="sect2" class="sect2" title2="Dense retrieval" no2="6.5.1"><a id="_idTextAnchor150"></a>6.5.1. Dense retrieval</h3>
			<p>The dense retrieval method relies on embedding vectors to represent text in a continuous, high-dimensional <a id="_idIndexMarker615"></a>space. Using embedding models, texts are <strong class="bold">encoded</strong> into fixed-length numerical vectors that are intended to capture semantic meaning. Queries are also encoded so that the similarity between them and the node vectors can be measured using geometric operations. In dense retrieval, nodes are embedded in vectors and stored in a specialized index such <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">VectorStoreIndex</strong></span><span class="No-Break">.</span></p>
			<p>We call them <strong class="bold">dense</strong> because these vectors are typically densely populated with non-zero values, representing rich and nuanced semantic information in a compact form. During retrieval, incoming queries are dynamically embedded and used to retrieve the top-k nodes using similarity search algorithms, such as those discussed in <a href="#_idTextAnchor104"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><span class="No-Break">.</span></p>
			<p>This approach has several advantages, particularly in terms of semantic understanding, speed, and scalability. Nodes that convey similar meanings tend to cluster closer together. Also, the words themselves do not have to match perfectly. Synonyms and polysemous words don’t affect precision <span class="No-Break">as much.</span></p>
			<p>Specialized indexing solutions, such as those provided by a Pinecone vector database (<a href="https://www.pinecone.io/product/" target="_blank" rel="noopener noreferrer">https://www.pinecone.io/product/</a>), also allow lightning-fast similarity searches over millions of vectors. Latencies range from milliseconds to less than a second and scaling is <span class="No-Break">easily achieved.</span></p>
			<p>There are, however, several drawbacks associated with <a id="_idIndexMarker616"></a><span class="No-Break">dense search:</span></p>
			<ul>
				<li><strong class="bold">Computational cost</strong>: Embedding and indexing large volumes of data can be computationally expensive <span class="No-Break">and time-consuming.</span></li>
				<li><strong class="bold">A trade-off between precision and recall</strong>: Dense retrieval systems can sometimes favor recall over precision or vice versa, depending on how the embedding model is tuned. Finding the right balance between retrieving all relevant documents and not retrieving too many irrelevant documents can <span class="No-Break">be difficult.</span></li>
				<li><strong class="bold">Difficulty in dealing with long documents</strong>: Dense models that generate fixed-length vectors can sometimes struggle with very long content, where important information can be diluted or lost in the <span class="No-Break">embedding process.</span></li>
				<li><strong class="bold">Logical reasoning gaps</strong>: While these methods are excellent at capturing semantic similarity, they typically lack logical reasoning capabilities. This means that they can identify documents that are semantically similar to the query but may struggle to understand<a id="_idIndexMarker617"></a> context or logical relationships that require reasoning beyond this pattern matching. As a result, they may retrieve documents that are<a id="_idIndexMarker618"></a> superficially related to the query but not truly relevant to the user’s intent, especially in cases where the query requires an understanding of complex relationships or <span class="No-Break">nuanced reasoning.</span></li>
				<li><strong class="bold">Dependence on model quality</strong>: The effectiveness of a dense retrieval system is highly dependent on the quality of the underlying embedding model. Poorly trained models can result in suboptimal <span class="No-Break">retrieval performance.</span></li>
			</ul>
			<p>Next, we’ll talk about <span class="No-Break">sparse retrieval.</span></p>
			<h3 id="f_10__idParaDest-152" data-type="sect2" class="sect2" title2="Sparse retrieval" no2="6.5.2"><a id="_idTextAnchor151"></a>6.5.2. Sparse retrieval</h3>
			<p>Sparse retrieval methods associate<a id="_idIndexMarker619"></a> documents with keywords. These methods are based on exact keyword matching or overlaps between the query and <span class="No-Break">the documents.</span></p>
			<p>The general process involves indexing documents by analyzing them for important terms. These keywords are then recorded in inverted indexes, which are data structures used to quickly retrieve documents containing a <span class="No-Break">given keyword.</span></p>
			<p>During the retrieval phase, queries are searched against these inverted indexes to find documents that share keywords with the query. Documents are ranked based on the number of common terms identified between the query and each indexed document. One of the most common techniques used in sparse <a id="_idIndexMarker620"></a>retrieval is the <strong class="bold">Term Frequency – Inverse Document Frequency</strong> (<span class="No-Break"><strong class="bold">TF-IDF</strong></span><span class="No-Break">) method.</span></p>
			<h4 data-type="sect3" class="sect3" title2="TF-IDF in sparse retrieval" no2="6.5.2.1">6.5.2.1. TF-IDF in sparse retrieval</h4>
			<p>TF-IDF is a numerical statistic that reflects how important a word is to each document in a collection of documents. This method transforms text into a numerical representation that captures the significance of words in documents, taking into account both their frequency in individual documents and across the entire collection <span class="No-Break">of documents.</span></p>
			<ul>
				<li><strong class="bold">Term Frequency</strong> (<strong class="bold">TF</strong>) measures <a id="_idIndexMarker621"></a>how often a term occurs in a <a id="_idIndexMarker622"></a>document, normalized by the total number of terms in the document. It’s calculated by dividing the number of times a particular term – that is, a word – appears in a document by the total number of words in that document. This indicates the importance of the term within the <span class="No-Break">specific document.</span></li>
				<li><strong class="bold">Inverse Document Frequency</strong> (<strong class="bold">IDF</strong>) assesses the importance of the term across the collection. It is calculated by taking the logarithm of the ratio of the total number of documents to <a id="_idIndexMarker623"></a>the number of documents containing the term. This helps to downplay the importance of terms that occur very frequently in many documents. Common terms such as <em class="italic">the</em> or <em class="italic">is</em> appear in many documents and are less informative, so they have lower IDF scores. Unique terms have higher <span class="No-Break">IDF scores.</span></li>
			</ul>
			<p>The <strong class="bold">TF-IDF score</strong>, which is obtained by <a id="_idIndexMarker624"></a>multiplying the TF by the IDF, represents the importance of each term in a document, adjusted for its commonness across the collection. In sparse retrieval, each document is represented as a vector in a high-dimensional space, where each dimension corresponds to a unique term and the value is the TF-IDF <span class="No-Break">score: </span><a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://en.wikipedia.org/wiki/Tf%E2%80%93idf</span></a><span class="No-Break">.</span></p>
			<p>We call it <em class="italic">sparse</em> because, in this high-dimensional vector space, most dimensions (terms) will have a value of zero for any given document, indicating that most terms in the collection do not appear in that document. If we were to visualize these vectors, this would result in a <em class="italic">sparse</em> representation, with many zeros, as most documents contain only a small subset of the total vocabulary of <span class="No-Break">the collection.</span></p>
			<p>During retrieval, a query is also converted into its TF-IDF vector representation. The relevance of each document to the query is calculated using measures such as cosine similarity, and the documents are ranked accordingly. The top-ranked documents with the highest similarity scores to the query are then returned <span class="No-Break">as results.</span></p>
			<p>Sparse retrieval methods such as TF-IDF are particularly effective for tasks where exact term matching is important. However, they may not capture the semantic meaning of the text or the context in which terms are used, which can be addressed by more advanced retrieval techniques such as dense <span class="No-Break">retrieval methods.</span></p>
			<p>As you’ve probably guessed, they have several advantages compared to <span class="No-Break">dense retrieval:</span></p>
			<ul>
				<li><strong class="bold">Efficient handling of large datasets</strong>: Sparse retrieval methods, such as TF-IDF, are generally more efficient at handling large datasets. The inverted index structure allows fast search and retrieval of documents based on keyword matching, making it suitable for large collections <span class="No-Break">of text</span></li>
				<li><strong class="bold">High precision</strong>: Sparse methods often<a id="_idIndexMarker625"></a> provide high accuracy in scenarios where the exact matching of terms is critical. They excel at retrieving documents that contain specific keywords present in the user’s query, which is beneficial in applications where keyword specificity <span class="No-Break">is essential</span></li>
				<li><strong class="bold">Simplicity and interpretability</strong>: Sparse retrieval methods are conceptually simpler and more interpretable than dense methods. The fact that they rely on explicit keyword frequencies makes it easier to understand why certain documents are retrieved in response to <span class="No-Break">a query</span></li>
				<li><strong class="bold">Less resource intensive</strong>: Unlike dense retrieval, sparse methods do not require complex neural network models to generate embeddings. This makes them less resource-intensive in terms of computing power and memory requirements. This means they’re easier to deploy <span class="No-Break">and maintain</span></li>
				<li><strong class="bold">Less dependence on model variability</strong>: Because sparse retrieval doesn’t depend on the nuances of machine learning models to the same extent as dense retrieval, it’s generally more robust to variations in model quality. Performance is more predictable and consistent across <span class="No-Break">different datasets</span></li>
			</ul>
			<p>Sparse methods also have their limitations. Some of the most important are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Lack of semantic understanding</strong>: Sparse methods may not capture the semantic relationships between words. They may miss documents that are contextually relevant but do not share exact keyword matches with <span class="No-Break">the query.</span></li>
				<li><strong class="bold">Vulnerability to synonymy and polysemy</strong>: These methods struggle with synonymy – different words with similar meanings – and polysemy – words with multiple meanings – leading to potential misses or <span class="No-Break">irrelevant retrievals.</span></li>
				<li><strong class="bold">Failure to capture context and nuance</strong>: Sparse retrieval does not effectively capture the broader context or nuances in language that can be critical to understanding<a id="_idIndexMarker626"></a> the true intent behind <span class="No-Break">a query.</span></li>
			</ul>
			<h3 id="f_10__idParaDest-153" data-type="sect2" class="sect2" title2="Implementing sparse retrieval in LlamaIndex" no2="6.5.3"><a id="_idTextAnchor152"></a>6.5.3. Implementing sparse retrieval in LlamaIndex</h3>
			<p>At a core level, constructs such as <strong class="source-inline">KeywordTableIndex</strong> can already be considered a basic form of sparse retrieval. After all, they share most of the principles and methods described above. However, there are<a id="_idIndexMarker627"></a> even more advanced<a id="_idIndexMarker628"></a> sparse retrieval capabilities available <span class="No-Break">in LlamaIndex.</span></p>
			<p>A perfect example is <strong class="source-inline">BM25Retriever</strong>, which implements the <strong class="bold">Best Matching 25</strong> (<strong class="bold">BM25</strong>) <span class="No-Break">retrieval algorithm.</span></p>
			<p>BM25, a refinement of the TF-IDF method, is a more sophisticated algorithm that’s used for sparse retrieval. Unlike TF-IDF, BM25 takes into account both term frequency and document length, providing a more nuanced approach to document relevance scoring. With this retriever, nodes are ranked based on their BM25 scores relative to the query. The top-k nodes with the highest scores are returned as query results, providing users with the most <span class="No-Break">relevant results.</span></p>
			<p>Let’s look at an example of how we can <span class="No-Break">use </span><span class="No-Break"><strong class="source-inline">BM25Retriever</strong></span><span class="No-Break">.</span></p>
			<p>To use this particular retriever, you’ll need to install the required Python package and the corresponding LlamaIndex integration package by running the <span class="No-Break">following commands:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_165" title2="(no caption)" no2="">pip install rank-bm25
pip install llama-index-retrievers-bm25</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>After installing the <strong class="source-inline">rank-bm25</strong> package, you can test it with this <span class="No-Break">sample code:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_166" title2="(no caption)" no2="">from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core import SimpleDirectoryReader
reader = SimpleDirectoryReader('files')
documents = reader.load_data()
splitter = SentenceSplitter.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;chunk_size=60,
&nbsp;&nbsp;&nbsp;&nbsp;chunk_overlap=0,
&nbsp;&nbsp;&nbsp;&nbsp;include_metadata=False
)
nodes = splitter.get_nodes_from_documents(
&nbsp;&nbsp;&nbsp;&nbsp;documents
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>We’re using the two initial<a id="_idIndexMarker629"></a> sample files containing data about ancient Rome and different breeds of dogs. In this example, I’ve used <strong class="source-inline">SentenceSplitter</strong>, configured with a relatively small chunk size. That is because the sample file is small in size and I <a id="_idIndexMarker630"></a>wanted to produce more granular nodes structured as sentences to better exemplify the workings of <strong class="source-inline">BM25Retriever</strong>. Next, let’s implement <span class="No-Break">the retriever:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_167" title2="(no caption)" no2="">retriever = BM25Retriever.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;nodes=nodes,
&nbsp;&nbsp;&nbsp;&nbsp;similarity_top_k=2
)
response = retriever.retrieve("Who built the Colosseum? ")
for node_with_score in response:
&nbsp;&nbsp;&nbsp;&nbsp;print('Text:'+node_with_score.node.text)
&nbsp;&nbsp;&nbsp;&nbsp;print('Score: '+str(node_with_score.score))</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>After chunking the two documents, we use the retriever to apply the BM25 algorithm and retrieve the two most relevant chunks relative to our query about <span class="No-Break">the Colosseum.</span></p>
			<p>You can further experiment with <a id="_idIndexMarker631"></a>this sample and try to adjust the <strong class="source-inline">similarity_top_k</strong> parameter, the query, or the chunking strategy to better understand<a id="_idIndexMarker632"></a> how this <span class="No-Break">retriever works.</span></p>
			<h4 data-type="sect3" class="sect3" title2="When should we use sparse retrieval instead of dense retrieval?" no2="6.5.3.1">6.5.3.1. When should we use sparse retrieval instead of dense retrieval?</h4>
			<p>Let’s consider a practical example <a id="_idIndexMarker633"></a>of when sparse retrieval might give better results than dense retrieval in an <span class="No-Break">RAG application.</span></p>
			<p class="callout-heading">A practical use case for sparse retrieval</p>
			<p class="callout">Suppose we’ve built a system for retrieving legal documents. In this scenario, user queries would likely include precise legal terms, citations, or specific phrases found in legal texts. Let’s assume a user inputs a query such as, “<em class="italic">Article 45 of the GDPR regarding personal data transfers on the basis of an adequacy decision.</em>” This query contains specific phrases, such as “Article 45” and “GDPR,” which are likely to be found in relevant legal documents exactly in <span class="No-Break">this form.</span></p>
			<p class="callout">Sparse search is likely to provide very accurate results for such a query. It will accurately locate documents that contain the specific article from the GDPR, reducing noise and irrelevant retrievals. Given that legal documents often have a structured format, with different sections and articles, sparse retrieval methods can efficiently parse through this structured data and retrieve nodes based on direct references found in <span class="No-Break">the query.</span></p>
			<p>Because dense retrieval methods tend to prioritize general meaning over exact term matching, they may produce less accurate results in such a specialized, <span class="No-Break">keyword-specific query.</span></p>
			<p>Unless trained specifically on legal texts, an embedding model used for dense retrieval might struggle to accurately interpret and match the complex legal jargon and specific citation styles used in <span class="No-Break">legal queries.</span></p>
			<h4 data-type="sect3" class="sect3" title2="When would dense retrieval be a better choice?" no2="6.5.3.2">6.5.3.2. When would dense retrieval be a better choice?</h4>
			<p>Here’s another <span class="No-Break">practical example.</span></p>
			<p>A typical use case where dense retrieval would most likely produce better results would be a customer support chatbot <a id="_idIndexMarker634"></a>designed to understand and respond to a wide range of customer queries. Let’s<a id="_idIndexMarker635"></a> say the chatbot is tasked with assisting users with various issues <a id="_idIndexMarker636"></a>related to technical products, such as hardware troubleshooting, software features, usage tips, and general inquiries about products <span class="No-Break">and services.</span></p>
			<p>A user might ask a question such as “<em class="italic">My laptop battery is draining really quickly, even when I’m not using it much. What can I do about it?</em>” Because dense search excels at understanding the semantic context of queries, in this case, it could understand the broader meaning behind phrases such as “battery drains really fast” and relate them to similar problems, even if the exact phrase isn’t in the <span class="No-Break">knowledge base.</span></p>
			<p>Sparse methods, on the other hand, may not perform well if the query doesn’t contain specific keywords that are present in the support documents. In our example, the user might describe a problem using different terms to those used in the technical manuals <span class="No-Break">or FAQs.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Can we combine the two methods in a single retriever?" no2="6.5.3.3">6.5.3.3. Can we combine the two methods in a single retriever?</h4>
			<p>The short answer is yes. You’ve probably already guessed that I’m building a case along these lines. By combining them, we’d get the best of both worlds in terms of benefits and features. A few pages ago, we talked about using selectors and routers to implement more complex query behavior in our <span class="No-Break">RAG application.</span></p>
			<p>I’ll leave it up to you to adapt the methods I’ve demonstrated there and implement a hybrid system that uses both dense and sparse retrieval methods. If you feel the need for an additional example, you can have a look at this one, which uses the Pinecone vector database to implement hybrid <span class="No-Break">search: </span><a href="https://docs.llamaindex.ai/en/stable/examples/vector_stores/PineconeIndexDemo-Hybrid.html" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/vector_stores/PineconeIndexDemo-Hybrid.html</span></a><span class="No-Break">.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Dealing with empty results from the retrieval process" no2="6.5.3.4">6.5.3.4. Dealing with empty results from the retrieval process</h4>
			<p>Sometimes, our retrievers may come up empty-handed, without finding any indexed content matching the current query. This typically means that there are no relevant nodes in the index for that <span class="No-Break">particular query.</span></p>
			<p>In such cases, the retriever may return an empty result set, indicating that no matching nodes were found. Depending on the type of index used, this situation can arise if the query keywords are very specific or rare, and none of the nodes in the index contain those exact keywords, or, in the case of embedding-based indexes, the similarity search that was performed during the search did not find any matching nodes with the current parameters used. To handle this scenario, we can consider <span class="No-Break">various approaches:</span></p>
			<ul>
				<li><strong class="bold">Fallback mechanisms</strong>: The search system can have fallback strategies in place, such as performing a more general search by adjusting the retriever’s parameters or suggesting alternative query terms to <span class="No-Break">the user.</span></li>
				<li><strong class="bold">Query expansion</strong>: The query can be automatically expanded to include synonyms, related terms, or broader concepts to increase the chances of finding <span class="No-Break">relevant nodes.</span></li>
				<li><strong class="bold">Relevance scoring</strong>: Even if no exact keyword matches are found, the search system can employ relevance<a id="_idIndexMarker637"></a> scoring algorithms to identify nodes that are semantically similar<a id="_idIndexMarker638"></a> to the query or contain <span class="No-Break">partial matches.</span></li>
			</ul>
			<h3 id="f_10__idParaDest-154" data-type="sect2" class="sect2" title2="Discovering other advanced retrieval methods" no2="6.5.4"><a id="_idTextAnchor153"></a>6.5.4. Discovering other advanced retrieval methods</h3>
			<p>In addition to the basic concepts just discussed, several other advanced retrieval methods are worth familiarizing yourself with. There is a special section in the official documentation where these methods are <span class="No-Break">explained: </span><a href="https://docs.llamaindex.ai/en/stable/optimizing/advanced_retrieval/advanced_retrieval.html" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/stable/optimizing/advanced_retrieval/advanced_retrieval.html</span></a><span class="No-Break">.</span></p>
			<p>There, you will learn more about special techniques, such as <em class="italic">Small-to-Big</em> <em class="italic">retrieval</em>, <em class="italic">recursive retrieval</em>,<em class="italic"> retrieval from embedded tables</em>, <em class="italic">multi-modal retrieval</em>, <em class="italic">auto-merging retrieval</em>, <span class="No-Break">and others.</span></p>
			<p>A detailed explanation of each retrieval strategy would go far beyond what I intend to cover in this book, but that doesn’t mean they aren’t important. After all, there is no point in ingesting and indexing the original documents if we cannot effectively extract the context we need <span class="No-Break">in RAG.</span></p>
			<p class="callout-heading">Practical advice</p>
			<p class="callout">Always read the latest version of the official documentation before starting a major project. Things move so fast, and new methods and techniques emerge so quickly, that it is a shame to waste time reinventing the wheel. As an anecdote, I can tell you from personal experience that I have spent hours <em class="italic">inventing</em> something very similar to the <em class="italic">small-to-big</em> method, only to discover a few days later that it was already a tested and <span class="No-Break">documented technique.</span></p>
			<p>That’s enough information <a id="_idIndexMarker639"></a>for one chapter. We’ll skip the PITS coding practice now as we’ll let more<a id="_idIndexMarker640"></a> information accumulate in the next chapter before implementing additional features in our personal <span class="No-Break">tutoring project.</span></p>
			<h2 id="f_10__idParaDest-155" data-type="sect1" class="sect1" title2="Summary" no2="6.6"><a id="_idTextAnchor154"></a>6.6. Summary</h2>
			<p>In this chapter, we explored various querying strategies and architectures within LlamaIndex with a deep focus on retrievers. Retrievers provide essential capabilities for extracting relevant information from indexes to generate useful responses in RAG systems. Throughout this chapter, we looked at basic retriever types such as <strong class="source-inline">VectorIndexRetriever</strong> and <strong class="source-inline">SummaryIndexRetriever</strong>. We also gained an understanding of advanced concepts such as asynchronous retrieval, metadata filters, tools, selectors, and query transformations. These allow us to build more sophisticated <span class="No-Break">retrieval logic.</span></p>
			<p>Additionally, we covered fundamental paradigms such as dense retrieval and sparse retrieval and discussed their strengths and weaknesses. Implementations in LlamaIndex such as BM25Retriever were <span class="No-Break">also introduced.</span></p>
			<p>Overall, this chapter provided an overview of retrieval capabilities in LlamaIndex, laying the foundation for building high-performance and contextually-aware <span class="No-Break">RAG applications.</span></p>
			<p>We’re now equipped with the necessary knowledge to effectively retrieve information from indexes. In the next chapter, we’ll build on this knowledge by addressing the other important components of a query engine: post-processors and <span class="No-Break">response synthesizers.</span></p>
		</div>
<div id="f_11__idContainer077" data-type="chapter" class="chapter" file="B21861_07_xhtml" title2="Querying Our Data, Part 2 – Postprocessing and Response Synthesis" no2="7">
			<h1 id="f_11__idParaDest-156" class="chapter-number"><a id="_idTextAnchor155"></a>7</h1>
			<h1 id="f_11__idParaDest-157"><a id="_idTextAnchor156"></a>Querying Our Data, Part 2 – Postprocessing and Response Synthesis</h1>
			<p>Building on the knowledge acquired in the previous chapter, we will now explore various postprocessing techniques to refine the retrieved context before covering the final query response synthesis. Afterward, we will learn how to bring all these components together into powerful query engines so that we can perform end-to-end natural language querying over documents. We’ll also get to practice our new skills by working on our <span class="No-Break">tutoring project.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Re-ranking, transforming, and filtering nodes <span class="No-Break">using postprocessors</span></li>
				<li>Understanding the <span class="No-Break">response synthesizers</span></li>
				<li>Implementing output <span class="No-Break">parsing techniques</span></li>
				<li>Building and using <span class="No-Break">query engines</span></li>
				<li>Hands-on – building quizzes <span class="No-Break">in PITS</span></li>
			</ul>
			<h2 id="f_11__idParaDest-158" data-type="sect1" class="sect1" title2="Technical requirements" no2="7.1"><a id="_idTextAnchor157"></a>7.1. Technical requirements</h2>
			<p>For this chapter, you will need to install the following packages in <span class="No-Break">your environment:</span></p>
			<ul>
				<li><em class="italic">spaCy</em>:  <a href="https://spacy.io/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://spacy.io/</span></a></li>
				<li><span class="No-Break"><em class="italic">Guardrails-AI</em></span><span class="No-Break">: </span><a href="https://www.guardrailsai.com/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://www.guardrailsai.com/</span></a></li>
				<li><span class="No-Break"><em class="italic">pandas</em></span><span class="No-Break">: </span><a href="https://pandas.pydata.org/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pandas.pydata.org/</span></a></li>
			</ul>
			<p>All the code samples in this chapter can be found in the <strong class="source-inline">ch7</strong> subfolder of this book’s GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex</span></a><span class="No-Break">.</span></p>
			<h2 id="f_11__idParaDest-159" data-type="sect1" class="sect1" title2="Re-ranking, transforming, and filtering nodes using postprocessors" no2="7.2"><a id="_idTextAnchor158"></a>7.2. Re-ranking, transforming, and filtering nodes using postprocessors</h2>
			<p>In the previous chapter, we discussed the various retrieval methods that LlamaIndex offers. We extracted the necessary context to be able to enrich and improve the query we are now sending to the LLM. But is <span class="No-Break">this enough?</span></p>
			<p>As we have already discussed, <em class="italic">naive</em> retrieval methods are unlikely to produce ideal results in any scenario. There will probably be many situations where the returned nodes will perhaps contain irrelevant information or will not be sorted in chronological order. These kinds of situations could put the LLM in difficulty, adversely affecting the quality of the prompt that our RAG <span class="No-Break">application builds.</span></p>
			<p class="callout-heading">A quick side notes</p>
			<p class="callout">In case it wasn’t already obvious, the main purpose of a RAG flow is to programmatically build prompts. Instead of manually building these prompts and then inputting them into a ChatGPT-like interface, LlamaIndex dynamically assembles the prompts from our documents, which are split into nodes and then indexed and selected using retrievers. Many things could go wrong in this process. Maybe we didn’t ingest the original documents completely or correctly, or maybe we didn’t choose the right <strong class="source-inline">chunk_size</strong> value and ended up with nodes that were too granular or too loaded with irrelevant information. Maybe we didn’t index them correctly, or maybe the retriever we used simply didn’t select the nodes in the correct order or brought in more information than <span class="No-Break">we wanted.</span></p>
			<p>There are many points where errors could creep into the whole process. That doesn’t sound very encouraging, <span class="No-Break">does it?</span></p>
			<p>The good news is that we still have an opportunity to improve this context before the final step of sending the information to the LLM. This opportunity comes in the form of <strong class="bold">node postprocessors</strong> and <span class="No-Break"><strong class="bold">response synthesizers</strong></span><span class="No-Break">.</span></p>
			<p>But first, let’s understand how <span class="No-Break">postprocessors work.</span></p>
			<p>Node postprocessors are critical in refining the results that are obtained from the retrieval process. That is because no matter how good the retrieval step is, there is always a chance of additional, unnecessary retrieved data <em class="italic">polluting</em> our context and confusing the LLM. In other cases, the retrieved nodes might be relevant but not necessarily in the correct order, and that can also affect the quality of the <span class="No-Break">LLM’s response.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.1</em> depicts the role of the postprocessors in a <span class="No-Break">RAG workflow:</span></p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B21861_07_1.jpg" alt="Figure 7.1 – The role of node postprocessors in RAG" width="1650" height="727" data-type="figure" id="untitled_figure_48" title2="– The role of node postprocessors in RAG" no2="7.1">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – The role of node postprocessors in RAG</p>
			<p>These processors operate on a set of nodes, applying transformations or filters to enhance the relevance and quality of the information. They can be used on their own, to process a given set of nodes, but they are more commonly used within query engines, after the node retrieval step and before response synthesis. LlamaIndex provides various built-in processors but also the option of building custom <span class="No-Break">postprocessing logic.</span></p>
			<p>Let’s begin by understanding the different purposes and operating modes of <span class="No-Break">node postprocessors.</span></p>
			<h3 id="f_11__idParaDest-160" data-type="sect2" class="sect2" title2="Exploring how postprocessors filter, transform, and re-rank nodes" no2="7.2.1"><a id="_idTextAnchor159"></a>7.2.1. Exploring how postprocessors filter, transform, and re-rank nodes</h3>
			<p>At their core, all node postprocessors work by adjusting the retrieved context before that context gets injected into a prompt and sent to the LLM for response synthesis. They operate by either filtering, transforming, or re-ranking nodes. Let’s have a look at these operating modes to get a <span class="No-Break">better understanding.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Node filtering postprocessors" no2="7.2.1.1">7.2.1.1. Node filtering postprocessors</h4>
			<p>Node filtering postprocessors are designed to remove irrelevant or unnecessary nodes from the <a id="_idIndexMarker641"></a>set of retrieved results. They work by applying specific criteria to each node and discarding those that don’t meet the requirements. For example, <strong class="source-inline">SimilarityPostprocessor</strong> filters out nodes whose similarity score falls below a specified threshold, ensuring that only highly relevant nodes are passed to the language model for response generation. Similarly, <strong class="source-inline">KeywordNodePostprocessor</strong> keeps only the nodes that contain certain required keywords or excludes nodes with specific unwanted keywords. Node filtering helps to reduce information overload and improve the quality of the final response by focusing on the most <span class="No-Break">pertinent information.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Node transforming postprocessors" no2="7.2.1.2">7.2.1.2. Node transforming postprocessors</h4>
			<p>Node transforming postprocessors modify the content of the retrieved nodes without necessarily <a id="_idIndexMarker642"></a>removing any of them. These postprocessors aim to enhance the relevance and usefulness of the information within each node. One example is <strong class="source-inline">MetadataReplacementPostprocessor</strong>, which replaces the content of a node with a specific field from that node’s metadata. This allows the text being used to be dynamically adjusted to represent a node based on its metadata rather than the original ingested content. Another example is <strong class="source-inline">SentenceEmbeddingOptimizer</strong>, which optimizes longer text passages by selecting the most relevant sentences within a node based on their semantic similarity to the query. By transforming the nodes’ content, these postprocessors help align the information more closely with the user’s query and improve the overall quality of the <span class="No-Break">generated response.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Node re-ranking postprocessors" no2="7.2.1.3">7.2.1.3. Node re-ranking postprocessors</h4>
			<p>These postprocessors don’t specifically remove or change the retrieved nodes. The purpose <a id="_idIndexMarker643"></a>of a re-ranker is to take the initial set of nodes returned by the retriever and reorder them based on their relevance to the given query. This is particularly important when dealing with long-form queries or complex information needs as many LLMs struggle to effectively process and generate accurate responses when provided with lengthy or multi-faceted contexts. By employing a re-ranker, the RAG system can prioritize the most pertinent information and present it to the LLM in a more coherent format, thus leading to <span class="No-Break">better responses.</span></p>
			<p>Re-rankers often leverage advanced techniques such as deep learning, transformers, or LLMs themselves to assess the relevance of each retrieved document or passage. They may consider factors such as semantic similarity, context overlap, or query-document alignment to <a id="_idIndexMarker644"></a>assign relevance scores to the retrieved nodes. The top-ranked nodes are then fed into the LLM, which generates the final response based on this refined context, enhancing the overall performance and utility of the RAG system. By incorporating a re-ranking step into the RAG pipeline, the system can overcome the limitations of LLMs in handling long or complex queries, ultimately providing more accurate, relevant, and useful responses <span class="No-Break">to users.</span></p>
			<p>Next, we’ll explore the built-in LlamaIndex postprocessors in all <span class="No-Break">three categories.</span></p>
			<h3 id="f_11__idParaDest-161" data-type="sect2" class="sect2" title2="SimilarityPostprocessor" no2="7.2.2"><a id="_idTextAnchor160"></a>7.2.2. SimilarityPostprocessor</h3>
			<p><strong class="source-inline">SimilarityPostprocessor</strong> filters nodes by comparing them to a similarity score threshold. Nodes <a id="_idIndexMarker645"></a>that score below this threshold are removed, ensuring only relevant and similar content to the query remains. This is particularly useful because it ensures that the nodes that are passed to the language model for response generation are relevant by having a high degree of semantic correlation with <span class="No-Break">the query.</span></p>
			<p class="callout-heading">A potential use cases</p>
			<p class="callout">An e-commerce company has a customer support chatbot powered by an LLM. Let’s assume that the chatbot retrieves nodes from <strong class="source-inline">KeywordTableIndex</strong> and tries to identify all contexts based on the keywords contained in the user query. For a query such as, <em class="italic">How do I return a damaged item I received yesterday?</em>, the retrieved nodes might include general return policies, product descriptions for items ordered by the customer, shipping information, and even irrelevant product advertisements or promotions. <strong class="source-inline">SimilarityPostprocessor</strong> could filter out nodes that are not closely related to the specific context of the query. In this case, it would prioritize nodes specifically discussing return policies for damaged items and recent orders by the customer, while discarding general product advertisements and unrelated shipping details. That would greatly increase the chance of the LLM producing a more <span class="No-Break">meaningful response.</span></p>
			<p>This postprocessor takes a list of nodes, typically fetched by a retriever, as input, each with an associated similarity score. The postprocessor can be configured with a <strong class="source-inline">similarity_cutoff</strong> parameter. This threshold determines the minimum score a node must have to be considered relevant. If a node’s similarity score is <strong class="source-inline">None</strong> or if it’s lower than <strong class="source-inline">similarity_cutoff</strong>, the node is considered not to meet the threshold and is therefore excluded from the final list. Essentially, this postprocessor filters out any nodes that have a similarity score below the set threshold. This ensures that only nodes closely <a id="_idIndexMarker646"></a>related to the query are retained. The nodes meeting or exceeding the similarity score threshold is then passed on for further processing or response synthesis. Here’s a simple example of how we can use it <span class="No-Break">in practice:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_168" title2="(no caption)" no2="">from llama_index.core.postprocessor import SimilarityPostprocessor
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
reader = SimpleDirectoryReader('files/other')
documents = reader.load_data()
index = VectorStoreIndex.from_documents(documents)
retriever = index.as_retriever(retriever_mode='default')
nodes = retriever.retrieve(
"What did Fluffy found in the gentle stream?"
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In the first part of the code, we took care of the imports and then ingested a sample file into a document. Then, we created a <strong class="source-inline">VectorStoreIndex</strong> index and used the default retriever to fetch relevant nodes based on <span class="No-Break">a query:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_169" title2="(no caption)" no2="">Print('Initial nodes:')
for node in nodes:
print(f"Node: {node.node_id} – Score: {node.score}")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Here, we printed the original list of nodes since they were fetched by the retriever. Now, let’s apply <span class="No-Break">the postprocessor.</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_170" title2="(no caption)" no2="">Pp = SimilarityPostprocessor(
&nbsp;&nbsp;&nbsp;&nbsp;nodes=nodes,
&nbsp;&nbsp;&nbsp;&nbsp;similarity_cutoff=0.86
)
remaining_nodes = pp.postprocess_nodes(nodes)
print('Remaining nodes:')
for node in remaining_nodes:
print(f"Node: {node.node_id} – Score: {node.score}")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>After building <a id="_idIndexMarker647"></a>and applying the postprocessor on the nodes, we print the remaining nodes. The output will be similar to <span class="No-Break">the following:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_171" title2="(no caption)" no2="">Initial nodes:
Node: da51464d-e83f-4aec-a9db-8bd839ab3a4c - Score: 0.8516122822966049
Node: f839ec27-e487-4132-b139-79e3695d5500 - Score: 0.8368901228748273
Remaining nodes:
Node: da51464d-e83f-4aec-a9db-8bd839ab3a4c - Score: 0.8516122822966049</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>As we can see, the second node from the initial list was removed because it had a score below the threshold we defined – <span class="No-Break">0.85.</span></p>
			<h3 id="f_11__idParaDest-162" data-type="sect2" class="sect2" title2="KeywordNodePostprocessor" no2="7.2.3"><a id="_idTextAnchor161"></a>7.2.3. KeywordNodePostprocessor</h3>
			<p><strong class="source-inline">KeywordNodePostprocessor</strong> is designed to refine the selection of nodes based on specific keywords. This <a id="_idIndexMarker648"></a>postprocessor works by ensuring that the retrieved nodes either contain certain required keywords or exclude specific unwanted keywords. It’s a great method for aligning the content of the nodes more closely with the user’s query by focusing on <span class="No-Break">keyword relevance.</span></p>
			<p class="callout-heading">Practical use case in a RAG scenario</p>
			<p class="callout">Imagine a scenario in a corporate environment where the RAG system is used to retrieve information from a vast internal database for employee queries. However, there are certain confidential files or sections of files that should not be accessible to all employees. By configuring <strong class="source-inline">KeywordNodePostprocessor</strong> with keywords that indicate sensitive content (such as <em class="italic">confidential</em>, <em class="italic">restricted</em>, or specific project code names), the system can automatically exclude nodes containing these keywords from the retrieval results. This setup ensures that sensitive information is not inadvertently disclosed, maintaining the integrity and confidentiality of the <span class="No-Break">corporate data.</span></p>
			<p>It takes a list of nodes as input, typically fetched by a retriever, and is configured with parameters <a id="_idIndexMarker649"></a>for required and excluded keywords. <strong class="source-inline">KeywordNodePostprocessor</strong> then processes these nodes, keeping only those that meet the keyword criteria. This ensures that the final set of nodes is highly relevant to the specific query, leading to more accurate and useful responses in a <span class="No-Break">RAG system.</span></p>
			<p class="callout-heading">Quick note</p>
			<p class="callout">The postprocessor <a id="_idIndexMarker650"></a>relies on the <strong class="source-inline">spaCy</strong> library (<a href="https://pypi.org/project/spacy/" target="_blank" rel="noopener noreferrer">https://pypi.org/project/spacy/</a>), which you must install on your system before running the next example. This is a powerful Python library for advanced NLP. Its features include neural network models for various NLP tasks such as tagging, parsing, and NER. It’s a piece of commercial open source software available under an <span class="No-Break">MIT license.</span></p>
			<p>To use <strong class="source-inline">KeywordNodePostprocessor</strong>, make sure you install spaCy in your environment by running the <span class="No-Break">following command:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_172" title2="(no caption)" no2="">pip install spacy</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Here’s a basic example of how to use this postprocessor to filter out some log entries based on their <span class="No-Break">classification label:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_173" title2="(no caption)" no2="">from llama_index.core.postprocessor import KeywordNodePostprocessor
from llama_index.core.schema import TextNode, NodeWithScore
nodes = [
&nbsp;&nbsp;&nbsp;&nbsp;TextNode(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text="Entry no: 1, &lt;SECRET&gt;, Attack at Dawn"
&nbsp;&nbsp;&nbsp;&nbsp;),
&nbsp;&nbsp;&nbsp;&nbsp;TextNode(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text="Entry no: 2, &lt;RESTRICTED&gt;, Go to point Bravo"
&nbsp;&nbsp;&nbsp;&nbsp;),
&nbsp;&nbsp;&nbsp;&nbsp;TextNode(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text="Entry no: 3, &lt;PUBLIC&gt;, text: Roses are Red"
&nbsp;&nbsp;&nbsp;&nbsp;),
]</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In this example, we’re manually defining the nodes instead of ingesting data from external files. After we define the nodes, we have to wrap them into <strong class="source-inline">NodeWithScore</strong> because that’s <a id="_idIndexMarker651"></a>the expected input of <span class="No-Break">the postprocessor:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_174" title2="(no caption)" no2="">node_with_score_list = [
&nbsp;&nbsp;&nbsp;&nbsp;NodeWithScore(node=node) for node in nodes
]
pp = KeywordNodePostprocessor(
&nbsp;&nbsp;&nbsp;&nbsp;exclude_keywords=["SECRET", "RESTRICTED"]
)
remaining_nodes = pp.postprocess_nodes(
&nbsp;&nbsp;&nbsp;&nbsp;node_with_score_list
)
print('Remaining nodes:')
for node_with_score in remaining_nodes:
&nbsp;&nbsp;&nbsp;&nbsp;node = node_with_score.node
&nbsp;&nbsp;&nbsp;&nbsp;print(f"Text: {node.text}")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In this example, <strong class="source-inline">KeywordNodePostprocessor</strong> filters the nodes fetched by the retriever, excluding those that include <strong class="source-inline">SECRET</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">RESTRICTED</strong></span><span class="No-Break">.</span></p>
			<p>Several parameters <a id="_idIndexMarker652"></a>can be customized with this postprocessor. The most important ones are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="source-inline">required_keywords</strong>: This is a list of strings, where each string represents a keyword <a id="_idIndexMarker653"></a>that must be present in the node for it to be included in the final output. If this list is not empty, the postprocessor will filter out any nodes that do not contain <span class="No-Break">these keywords.</span></li>
				<li><strong class="source-inline">exclude_keywords</strong>: Similar to <strong class="source-inline">required_keywords</strong>, this is also a list of strings. However, in this case, any node containing a keyword from this list will be excluded <a id="_idIndexMarker654"></a>from the final output. It’s used for filtering out nodes based on <span class="No-Break">unwanted content.</span></li>
				<li><strong class="source-inline">lang</strong>: This argument <a id="_idIndexMarker655"></a>specifies the language model to be used by the internal spaCy NLP library. The default value is <em class="italic">en</em> for English, but it can be set to other language codes supported by Spacy. The effectiveness and accuracy of keyword matching might depend on the language-specific processing of the text. For example, the way words are tokenized by Spacy can affect how keywords <span class="No-Break">are identified.</span></li>
			</ul>
			<p>Keep in mind that keywords – both required and excluded – are processed in a case-sensitive way. To ensure consistent behavior regardless of case, you might consider converting both the keywords and the text in the nodes into the same case (for example, all lowercase) <span class="No-Break">before processing.</span></p>
			<h3 id="f_11__idParaDest-163" data-type="sect2" class="sect2" title2="PrevNextNodePostprocessor" no2="7.2.4"><a id="_idTextAnchor162"></a>7.2.4. PrevNextNodePostprocessor</h3>
			<p><strong class="source-inline">PrevNextNodePostprocessor</strong> is designed to enhance node retrieval by fetching additional <a id="_idIndexMarker656"></a>nodes based on their relational context in the document. This postprocessor can operate in three modes – <strong class="source-inline">previous</strong>, <strong class="source-inline">next</strong>, or <strong class="source-inline">both</strong> – allowing users to retrieve nodes that are either preceding, succeeding, or both concerning the current set <span class="No-Break">of nodes.</span></p>
			<p class="callout-heading">A potential use cases</p>
			<p class="callout">Consider a legal research scenario where a user queries a RAG system about a specific legal case. <strong class="source-inline">PrevNextNodePostprocessor</strong> can be set in <em class="italic">both</em> modes to retrieve not only the nodes directly related to the case but also the preceding and succeeding nodes that might contain vital contextual information, such as related legal precedents or subsequent rulings. This ensures a comprehensive understanding of the case by providing a broader context, which is especially crucial in legal research where every <span class="No-Break">detail matters.</span></p>
			<p>The process begins by taking a list of nodes, typically fetched by a retriever. It then extends this <a id="_idIndexMarker657"></a>list by adding nodes that are directly preceding, succeeding, or both, based on the configured mode. This results in a more contextually enriched set of nodes, leading to responses that are more nuanced and comprehensive <a id="_idIndexMarker658"></a>in a RAG system. Here’s a list of the parameters for <span class="No-Break">this postprocessor:</span></p>
			<ul>
				<li><strong class="source-inline">docstore</strong>: The actual document store storing <span class="No-Break">the nodes.</span></li>
				<li><strong class="source-inline">num_nodes</strong>: This sets the number of nodes to return. By default, it returns 1 node in the <span class="No-Break">chosen direction.</span></li>
				<li><strong class="source-inline">mode</strong>: Can be set to previous, next, <span class="No-Break">or both.</span></li>
			</ul>
			<p>Additionally, we have <strong class="source-inline">AutoPrevNextNodePostprocessor</strong>, which is an advanced variation of <strong class="source-inline">PrevNextNodePostprocessor</strong>. This one is intelligently inferring whether to fetch additional nodes based on the <em class="italic">previous</em>, <em class="italic">next</em>, or neither relationship in response to the <span class="No-Break">query context.</span></p>
			<p>In comparison to <strong class="source-inline">PrevNextNodePostprocessor</strong>, which requires manual setting for mode selection, <strong class="source-inline">AutoPrevNextNodePostprocessor</strong> automates this process. It utilizes specific prompts to infer the direction (previous, next, or none) based on the current context and <span class="No-Break">the query.</span></p>
			<p>This inference is particularly useful in scenarios where the direction of node retrieval isn’t explicitly clear or when it needs to be dynamically determined based on the nature of the query and existing answers. For example, in a scenario where a RAG system is used for historical research, <strong class="source-inline">AutoPrevNextNodePostprocessor</strong> can automatically determine whether to fetch preceding or succeeding historical events or data points based on the query’s context, enhancing the relevance and comprehensiveness of <span class="No-Break">the response.</span></p>
			<p>This capability makes it useful in applications where the sequence of information and its contextual <a id="_idIndexMarker659"></a>relevance are essential for generating accurate and <span class="No-Break">useful responses.</span></p>
			<p>The prompts can be customized using the <strong class="source-inline">infer_prev_next_tmpl</strong> and <strong class="source-inline">refine_prev_next_tmpl</strong> arguments. There’s also a <strong class="source-inline">Verbose</strong> argument, which provides more visibility on the <span class="No-Break">selection process.</span></p>
			<h3 id="f_11__idParaDest-164" data-type="sect2" class="sect2" title2="LongContextReorder" no2="7.2.5"><a id="_idTextAnchor163"></a>7.2.5. LongContextReorder</h3>
			<p><strong class="source-inline">LongContextReorder</strong> is specifically designed to improve the performance of LLMs in handling <a id="_idIndexMarker660"></a>long context scenarios. Research has shown that significant details in extended contexts are better utilized when positioned at the start or end of the input context <em class="italic">(Liu et al., Lost in the Middle: How Language Models Use Long Contexts (2023)</em> – <a href="https://arxiv.org/abs/2307.03172" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2307.03172</a>). The <strong class="source-inline">LongContextReorder</strong> postprocessor addresses this by reordering the nodes, placing crucial information where it’s more accessible to <span class="No-Break">the model.</span></p>
			<p class="callout-heading">A practical scenario</p>
			<p class="callout">In a RAG system, particularly in academic or research-oriented queries where long, detailed documents are common, <strong class="source-inline">LongContextReorder</strong> can be very useful. For instance, if a user queries about detailed historical events, the system might retrieve lengthy nodes encompassing extensive details. <strong class="source-inline">LongContextReorder</strong> would rearrange these nodes, ensuring that the most relevant details are positioned at the beginning or end, thereby enhancing the model’s ability to extract and utilize this crucial information effectively. This results in responses that are more coherent and contextually rich, significantly improving the overall quality of the output in cases involving <span class="No-Break">lengthy contexts.</span></p>
			<p><strong class="source-inline">LongContextReorder</strong> takes a list of nodes, typically fetched by a retriever, and reorders them based on their relevance scores. The goal is to optimize the arrangement of information in a way that maximizes the language model’s ability to access and process significant details, especially in cases where the context length might otherwise <span class="No-Break">hinder performance.</span></p>
			<p>This postprocessor is particularly effective in scenarios where detailed and comprehensive responses are required, ensuring that the most relevant information is presented in a way that is most accessible to <span class="No-Break">the model.</span></p>
			<h3 id="f_11__idParaDest-165" data-type="sect2" class="sect2" title2="PIINodePostprocessor and NERPIINodePostprocessor" no2="7.2.6"><a id="_idTextAnchor164"></a>7.2.6. PIINodePostprocessor and NERPIINodePostprocessor</h3>
			<p>These <a id="_idIndexMarker661"></a>postprocessors mask <strong class="bold">personally identifiable information</strong> (<strong class="bold">PII</strong>) in nodes, improving privacy and security. <strong class="source-inline">PIINodePostprocessor</strong> is designed to use a local model, while <strong class="source-inline">NERPIINodePostprocessor</strong> relies on a NER model from Hugging Face. We saw an example of <a id="_idIndexMarker662"></a>how this postprocessor works in <a href="#_idTextAnchor068"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Ingesting Data into Our RAG Workflow</em>, in the <em class="italic">Scrubbing personal data and other sensitive </em><span class="No-Break"><em class="italic">information</em></span><span class="No-Break"> section.</span></p>
			<p><strong class="source-inline">PIINodePostprocessor</strong> takes <a id="_idIndexMarker663"></a>the <span class="No-Break">following arguments:</span></p>
			<ul>
				<li><strong class="source-inline">llm</strong>: This object should contain a local model <span class="No-Break">for processing.</span></li>
				<li><strong class="source-inline">pii_str_tmpl</strong>: This can be used to customize the default prompt template used for masking <span class="No-Break">personal data.</span></li>
				<li><strong class="source-inline">pii_node_info_key</strong>: This string serves as a key in the node’s metadata to store information related to PII processing. It’s used to track and reference the PII data processed within each node. It can be used to later recompose the original information <span class="No-Break">if required.</span></li>
			</ul>
			<p><strong class="source-inline">NERPIINodePostprocessor</strong> can be configured with the <strong class="source-inline">pii_node_info_key</strong> parameter. Similar to <a id="_idIndexMarker664"></a>the previous postprocessor, this string key is used to store information related to PII processing in the node’s metadata. It’s a unique identifier within the node metadata for tracking the PII data that has <span class="No-Break">been processed.</span></p>
			<p class="callout-heading">Best practice</p>
			<p class="callout">As we discussed in <a href="#_idTextAnchor068"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Ingesting Data into Our RAG Workflow</em>, for maximum privacy, the best approach is to apply PII masking before the actual retrieval. This way, you ensure that no sensitive data is sent to any <span class="No-Break">external LLM.</span></p>
			<p>Let’s see what other postprocessors <span class="No-Break">we have.</span></p>
			<h3 id="f_11__idParaDest-166" data-type="sect2" class="sect2" title2="MetadataReplacementPostprocessor" no2="7.2.7"><a id="_idTextAnchor165"></a>7.2.7. MetadataReplacementPostprocessor</h3>
			<p><strong class="source-inline">MetadataReplacementPostProcessor</strong> is designed to replace the content of a node <a id="_idIndexMarker665"></a>with a specific field from that node’s metadata. This allows us to dynamically switch the text that’s used to represent a node based on metadata instead of the original <span class="No-Break">ingested content.</span></p>
			<p class="callout-heading">A useful application for this postprocessor</p>
			<p class="callout">Imagine a workflow where files are ingested via <strong class="source-inline">SentenceWindowNodeParser</strong>, which splits text into sentence-level nodes and captures the surrounding text in metadata. By configuring the processor to swap the node’s content with the metadata field containing the <em class="italic">sentence window</em>, queries would retrieve full sentence context instead of sentence fragments. This allows the retriever to operate on sentences for higher accuracy while still exposing broader document context to the LLM. This technique can be very useful for processing large documents. You can find a complete example <span class="No-Break">here: </span><a href="https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/MetadataReplacementDemo.html" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/MetadataReplacementDemo.html</span></a><span class="No-Break">.</span></p>
			<p>This postprocessor takes a list of nodes as input and is configured with the <strong class="source-inline">target_metadata_key</strong> parameter, specifying which metadata field to use for the replacement. <strong class="source-inline">MetadataReplacementPostProcessor</strong> processes the nodes by replacing the <strong class="source-inline">text</strong> attribute of each node with the contents of the given metadata key. If the key is missing, the original text is kept. This provides flexibility to transform node content on <span class="No-Break">the fly.</span></p>
			<p>Here’s another, simple example that will help you understand <span class="No-Break">its functionality:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_175" title2="(no caption)" no2="">from llama_index.core.postprocessor import 
&nbsp;&nbsp;&nbsp;&nbsp;MetadataReplacementPostProcessor
from llama_index.core.schema import TextNode, NodeWithScore
nodes = [
&nbsp;&nbsp;&nbsp;&nbsp;TextNode(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text="Article 1",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata={"summary": "Summary of article 1"}
&nbsp;&nbsp;&nbsp;&nbsp;),
&nbsp;&nbsp;&nbsp;&nbsp;TextNode(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text="Article 2",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata={"summary": "Summary of article 2"}
&nbsp;&nbsp;&nbsp;&nbsp;),
]</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>First, we defined two sample nodes on which we’ll now apply the postprocessor. We’ll instruct it <a id="_idIndexMarker666"></a>to replace the content of each node with the values stored in the <strong class="source-inline">summary</strong> <span class="No-Break">metadata field:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_176" title2="(no caption)" no2="">node_with_score_list = [
&nbsp;&nbsp;&nbsp;&nbsp;NodeWithScore(node=node) for node in nodes
]
pp = MetadataReplacementPostProcessor(
&nbsp;&nbsp;&nbsp;&nbsp;target_metadata_key="summary"
)
processed_nodes = pp.postprocess_nodes(
&nbsp;&nbsp;&nbsp;&nbsp;node_with_score_list
)
for node_with_score in processed_nodes:
&nbsp;&nbsp;&nbsp;&nbsp;print(f"Replaced Text: {node_with_score.node.text}")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>After processing takes place, the output should look <span class="No-Break">like this:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_177" title2="(no caption)" no2="">Replaced Text: Summary of article 1
Replaced Text: Summary of article 2</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Let’s explore the other postprocessing options that <span class="No-Break">LlamaIndex provides.</span></p>
			<h3 id="f_11__idParaDest-167" data-type="sect2" class="sect2" title2="SentenceEmbeddingOptimizer" no2="7.2.8"><a id="_idTextAnchor166"></a>7.2.8. SentenceEmbeddingOptimizer</h3>
			<p><strong class="source-inline">SentenceEmbeddingOptimizer</strong> is built to optimize longer text passages by selecting <a id="_idIndexMarker667"></a>the most relevant sentences given a query based on semantic similarity. It uses advanced NLP techniques to score sentence relevance and discard less <span class="No-Break">useful sentences.</span></p>
			<p class="callout-heading">Why and where should we use it?</p>
			<p class="callout">In a workflow that’s ingesting lengthy documents, retrieving full passages may exceed model context size limits. <strong class="source-inline">SentenceEmbeddingOptimizer</strong> allows us to send only the most <a id="_idIndexMarker668"></a>important sentences to the LLM while preserving enough context. This prevents wasted tokens on irrelevant text by reducing noisy content. Removing irrelevant parts of the content also improves the response time and can greatly reduce the cost associated with the final <span class="No-Break">LLM call.</span></p>
			<p>The postprocessor takes a list of nodes as input and uses embeddings to analyze the semantic similarity of each sentence to the search query. Sentences closest to the query vector are retained while distant, unrelated sentences are <span class="No-Break">stripped away.</span></p>
			<p>This is how we use it <span class="No-Break">in practice:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_178" title2="(no caption)" no2="">from llama_index.core.postprocessor.optimizer import 
&nbsp;&nbsp;&nbsp;&nbsp;SentenceEmbeddingOptimizer
optimizer = SentenceEmbeddingOptimizer(
&nbsp;&nbsp;&nbsp;&nbsp;percentile_cutoff=0.8,
&nbsp;&nbsp;&nbsp;&nbsp;threshold_cutoff=0.7
)
query_engine = index.as_query_engine(
&nbsp;&nbsp;&nbsp;&nbsp;optimizer=optimizer
)
response = query_engine.query("&lt;your_query_here&gt;")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In this example, <strong class="source-inline">SentenceEmbeddingOptimizer</strong> uses a <strong class="source-inline">percentile_cutoff</strong> value of 0.8 and a <strong class="source-inline">threshold_cutoff</strong> value of <strong class="source-inline">0.7</strong> to select sentences. This means it aims to retain the top 80% of sentences by similarity score and further filters to include only those with similarity scores above <strong class="source-inline">0.7</strong>. The main parameters that can be customized <a id="_idIndexMarker669"></a>are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="source-inline">percentile_cutoff</strong>: The percentage of top sentences above the similarity threshold to preserve. This allows us to compact nodes to the most relevant 75% of sentences, <span class="No-Break">for example.</span></li>
				<li><strong class="source-inline">threshold_cutoff</strong>: An absolute similarity score threshold where only sentences with similarity above this value are kept. This is useful for more <span class="No-Break">stringent filtering.</span></li>
				<li><strong class="source-inline">context_before</strong> and <strong class="source-inline">context_after</strong>: These allow us to keep several sentences before and after the matches for <span class="No-Break">more context.</span></li>
			</ul>
			<p>In a similar fashion to <strong class="source-inline">KeywordNodePostprocessor</strong>, the <strong class="source-inline">SentenceEmbeddingOptimizer</strong> postprocessor removes less relevant sentences from nodes. However, in this case, it does so use vector search rather <span class="No-Break">than keywords.</span></p>
			<p>This postprocessor <a id="_idIndexMarker670"></a>is more about refining and shortening the content within each node for better alignment with the query. This allows for optimal information density tailored to the query while accounting for the <span class="No-Break">LLM’s limitations.</span></p>
			<p>In contrast, processors such as <strong class="source-inline">KeywordNodePostprocessor</strong> and <strong class="source-inline">SimilarityPostprocessor</strong> operate at the node level, keeping or removing entire nodes based on keywords or similarity <span class="No-Break">scores, respectively.</span></p>
			<h3 id="f_11__idParaDest-168" data-type="sect2" class="sect2" title2="Time-based postprocessors" no2="7.2.9"><a id="_idTextAnchor167"></a>7.2.9. Time-based postprocessors</h3>
			<p><strong class="bold">Time-based postprocessors</strong> are designed to prioritize recency and provide users with the <a id="_idIndexMarker671"></a>latest, most up-to-date information. They achieve this goal through various techniques, such as sorting nodes by <strong class="source-inline">date</strong> metadata, filtering based on embedding similarity, or applying time-decay <span class="No-Break">scoring models.</span></p>
			<p>Let’s get an overview of <span class="No-Break">these processors.</span></p>
			<h4 data-type="sect3" class="sect3" title2="FixedRecencyPostprocessor" no2="7.2.9.1">7.2.9.1. FixedRecencyPostprocessor</h4>
			<p>This simple postprocessor focuses results on the most recent data by sorting nodes based on their <strong class="source-inline">date</strong> metadata and then returning the <strong class="source-inline">top_k</strong> nodes sorted by date. This ensures <a id="_idIndexMarker672"></a>we get the latest data, which is critical for applications such as environmental monitoring, where having current <a id="_idIndexMarker673"></a>information is vital. For example, when querying about recent air quality metrics, the postprocessor guarantees that only the most up-to-date readings are provided. They focus the results on the <span class="No-Break">latest information.</span></p>
			<p>The two configurable parameters for this processor are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="source-inline">top_k</strong>: The number of top recent nodes <span class="No-Break">to return</span></li>
				<li><strong class="source-inline">date_key</strong>: The metadata key that’s used to identify the date in <span class="No-Break">each node</span></li>
			</ul>
			<h4 data-type="sect3" class="sect3" title2="EmbeddingRecencyPostprocessor" no2="7.2.9.2">7.2.9.2. EmbeddingRecencyPostprocessor</h4>
			<p>This postprocessor further refines recency-sorted results by comparing node contents using embedding similarity and removing those too similar to earlier nodes. Nodes that are too <a id="_idIndexMarker674"></a>similar to earlier ones are filtered out, ensuring that the content is both recent and diverse. The output it <a id="_idIndexMarker675"></a>produces is not just recent but also diverse in terms of the information <span class="No-Break">it contains.</span></p>
			<p><strong class="source-inline">EmbeddingRecencyPostprocessor</strong> sorts the nodes by date using the specified <strong class="source-inline">date_key</strong> metadata field. Then, it generates a query embedding for each node by inserting the node’s content into the <strong class="source-inline">query_embedding_tmpl</strong> template. This query embedding is used to find <span class="No-Break">similar documents.</span></p>
			<p class="callout-heading">Where could that be useful?</p>
			<p class="callout">Let’s think, for example, about a news aggregation service. When users query about a recent event, the system retrieves a set of nodes (news articles, in this case) sorted by date. However, many articles might cover the same event, leading to redundant information. <strong class="source-inline">EmbeddingRecencyPostprocessor</strong> examines these articles and filters out those that are too similar in content to more recent articles. This prevents us from presenting multiple redundant articles about the same event by eliminating those whose content significantly overlaps with more <span class="No-Break">recent coverage.</span></p>
			<p>Its <a id="_idIndexMarker676"></a>configurable parameters are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="source-inline">similarity_cutoff</strong>: The threshold for embedding similarity, above which nodes are considered too similar and <span class="No-Break">filtered out</span></li>
				<li><strong class="source-inline">date_key</strong>: This specifies the metadata key that’s used for sorting nodes <span class="No-Break">by date</span></li>
				<li><strong class="source-inline">query_embedding_tmpl</strong>: This is the template that’s used to generate query embeddings for <span class="No-Break">each node</span></li>
			</ul>
			<h4 data-type="sect3" class="sect3" title2="TimeWeightedPostprocessor" no2="7.2.9.3">7.2.9.3. TimeWeightedPostprocessor</h4>
			<p><strong class="source-inline">TimeWeightedPostprocessor</strong> prioritizes newer results by reranking nodes based on a <strong class="bold">time-decay function</strong> accounting <a id="_idIndexMarker677"></a>for how recently they were accessed. This favors fresh, less repeated content, which is critical for use cases such as trending news aggregation, where users want the latest updates rather than the <span class="No-Break">same information.</span></p>
			<p>The scoring dynamically adapts to changing access patterns over time. <strong class="source-inline">TimeWeightedPostprocessor</strong> is engineered to re-rank nodes based on their recency and <a id="_idIndexMarker678"></a>prior access history, applying a time-weighted scoring system. This postprocessor is particularly effective in scenarios where it’s crucial to <a id="_idIndexMarker679"></a>avoid repeatedly presenting the same information and where the freshness of <span class="No-Break">content matters.</span></p>
			<p>It works by adjusting the score of each node based on the last time it was accessed, applying a decay factor to prioritize less recently accessed content. This dynamic reranking ensures that the output is not just relevant but also timely and varied. This works great for applications where keeping the users updated with the most recent information <span class="No-Break">is essential.</span></p>
			<p>It also <a id="_idIndexMarker680"></a>has several parameters that we <span class="No-Break">can tweak:</span></p>
			<ul>
				<li><strong class="source-inline">time_decay</strong>: The decay factor for the <span class="No-Break">time-weighted scoring</span></li>
				<li><strong class="source-inline">last_accessed_key</strong>: Metadata key for tracking when a node was <span class="No-Break">last accessed</span></li>
				<li><strong class="source-inline">time_access_refresh</strong>: A Boolean to determine if the last accessed time should <span class="No-Break">be updated</span></li>
				<li><strong class="source-inline">now</strong>: An optional <a id="_idIndexMarker681"></a>parameter to set the current time. This is useful <span class="No-Break">for testing</span></li>
				<li><strong class="source-inline">top_k</strong>: The number of top nodes to return after reranking. The default value <span class="No-Break">is 1</span></li>
			</ul>
			<p>With these advanced time-aware postprocessors, our RAG system transforms into a dynamic information curator, adept at navigating the temporal aspects of data. They ensure <a id="_idIndexMarker682"></a>that our system doesn’t just retrieve information <a id="_idIndexMarker683"></a>but smartly selects content that’s not only recent but also varied <span class="No-Break">and relevant.</span></p>
			<p>This makes them indispensable for scenarios where timely and diverse information is crucial, offering us a consistently fresh and <span class="No-Break">rich experience.</span></p>
			<h3 id="f_11__idParaDest-169" data-type="sect2" class="sect2" title2="Re-ranking postprocessors" no2="7.2.10"><a id="_idTextAnchor168"></a>7.2.10. Re-ranking postprocessors</h3>
			<p>Along with the basic processors we’ve discussed so far, LlamaIndex provides several more sophisticated options that make use of LLMs or embedding models for re-ranking nodes. As a <a id="_idIndexMarker684"></a>general principle, they work by re-ordering the nodes based on their relevance to the query, rather than removing them or altering their content. Some of these postprocessors, such as<strong class="source-inline">SentenceTransformerRerank</strong>, also update the relevance scores of the nodes to reflect their similarity to <span class="No-Break">the query.</span></p>
			<p>They all accept a <strong class="source-inline">top_n</strong> parameter, which specifies how many re-ordered nodes they should return. You can explore them in full detail by consulting the official <span class="No-Break">docs: </span><a href="https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/</span></a><span class="No-Break">.</span></p>
			<p>This section provides a quick overview of the available <span class="No-Break">LLM-based processors.</span></p>
			<h4 data-type="sect3" class="sect3" title2="LLMRerank" no2="7.2.10.1">7.2.10.1. LLMRerank</h4>
			<p>This processor re-orders nodes by asking an LLM to assign relevance scores. It selects the <strong class="source-inline">top_n</strong> most relevant nodes from a given set based on the user’s query. The prompt that’s used by this postprocessor can be customized via the <span class="No-Break"><strong class="source-inline">choice_select_prompt</strong></span><span class="No-Break"> parameter.</span></p>
			<p>To increase <a id="_idIndexMarker685"></a>efficiency, it works in batches. The batch <a id="_idIndexMarker686"></a>size can also be customized by using the <strong class="source-inline">choice_batch_size</strong> argument. It requires a <strong class="source-inline">query_bundle</strong> argument for processing and uses the model configured in <strong class="source-inline">llm</strong>. Its reranking process involves formatting node contents into prompts, using the LLM to assess relevance, and then reordering nodes based on their calculated <span class="No-Break">relevance scores.</span></p>
			<h4 data-type="sect3" class="sect3" title2="CohereRerank" no2="7.2.10.2">7.2.10.2. CohereRerank</h4>
			<p>This <a id="_idIndexMarker687"></a>processor re-ranks nodes using Cohere’s neural models (<a href="https://cohere.com/rerank" target="_blank" rel="noopener noreferrer">https://cohere.com/rerank</a>) to sort <a id="_idIndexMarker688"></a>nodes by relevance. The default model that’s used is <em class="italic">rerank-english-v2.0</em>. The <strong class="source-inline">top_n</strong> nodes deemed <a id="_idIndexMarker689"></a>most relevant by the Cohere model are selected <span class="No-Break">and returned.</span></p>
			<p>This processor allows us to leverage powerful relevance algorithms provided by Cohere but requires a Cohere API key and their libraries to be installed in the <span class="No-Break">local environment.</span></p>
			<h4 data-type="sect3" class="sect3" title2="SentenceTransformerRerank" no2="7.2.10.3">7.2.10.3. SentenceTransformerRerank</h4>
			<p><strong class="source-inline">SentenceTransformerRerank</strong> uses sentence transformer models to re-rank nodes based <a id="_idIndexMarker690"></a>on their relevance to a <span class="No-Break">given query.</span></p>
			<p>This process <a id="_idIndexMarker691"></a>involves scoring nodes using a sentence transformer model, with the default being <em class="italic">cross-encoder/stsb-distilroberta-base</em>, and then reordering them based on these scores. It selects the top-ranked nodes to return, up to the specified <strong class="source-inline">top_n</strong> limit. You can find more information <span class="No-Break">here:</span><span class="No-Break"><span class="P---URL"> </span></span><a href="https://www.sbert.net/examples/applications/retrieve_rerank/README.html" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://www.sbert.net/examples/applications/retrieve_rerank/README.html</span></a><span class="No-Break">.</span></p>
			<h4 data-type="sect3" class="sect3" title2="RankGPTRerank" no2="7.2.10.4">7.2.10.4. RankGPTRerank</h4>
			<p>This re-ranking <a id="_idIndexMarker692"></a>postprocessor is designed to <a id="_idIndexMarker693"></a>improve retrieval results relevance using an LLM such as GPT-3.5. It involves a process where the user’s query and content from nodes are formatted into prompts, guiding the language model to rank these nodes based <span class="No-Break">on relevance.</span></p>
			<p>The model’s output is then used to re-order the nodes, ensuring that the most relevant ones appear <a id="_idIndexMarker694"></a>at the top. When the context that’s retrieved is too large for the LLM’s context window, <strong class="source-inline">RankGPTRerank</strong> uses a sliding window approach to gradually re-rank a segment <span class="No-Break">of chunks.</span></p>
			<p>This method <a id="_idIndexMarker695"></a>is based on a paper by Sun et al. (2023), <em class="italic">Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking </em><span class="No-Break"><em class="italic">Agents</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/abs/2304.09542v2" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://arxiv.org/abs/2304.09542v2</span></a><span class="No-Break">).</span></p>
			<h4 data-type="sect3" class="sect3" title2="LongLLMLinguaPostprocessor" no2="7.2.10.5">7.2.10.5. LongLLMLinguaPostprocessor</h4>
			<p>This very useful postprocessor is designed to optimize node texts concerning queries by compressing them. It’s based on a method described in a paper by Jiang et al. (2023), <em class="italic">LLMLingua: Compressing Prompts for Accelerated Inference of Large Language </em><span class="No-Break"><em class="italic">Models</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/abs/2310.05736v2" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://arxiv.org/abs/2310.05736v2</span></a><span class="No-Break">).</span></p>
			<p><strong class="source-inline">LongLLMLinguaPostprocessor</strong> addresses several issues associated with LLMs, such as increased API latency, context window limit overruns, and expensive <span class="No-Break">API costs.</span></p>
			<p>The key idea <a id="_idIndexMarker696"></a>is to intelligently <a id="_idIndexMarker697"></a>compress prompts in a way that they focus on the most relevant information, enabling more efficient and accurate processing by the LLM. It offers a balance between performance and efficiency, demonstrating that prompt compression – with up to 20x achievements – can lead to substantial improvements in model inference and cost-effectiveness without considerable loss <span class="No-Break">in performance.</span></p>
			<p>The processor is designed to work with a local, well-trained language model. This setup allows for the efficient compression of prompts for use with LLMs, supporting the optimization process locally without relying on external <span class="No-Break">API calls.</span></p>
			<p>You can find a complete demo <span class="No-Break">here: </span><a href="https://github.com/microsoft/LLMLingua/blob/main/examples/RAGLlamaIndex.ipynb" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://github.com/microsoft/LLMLingua/blob/main/examples/RAGLlamaIndex.ipynb</span></a><span class="No-Break">.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Measuring the effectiveness of LLM-based re-ranking" no2="7.2.10.6">7.2.10.6. Measuring the effectiveness of LLM-based re-ranking</h4>
			<p>A common source of concern – especially when using LLM-based re-rankers – is the quality of their output. Because LLMs are trained on vast amounts of data, they can sometimes generate results that are biased, inconsistent, or even factually incorrect. This is <a id="_idIndexMarker698"></a>particularly problematic when dealing with specialized domains or sensitive information. To verify that the LLM-based postprocessors are re-ranking the nodes well enough, it is important to properly evaluate their performance. Here are a few approaches you can use to gauge the quality of the <span class="No-Break">re-ranking step:</span></p>
			<ul>
				<li><strong class="bold">Manual relevance assessment</strong>: Manually examine the re-ranked results to check if the most relevant nodes are indeed appearing at the top. This qualitative evaluation depends on human judgment to determine if the re-ranking matches the query’s intent. While not exactly very scientific, this simple approach may suffice for simple use cases, experiments, or non-production <span class="No-Break">RAG applications.</span></li>
				<li><strong class="bold">Benchmark datasets</strong>: Evaluate the re-ranking performance on standard <strong class="bold">information retrieval</strong> (<strong class="bold">IR</strong>) benchmarks <a id="_idIndexMarker699"></a>that have pre-defined queries and relevance judgments. This process can be time-consuming and it may require a well-prepared evaluation dataset but it will save you from troubles later in the RAG workflow. By comparing the re-ranked results against the ground truth, you can calculate metrics such as precision, recall, and others to quantify the re-ranking quality. We’ll cover the evaluation process in more detail in <a href="#_idTextAnchor197"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Customizing and Deploying Our </em><span class="No-Break"><em class="italic">LlamaIndex Project.</em></span></li>
				<li><strong class="bold">User feedback</strong>: In real-world applications, collect user feedback on the re-ranked search results. User satisfaction scores, click-through rates, or other engagement metrics can indicate if the re-ranking enhances the user experience and provides more relevant results. There’s an inherent advantage to this method. Because it relies on human feedback directly collected in the live environment, it becomes <a id="_idIndexMarker700"></a>a form of <strong class="bold">continuous evaluation</strong>. This makes it useful in <a id="_idIndexMarker701"></a>detecting any potential <strong class="bold">model drift</strong>, thus enabling timely adjustments to our pipeline to help us avoid quality degradation <span class="No-Break">over time.</span></li>
				<li><strong class="bold">A/B testing</strong>: Another form of gathering user feedback would be by running controlled experiments where some users are shown the original ranking, while others see the LLM-based re-ranked results. Compare the performance metrics between the two groups to assess if the re-ranking leads to <span class="No-Break">improved outcomes.</span></li>
				<li><strong class="bold">Domain expert evaluation</strong>: For specialized domains, ask subject matter experts to review the re-ranked results and provide feedback on their relevance and quality. While <a id="_idIndexMarker702"></a>more expensive and difficult than the other options, this method could be the best solution when dealing with highly technical or niche topics that require a deep understanding of the <span class="No-Break">subject matter.</span></li>
			</ul>
			<p>The evaluation method you choose will depend on your specific use case, available resources, and the level of rigor you need. Using a mix of qualitative and quantitative approaches can give you a thorough assessment of the LLM’s <span class="No-Break">re-ranking performance.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Understanding the model drift phenomenon" no2="7.2.10.7">7.2.10.7. Understanding the model drift phenomenon</h4>
			<p>While not necessarily specific to re-ranking, model drift can significantly impact the quality of <a id="_idIndexMarker703"></a>our RAG pipelines and it’s an important factor to consider. Our models are static representations of the snapshot datasets that are used for their training. But in time, that data changes. For example, new concepts may emerge that were not included in the training data, or the data itself may shift in distribution. This phenomenon is known as <em class="italic">model drift</em>, and it can manifest in <span class="No-Break">multiple forms:</span></p>
			<ul>
				<li><strong class="bold">Data drift</strong>: This occurs when the statistical properties or distribution of the input data <a id="_idIndexMarker704"></a>change over time. For instance, if a model <a id="_idIndexMarker705"></a>was trained on a dataset of customer reviews from a specific period, it may not perform as well on newer reviews that contain different language patterns, sentiments, <span class="No-Break">or topics.</span></li>
				<li><strong class="bold">Concept drift</strong>: This happens <a id="_idIndexMarker706"></a>when the relationships <a id="_idIndexMarker707"></a>between the input features and the target variable evolve. In a RAG system designed to assist with medical queries, the introduction of new diseases, treatments, or medical terminology can lead to concept drift. The model’s understanding of the domain becomes outdated, and its performance <span class="No-Break">may degrade.</span></li>
				<li><strong class="bold">Upstream data changes</strong>: This type of drift happens when the data used to train the <a id="_idIndexMarker708"></a>model is different from the data used in production. For example, if a RAG system is trained on a curated dataset but then applied to raw, unprocessed data in production, the model’s performance may suffer due to differences in data quality, format, <span class="No-Break">or distribution.</span></li>
				<li><strong class="bold">Feedback loops</strong>: In some cases, the outputs of a model can influence its future inputs, creating a feedback loop. For instance, if a RAG system is used to recommend <a id="_idIndexMarker709"></a>articles to users, and those recommendations are then used to update the retrieval component, the model may become biased toward its previous outputs, leading to a narrowing of the information it provides <span class="No-Break">over time.</span></li>
				<li><strong class="bold">Domain shift</strong>: This occurs when a model is applied to a different domain or context than <a id="_idIndexMarker710"></a>it was originally trained for. In a RAG workflow, if the retrieval component is trained on data from one domain (for example, legal documents) but then used to answer queries in another domain (for example, medical questions), the model’s performance may suffer due to differences in language, terminology, or <span class="No-Break">underlying concepts.</span></li>
				<li><strong class="bold">Temporal drift</strong>: This type of drift is related to the passage of time and can encompass <a id="_idIndexMarker711"></a>both data drift and concept drift. As time passes, the data and concepts relevant to a particular task may evolve, leading to a gradual decline in model performance if <span class="No-Break">not addressed.</span></li>
			</ul>
			<p>To mitigate these various types of model drift, it’s important to continuously monitor the performance of a RAG system, regularly update its retrieval component with new data, and adapt it to changes in the underlying data distribution, concepts, or domain. Additionally, implementing feedback loops carefully and ensuring that the training data is representative of the production environment can help minimize the impact of upstream data changes and feedback-related drift. This helps ensure that our RAG system remains accurate, up-to-date, and aligned with the evolving needs of <span class="No-Break">the users.</span></p>
			<h3 id="f_11__idParaDest-170" data-type="sect2" class="sect2" title2="Final thoughts about node postprocessors" no2="7.2.11"><a id="_idTextAnchor169"></a>7.2.11. Final thoughts about node postprocessors</h3>
			<p>If the existing <a id="_idIndexMarker712"></a>ones are not exactly fit for our particular use case, we have the option to build our own. <strong class="bold">Custom postprocessors</strong> can be built by extending <strong class="source-inline">BaseNodePostprocessor</strong>. You can find a complete example <span class="No-Break">here: </span><a href="https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/root.html#custom-node-postprocessor" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/root.html#custom-node-postprocessor</span></a><span class="No-Break">.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">In more complex scenarios, postprocessors can also be chained to apply multiple transformations to the nodes before they’re passed to the <span class="No-Break">response synthesizer.</span></p>
			<p>The key is <a id="_idIndexMarker713"></a>applying the right processors to remove noise, improve relevance signal, inject diversity, and handle sensitive content – leading to higher quality and more reliable <span class="No-Break">generated responses.</span></p>
			<p>For now, let’s shift our focus to the final piece of our puzzle: <span class="No-Break"><strong class="bold">response synthesizers</strong></span><span class="No-Break">.</span></p>
			<h2 id="f_11__idParaDest-171" data-type="sect1" class="sect1" title2="Understanding response synthesizers" no2="7.3"><a id="_idTextAnchor170"></a>7.3. Understanding response synthesizers</h2>
			<p>The final step before sending our hard-worked contextual data to the LLM is the response synthesizer. It’s the component that’s responsible for generating responses from a language <a id="_idIndexMarker714"></a>model using a user query and the <span class="No-Break">retrieved context.</span></p>
			<p>It simplifies the process of querying an LLM and synthesizing an answer across our proprietary data. Just like the other components of the framework, response synthesizers can be used on their own or configured in query engines to handle the final step of response generation after nodes have been retrieved <span class="No-Break">and postprocessed.</span></p>
			<p>Here’s a simple example demonstrating how to use one directly on a given set <span class="No-Break">of nodes:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_179" title2="(no caption)" no2="">from llama_index.core.schema import TextNode, NodeWithScore
from llama_index.core import get_response_synthesizer
nodes = [
&nbsp;&nbsp;&nbsp;&nbsp;TextNode(text=
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"The town square clock was built in 1895"
&nbsp;&nbsp;&nbsp;&nbsp;),
&nbsp;&nbsp;&nbsp;&nbsp;TextNode(text=
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"A turquoise parrot lives in the Amazon"
&nbsp;&nbsp;&nbsp;&nbsp;),
&nbsp;&nbsp;&nbsp;&nbsp;TextNode(text=
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"A rare orchid blooms only at midnight"
&nbsp;&nbsp;&nbsp;&nbsp;),
]
node_with_score_list = [NodeWithScore(node=node) for node in nodes]</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The first part <a id="_idIndexMarker715"></a>of the code, we’ve defined some arbitrary nodes. That’s going to be our <em class="italic">proprietary</em> context. Next, we’ll use a response synthesizer to run an LLM query based on <span class="No-Break">our context:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_180" title2="(no caption)" no2="">synth = get_response_synthesizer(
&nbsp;&nbsp;&nbsp;&nbsp;response_mode="refine",
&nbsp;&nbsp;&nbsp;&nbsp;use_async=False,
&nbsp;&nbsp;&nbsp;&nbsp;streaming=False,
)
response = synth.synthesize(
&nbsp;&nbsp;&nbsp;&nbsp;"When was the clock built?",
&nbsp;&nbsp;&nbsp;&nbsp;nodes=node_with_score_list
)
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The output is <span class="No-Break">as follows:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_181" title2="(no caption)" no2="">The clock was built in 1895.</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Curious to take a peek under the hood? What happened in the background here? OK, bear with me for the next few lines – once you understand this example, you’ll know exactly how a response synthesizer works. Let me show you a <span class="No-Break">diagram first:</span></p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/B21861_07_2.jpg" alt="Figure 7.2 – The refine response synthesizer" width="1650" height="759" data-type="figure" id="untitled_figure_49" title2="– The refine response synthesizer" no2="7.2">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – The refine response synthesizer</p>
			<p>Here’s a <a id="_idIndexMarker716"></a>description of <span class="No-Break">the process:</span></p>
			<ol>
				<li>The synthesizer begins by building a special-purpose prompt, starting with the first node in the list as context. This prompt includes the query, specific instructions, and the context – which in this case is our first node. It uses a default value but can be customized via the <span class="No-Break"><strong class="source-inline">text_qa_template</strong></span><span class="No-Break"> parameter:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_182" title2="(no caption)" no2="">System: "You are an expert Q&amp;A system that is trusted around the world. Always answer the query using the provided context information, and not prior knowledge. Some rules to follow: 1. Never directly reference the given context in your answer. 2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."
User: "Context information is below. The town square clock was built in 1895. Given the context information and not prior knowledge, answer the query. Query: When was the clock built? Answer: "</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>				<li> The next step is to send this prompt to the LLM and wait for <span class="No-Break">an answer.</span></li>
				<li>After the initial answer comes back, it builds the prompt for the next node while also integrating the first answer in the prompt and refining the final answer using a prompt that can be customized <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">refine_template</strong></span><span class="No-Break">.</span></li>
				<li>It then repeats this iterative process for all nodes while constantly refining the <span class="No-Break">final answer.</span></li>
				<li> Once the nodes are exhausted, it returns the <em class="italic">refined</em> <span class="No-Break">final answer.</span></li>
			</ol>
			<p>In this case, the <a id="_idIndexMarker717"></a>behavior of the synthesizer is dictated <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">response_mode="refine"</strong></span><span class="No-Break">.</span></p>
			<p>However, <strong class="source-inline">refine</strong> mode is just one of the several predefined synthesizers in LlamaIndex. Synthesizer mode can be specified using the <strong class="source-inline">response_mode</strong> parameter. Here’s a list of the available <a id="_idIndexMarker718"></a><span class="No-Break">response modes:</span></p>
			<ul>
				<li><strong class="source-inline">refine</strong>: As we saw in the previous example, <strong class="source-inline">refine</strong> queries each node individually using <strong class="source-inline">text_qa_template</strong> and <strong class="source-inline">refine_template prompts</strong> to iteratively construct a detailed response. This mode is ideal for constructing detailed responses, ensuring that each piece of information is carefully considered. We can also set <strong class="source-inline">Verbose</strong> to <strong class="source-inline">True</strong> for more visibility on the inner workings of this synthesizer and use <strong class="source-inline">output_cls</strong> to specify a <strong class="source-inline">pydantic</strong> object to use as a <span class="No-Break">response template.</span></li>
				<li><strong class="source-inline">compact</strong>: This one is similar to <strong class="source-inline">refine</strong> but it concatenates nodes to reduce the number of required LLM queries, balancing detail, <span class="No-Break">and efficiency.</span></li>
				<li><strong class="source-inline">tree_summarize</strong>: This mode uses recursive summarization, processing each node with <strong class="source-inline">summary_template</strong>. It recursively summarizes and queries nodes, concatenating them in each iteration until a single final response remains. It’s very useful for summarization and best suited for creating comprehensive summaries from multiple pieces <span class="No-Break">of information.</span></li>
				<li><strong class="source-inline">simple_summarize</strong>: This mode truncates nodes to fit in one LLM query for basic summarization. It’s great for brief overviews as it’s quick and cheap, but it may omit <span class="No-Break">finer details.</span></li>
				<li><strong class="source-inline">accumulate</strong>: This mode applies the query to each node individually and accumulates <a id="_idIndexMarker719"></a>the responses. It’s best suited for analyzing or comparing responses from <span class="No-Break">multiple sources.</span></li>
				<li><strong class="source-inline">no_text</strong>: In this operating mode, the response synthesizer fetches nodes without querying the LLM. This is mainly useful for debugging, analyzing raw data, or inspecting the retrieval or <span class="No-Break">postprocessing outputs.</span></li>
				<li><strong class="source-inline">compact_accumulate</strong>: A blend of compact and accumulate, this mode compacts prompts, similar to <strong class="source-inline">compact</strong> mode, and applies the query across nodes. This is especially suitable for efficiently processing <span class="No-Break">multiple sources.</span></li>
			</ul>
			<p>In addition to <a id="_idIndexMarker720"></a>these predefined modes, custom response synthesizers can be created by subclassing <strong class="source-inline">BaseSynthesizer</strong> and implementing the <strong class="source-inline">get_response</strong> method. You can find a complete example in the official documentation: <a href="https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html#custom-response-synthesizers" target="_blank" rel="noopener noreferrer">https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html#custom-response-synthesizers</a>. This provides you with the flexibility to design specialized response <span class="No-Break">generation approaches.</span></p>
			<p>Features such as <strong class="source-inline">structured_answer_filtering</strong> can also be enabled on the <em class="italic">refine</em> and <em class="italic">compact</em> synthesizers. It uses the LLM to filter out retrieved nodes that are irrelevant to the question, improving <span class="No-Break">response quality.</span></p>
			<p>Prompt templates such as <strong class="source-inline">text_qa_template</strong> and <strong class="source-inline">refine_template</strong> allow us to customize the prompts that are used at different stages of response synthesis. Additional variables can also be passed to influence <span class="No-Break">response generation.</span></p>
			<p>Overall, response synthesizers handle the critical task of querying nodes and producing a final response, providing options to balance performance, customizability, <span class="No-Break">and accuracy.</span></p>
			<p>But guess what? We’re not out of the <span class="No-Break">woods yet.</span></p>
			<p>Let’s talk about another challenge in <span class="No-Break">our path.</span></p>
			<h2 id="f_11__idParaDest-172" data-type="sect1" class="sect1" title2="Implementing output parsing techniques" no2="7.4"><a id="_idTextAnchor171"></a>7.4. Implementing output parsing techniques</h2>
			<p>Our next topic addresses a common problem that’s encountered in RAG applications that rely on <a id="_idIndexMarker721"></a>structured outputs produced by an LLM. When those outputs are to become inputs in the next processing steps of the application, their structure becomes <span class="No-Break">very important.</span></p>
			<p class="callout-heading">A bit of background</p>
			<p class="callout">Due to their non-deterministic nature, LLMs have the bad habit of sometimes producing responses in a format other than the requested one, adding unsolicited comments or descriptions – just like humans if you think about it. Simply relying on clever prompting techniques may not be enough to completely avoid <span class="No-Break">this behavior.</span></p>
			<p>Even models specifically trained to follow precise instructions occasionally deviate from the structure we’ve requested. In cases where that output is simply returned to the user, this doesn’t matter much – it might even create a more <span class="No-Break">natural experience.</span></p>
			<p>The problems arise when the structure of the response matters – for example, when we are going to store that output in a set of variables and then send it to further processing. Have a look at <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.3</em> for a <span class="No-Break">better understanding:</span></p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/B21861_07_3.jpg" alt="Figure 7.3 – LLMs may produce unpredictable outputs" width="1189" height="724" data-type="figure" id="untitled_figure_50" title2="– LLMs may produce unpredictable outputs" no2="7.3">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – LLMs may produce unpredictable outputs</p>
			<p>So, how can we make sure that we receive a structured and predictable output from an LLM? As usual, LlamaIndex comes to our rescue – this time in the form of the <strong class="bold">output parsers</strong> and <strong class="bold">Pydantic programs</strong>. Here’s an <a id="_idIndexMarker722"></a>overview of the methods that are used to ensure a <span class="No-Break">structured output.</span></p>
			<h3 id="f_11__idParaDest-173" data-type="sect2" class="sect2" title2="Extracting structured outputs using output parsers" no2="7.4.1"><a id="_idTextAnchor172"></a>7.4.1. Extracting structured outputs using output parsers</h3>
			<p>Output parsers <a id="_idIndexMarker723"></a>are essential for managing the unpredictability of LLM <a id="_idIndexMarker724"></a>responses. They ensure that <a id="_idIndexMarker725"></a>outputs from LLMs are structured and formatted correctly for subsequent steps in an application. These parsers come in various forms, each with a unique approach to handling and refining <span class="No-Break">the output.</span></p>
			<h4 data-type="sect3" class="sect3" title2="GuardrailsOutputParser" no2="7.4.1.1">7.4.1.1. GuardrailsOutputParser</h4>
			<p>This particular <a id="_idIndexMarker726"></a>one is based on the <strong class="bold">Guardrails</strong> library provided by Guardrails AI: <a href="https://www.guardrailsai.com/" target="_blank" rel="noopener noreferrer">https://www.guardrailsai.com/</a>. Guardrails ensures the outputs from LLMs adhere to specified structures and types. This is particularly useful in RAG applications, where outputs need to be consistent and structured for <span class="No-Break">further processing.</span></p>
			<p>Guardrails does <a id="_idIndexMarker727"></a>this by validating the LLM outputs against a <a id="_idIndexMarker728"></a>defined format and can take corrective actions such as re-asking the LLM if the outputs don’t meet the specified standards. This feature is essential for maintaining the integrity and usability of LLM outputs in <span class="No-Break">automated processes.</span></p>
			<p class="callout-heading">Under the hood</p>
			<p class="callout">At the core of how Guardrails works, we find the notion of <strong class="bold">rails</strong>. In the Guardrails library, a rail serves as a specification tool for LLM outputs. It is used to enforce specific structures, types, and validation criteria on these outputs. Rails can be defined using either <a id="_idIndexMarker729"></a>the <strong class="bold">Reliable AI Markup Language</strong> (<strong class="bold">RAIL</strong>) for structured outputs or directly in Python <span class="No-Break">Pydantic structures.</span></p>
			<p>The purpose of a rail is to ensure that the LLM outputs adhere to predefined quality and format standards, which includes setting validators and corrective actions if the output deviates from <span class="No-Break">these standards.</span></p>
			<p>This parser operates based on the <span class="No-Break">following logic:</span></p>
			<ol>
				<li>First, it takes the initial prompt and an output format specification <span class="No-Break">as input.</span></li>
				<li>Based on the output format specification, it re-formats the prompt, adapting it for the <span class="No-Break">target LLM.</span></li>
				<li>It can also verify the output received from the LLM. If the specification is not validated, it can regenerate the output until the structure <span class="No-Break">is valid.</span></li>
			</ol>
			<p>This parser <a id="_idIndexMarker730"></a>can be configured with the <span class="No-Break">following parameters:</span></p>
			<ul>
				<li><strong class="source-inline">guard</strong>: An instance of the <strong class="source-inline">Guard</strong> class from the Guardrails library. This class encapsulates the core functionality of the Guardrails system. It is responsible for enforcing the specifications defined in a <span class="No-Break">RAIL structure</span></li>
				<li><strong class="source-inline">llm</strong>: This parameter is optional and is used to select the language model that’s used in conjunction with the <span class="No-Break">Guardrails parser</span></li>
				<li><strong class="source-inline">format_key</strong>: This optional parameter is useful when you want to inject specific formatting instructions into the query based on the output <span class="No-Break">format required</span></li>
			</ul>
			<p>You can find a complete example of using this method <span class="No-Break">here: </span><a href="https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/output_parser.html#guardrails" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/output_parser.html#guardrails</span></a><span class="No-Break">.</span></p>
			<p>Once you’ve <a id="_idIndexMarker731"></a>familiarized yourself with the RAIL language, the Guardrails library becomes an easy-to-use parsing solution for <span class="No-Break">your apps.</span></p>
			<p>Just make sure you install the Guardrails library in your environment first by running the <span class="No-Break">following command:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_183" title2="(no caption)" no2="">pip install guardrails-ai</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In case you’re wondering how you could build an output parser and implement any custom guard rail logic in it, you can find a complete example <span class="No-Break">here: </span><a href="https://docs.llamaindex.ai/en/latest/examples/output_parsing/llm_program/#define-a-custom-output-parser" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/latest/examples/output_parsing/llm_program/#define-a-custom-output-parser</span></a><span class="No-Break">.</span></p>
			<h4 data-type="sect3" class="sect3" title2="LangchainOutputParser" no2="7.4.1.2">7.4.1.2. LangchainOutputParser</h4>
			<p>Apart from <strong class="source-inline">GuardrailsOutputParser</strong>, LlamaIndex also supports the output parsers provided <span class="No-Break">by Langchain.</span></p>
			<p>Instead of <a id="_idIndexMarker732"></a>using the more complex RAIL language to define validation criteria and corrective actions, <strong class="source-inline">LangchainOutputParser</strong> relies on a simpler <a id="_idIndexMarker733"></a>concept called a <span class="No-Break"><strong class="bold">response schema</strong></span><span class="No-Break">.</span></p>
			<p>Response <a id="_idIndexMarker734"></a>schemas in Langchain are primarily used for structuring the output and focus on defining specific fields that the output should contain. These schemas guide the Langchain system to ensure that the output matches the <span class="No-Break">expected format.</span></p>
			<p>This approach is less about enforcing stringent validation rules or corrective actions and more about organizing the output data in a coherent and <span class="No-Break">predictable structure.</span></p>
			<p>Here’s an example that implements a very simple quotation system based on <span class="No-Break">this method:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_184" title2="(no caption)" no2="">from langchain.output_parsers import (
&nbsp;&nbsp;&nbsp;&nbsp;StructuredOutputParser, ResponseSchema)
from llama_index.core.output_parsers import LangchainOutputParser
from llama_index.llms.openai import OpenAI
from llama_index.core.schema import TextNode
from llama_index.core import VectorStoreIndex
from pydantic import BaseModel
from typing import List
nodes = [
&nbsp;&nbsp;&nbsp;&nbsp;TextNode(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text="Roses have vibrant colors and smell nice."),
&nbsp;&nbsp;&nbsp;&nbsp;TextNode(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text="Oak trees are tall and have green leaves."),
]</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In the first part of our code, we took care of the necessary imports and then defined some random <em class="italic">proprietary data</em> contained in two nodes. Next, we must define the response <a id="_idIndexMarker735"></a>schemas that will be used to structure <a id="_idIndexMarker736"></a>the <span class="No-Break">LLM’s output:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_185" title2="(no caption)" no2="">schemas = [
&nbsp;&nbsp;&nbsp;&nbsp;ResponseSchema(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;name="answer",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;description=(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"answer to the user's question"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;),
&nbsp;&nbsp;&nbsp;&nbsp;ResponseSchema(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;name="source",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;description=(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"the source text used to answer the user's question, "
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"should be a quote from the original prompt."
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;)
]</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>As you can see, the schema defines the expected output structure. Now, we can define the Langchain parser and an OpenAI <strong class="source-inline">llm</strong> object that’s been configured to <span class="No-Break">use it:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_186" title2="(no caption)" no2="">lc_parser = StructuredOutputParser.from_response_schemas(schemas)
output_parser = LangchainOutputParser(lc_parser)
llm = OpenAI(output_parser=output_parser)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Now, it’s time to build an index and <strong class="source-inline">QueryEngine</strong> from our Nodes. <strong class="source-inline">QueryEngine</strong> will be configured to use the Langchain parser so that it can structure <span class="No-Break">the output:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_187" title2="(no caption)" no2="">index = VectorStoreIndex(nodes=nodes)
query_engine = index.as_query_engine(llm=llm)
response = query_engine.query(
&nbsp;&nbsp;&nbsp;&nbsp;"Are oak trees small? yes or no",
)
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The output is <span class="No-Break">as follows:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_188" title2="(no caption)" no2="">{'answer': 'no', 'source': 'Oak trees are tall and have green leaves.'}</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Neat, <span class="No-Break">isn’t it?</span></p>
			<p>Note that <a id="_idIndexMarker737"></a>citations are useful in a RAG system as they increase <a id="_idIndexMarker738"></a>transparency and allow the answers to be validated against our <span class="No-Break">proprietary data.</span></p>
			<p>The Langchain <a id="_idIndexMarker739"></a>parser has two <span class="No-Break">configurable parameters:</span></p>
			<ul>
				<li><strong class="source-inline">output_parser</strong>: This parameter accepts an instance of a Langchain output parser (<strong class="source-inline">LCOutputParser</strong>). This is where the primary logic for parsing and structuring the output is defined. As seen in the previous example, the parser provided here determines how the output from the LLM is processed <span class="No-Break">and formatted</span></li>
				<li><strong class="source-inline">format_key</strong>: This is an optional parameter that, if provided, is used to insert additional format instructions into the query. This can be particularly useful when the query needs to be formatted with specific instructions that guide the output generation of the <span class="No-Break">language model</span></li>
			</ul>
			<p>While both <strong class="source-inline">GuardrailsOutputParser</strong> and <strong class="source-inline">LangchainOutputParser</strong> aim to structure and validate LLM outputs, their specific mechanisms and extent of control over the output format vary. The Langchain parser is more focused on processing the LLM output, while the Guardrails parser has a more proactive role in shaping the query and output format. We’ll talk about the other <span class="No-Break">method next.</span></p>
			<h3 id="f_11__idParaDest-174" data-type="sect2" class="sect2" title2="Extracting structured outputs using Pydantic programs" no2="7.4.2"><a id="_idTextAnchor173"></a>7.4.2. Extracting structured outputs using Pydantic programs</h3>
			<p>Pydantic programs represent another way to generate structured outputs. Pydantic programs <a id="_idIndexMarker740"></a>are a form of abstraction in LLM workflows that convert input strings into structured pydantic object types. They <a id="_idIndexMarker741"></a>can either call functions or rely on text completions, along with <span class="No-Break">output parsers.</span></p>
			<p>They are highly versatile and can be used for various applications, being both composable and adaptable for general or specific use cases. There are multiple programs available for various <span class="No-Break">use cases.</span></p>
			<p>You can find an overview and working examples <span class="No-Break">here: </span><a href="https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/pydantic_program.html" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/pydantic_program.html</span></a><span class="No-Break">.</span></p>
			<p>You’ll learn how to use a Pydantic program – in this case, <strong class="source-inline">OpenAIPydanticProgram</strong>, later in this chapter, when we continue working on our PITS <span class="No-Break">tutoring app.</span></p>
			<h2 id="f_11__idParaDest-175" data-type="sect1" class="sect1" title2="Building and using query engines" no2="7.5"><a id="_idTextAnchor174"></a>7.5. Building and using query engines</h2>
			<p>Our puzzle is now complete. Throughout the previous chapters, we’ve gradually learned about the key <a id="_idIndexMarker742"></a>ingredients in a RAG setup. Now, it’s time to bring everything together: the nodes, indexes, retrievers, postprocessors, response synthesizers, and <span class="No-Break">output parsers.</span></p>
			<p>In this chapter, we’ll focus on blending these elements into a complex construct: the query engine. We’ll learn about how query engines work and the neat tricks they have up <span class="No-Break">their sleeves.</span></p>
			<h3 id="f_11__idParaDest-176" data-type="sect2" class="sect2" title2="Exploring different methods of building query engines" no2="7.5.1"><a id="_idTextAnchor175"></a>7.5.1. Exploring different methods of building query engines</h3>
			<p>At its core, <strong class="source-inline">QueryEngine</strong> is an <a id="_idIndexMarker743"></a>interface that processes natural language queries to generate rich responses. It often relies on one or more indexes through retrievers and can also be combined with other query engines for <span class="No-Break">enhanced capabilities.</span></p>
			<p>The easiest <a id="_idIndexMarker744"></a>way to define <strong class="source-inline">QueryEngine</strong> is using the <strong class="bold">high-level API</strong> provided by LlamaIndex, <span class="No-Break">like this:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_189" title2="(no caption)" no2="">query_engine = index.as_query_engine()</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>With just a single line of code, we’ve built a simple query engine from an existing index. Although fast, this method uses <strong class="source-inline">RetrieverQueryEngine</strong> under the hood with the default settings and does not provide many opportunities <span class="No-Break">for customization.</span></p>
			<p>If we want <a id="_idIndexMarker745"></a>to have complete control over its parameters and full customization options, we can use the <strong class="bold">low-level API</strong> to explicitly build the <span class="No-Break">query engine.</span></p>
			<p>Let’s have <a id="_idIndexMarker746"></a>a look at <span class="No-Break">an example:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_190" title2="(no caption)" no2="">from llama_index.core.retrievers import SummaryIndexEmbeddingRetriever
from llama_index.core.postprocessor import SimilarityPostprocessor
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core import (
&nbsp;&nbsp;&nbsp;&nbsp;SummaryIndex, SimpleDirectoryReader, get_response_synthesizer)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>As usual, we start by handling the imports. Next, we ingest our demo files and build a <span class="No-Break">simple </span><span class="No-Break"><strong class="source-inline">SummaryIndex</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_191" title2="(no caption)" no2="">documents = SimpleDirectoryReader("files").load_data()
index = SummaryIndex.from_documents(documents)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Then, we throw in a retriever, a response synthesizer, and a node postprocessor. Building a query engine with this low-level API approach allows us to fully customize <span class="No-Break">each component:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_192" title2="(no caption)" no2="">retriever = SummaryIndexEmbeddingRetriever(
&nbsp;&nbsp;&nbsp;&nbsp;index=index,
&nbsp;&nbsp;&nbsp;&nbsp;similarity_top_k=3,
)
response_synthesizer = get_response_synthesizer(
&nbsp;&nbsp;&nbsp;&nbsp;response_mode="tree_summarize",
&nbsp;&nbsp;&nbsp;&nbsp;verbose=True
)
pp = SimilarityPostprocessor(similarity_cutoff=0.7)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Now, it’s time <a id="_idIndexMarker747"></a>to bring them all together and assemble <span class="No-Break">our </span><span class="No-Break"><strong class="source-inline">QueryEngine</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_193" title2="(no caption)" no2="">query_engine = RetrieverQueryEngine(
&nbsp;&nbsp;&nbsp;&nbsp;retriever=retriever,
&nbsp;&nbsp;&nbsp;&nbsp;response_synthesizer=response_synthesizer,
&nbsp;&nbsp;&nbsp;&nbsp;node_postprocessors=[pp]
)
response = query_engine.query(
&nbsp;&nbsp;&nbsp;&nbsp;"Enumerate iconic buildings in ancient Rome"
)
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The output should look similar to <span class="No-Break">the following:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_194" title2="(no caption)" no2="">The iconic buildings in ancient Rome included the Colosseum and the Pantheon.</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Now that we’ve built a simple query engine, let’s take a look at some more <span class="No-Break">advanced scenarios.</span></p>
			<h3 id="f_11__idParaDest-177" data-type="sect2" class="sect2" title2="Advanced uses of the QueryEngine interface" no2="7.5.2"><a id="_idTextAnchor176"></a>7.5.2. Advanced uses of the QueryEngine interface</h3>
			<p>The LlamaIndex <a id="_idIndexMarker748"></a>community has gradually developed – and continues to develop – various advanced query methods while using <strong class="source-inline">QueryEngine</strong> as a <span class="No-Break">main component.</span></p>
			<p>Apart from the query engines that I’m already covering in this book, <em class="italic">Table 7.1</em> provides an overview <a id="_idIndexMarker749"></a>of other available engines at the time <span class="No-Break">of writing:</span></p>
			<table id="table001-2" class="No-Table-Style _idGenTablePara-1" data-type="table" title2="– Different query engine modules available in LlamaIndex" no2="7.1"><colgroup><col><col></colgroup><thead><tr class="No-Table-Style"><th class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">QueryEngine Class</strong></span></p>
						</th><th class="No-Table-Style">
							<p><strong class="bold">Short Description and </strong><span class="No-Break"><strong class="bold">Use Cases</strong></span></p>
						</th></tr></thead><tbody><tr class="No-Table-Style"><th class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline" lang="en-US" xml:lang="en-US">CitationQueryEngine</strong></span></p>
						</th><th class="No-Table-Style">
							<p>Designed for situations requiring citations from multiple sources to support answers. It is especially useful in academic research, legal analysis, or any context where validated, source-based information is important. When generating responses, this query engine incorporates and cites relevant sources, ensuring answers are not only accurate but also verifiably supported by <span class="No-Break">documented evidence.</span></p>
						</th></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">CogniswitchQueryEngine</strong></span></p>
						</td><td class="No-Table-Style">
							<p>Integrates with the Cogniswitch service (<a href="https://www.cogniswitch.ai/" target="_blank" rel="noopener noreferrer">https://www.cogniswitch.ai/</a>) to answer queries using a combination of Cogniswitch’s knowledge processing capabilities and <span class="No-Break">OpenAI’s models.</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">ComposableGraphQueryEngine</strong></span></p>
						</td><td class="No-Table-Style">
							<p>Designed to operate within a composable graph structure, enabling flexible, modular querying across different data sources and indices. It is ideal for complex data ecosystems where different types of information <span class="No-Break">are interconnected.</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">QASummaryQueryEngineBuilder</strong></span></p>
						</td><td class="No-Table-Style">
							<p>Combines <strong class="source-inline">SummaryIndex</strong> and <strong class="source-inline">VectorStoreIndex</strong>. This is useful both to retrieve specific information from documents and to get concise summaries <span class="No-Break">of content.</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">TransformQueryEngine</strong></span></p>
						</td><td class="No-Table-Style">
							<p>Designed to preprocess queries using a specific transformation before they are submitted to an underlying query engine. When queries vary greatly in format or clarity, applying a transformation to normalize or enhance them can greatly <span class="No-Break">improve retrieval.</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">MultiStepQueryEngine</strong></span></p>
						</td><td class="No-Table-Style">
							<p>Works by decomposing complex queries into simpler, sequential steps. It can be useful for handling complex or multi-faceted questions that require a series of <span class="No-Break">logical steps.</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">ToolRetrieverRouterQueryEngine</strong></span></p>
						</td><td class="No-Table-Style">
							<p>Can dynamically choose from multiple candidate query engines based on the query’s context. It uses the most appropriate query engine tool for each <span class="No-Break">specific query.</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">SQLJoinQueryEngine</strong></span></p>
						</td><td class="No-Table-Style">
							<p>Designed for cases that require a combination of SQL database queries and additional information retrieval or processing. This is especially useful when the SQL query results need to be augmented or refined using <span class="No-Break">further queries.</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">SQLAutoVectorQueryEngine</strong></span></p>
						</td><td class="No-Table-Style">
							<p>Integrates SQL database queries with vector-based retrieval, enabling a two-step process where a query can be executed against a SQL database. Based on those results, further information can be fetched from a <span class="No-Break">vector store.</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">RetryQueryEngine</strong></span></p>
						</td><td class="No-Table-Style">
							<p>When the initial response to a query does not meet certain evaluation criteria, it automatically retries the query if it <span class="No-Break">fails evaluation.</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">RetrySourceQueryEngine</strong></span></p>
						</td><td class="No-Table-Style">
							<p>Designed to perform retries on a query with different source nodes based on evaluation criteria. If the initial response from the query engine does not pass the evaluator’s criteria, it attempts to find alternative source nodes that may yield a <span class="No-Break">better response.</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">RetryGuidelineQueryEngine</strong></span></p>
						</td><td class="No-Table-Style">
							<p>Similar to <strong class="source-inline">RetryQueryEngine</strong>, this one also transforms the query on each retry, based on feedback from the <span class="No-Break">evaluation process.</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">PandasQueryEngine</strong></span></p>
						</td><td class="No-Table-Style">
							<p>Converts natural language queries into executable pandas Python code, allowing for data manipulation and analysis over <span class="No-Break">pandas DataFrames.</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">JSONalyzeQueryEngine</strong></span></p>
						</td><td class="No-Table-Style">
							<p>Designed to analyze JSON list-shaped data by converting natural language queries into SQL queries that are executed within an in-memory <span class="No-Break">SQLite database.</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">KnowledgeGraphQueryEngine</strong></span></p>
						</td><td class="No-Table-Style">
							<p>Generates and processes queries for knowledge graphs, translating natural language queries into graph-specific queries and synthesizing responses based on graph query results. This is useful for applications requiring interaction with <span class="No-Break">knowledge graphs.</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">FLAREInstructQueryEngine</strong></span></p>
						</td><td class="No-Table-Style">
							<p>Implementing the <strong class="bold">Forward-Looking Active REtrieval</strong> (<strong class="bold">FLARE</strong>) method, this query engine allows the model to continually access and incorporate external knowledge as it generates content. This is particularly useful for generating long, knowledge-intensive texts. By actively predicting future content needs and retrieving information accordingly, FLARE aims to reduce hallucinations and improve the factual accuracy of generated responses. It’s based on a paper by <span lang="en-US" xml:lang="en-US">Jiang et al. (2023), </span><em class="italic">Active Retrieval Augmented </em><span class="No-Break"><em class="italic">Generation</em></span><span class="No-Break" lang="en-US" xml:lang="en-US"> (</span><span class="No-Break">https://arxiv.org/abs/2305.06983v2</span><span class="No-Break">).</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">SimpleMultiModalQueryEngine</strong></span></p>
						</td><td class="No-Table-Style">
							<p>A multi-modal query engine that can process queries involving both text and images, assuming that the retrieved text and images can fit within the LLM’s context window. It retrieves relevant text and images based on the query and then synthesizes a response using a <span class="No-Break">multi-modal LLM.</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">SQLTableRetrieverQueryEngine</strong></span></p>
						</td><td class="No-Table-Style">
							<p>Converts natural language queries into SQL queries but also synthesizes responses from the query results, making the responses more understandable and relevant to the user’s natural <span class="No-Break">language query.</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">PGVectorSQLQueryEngine</strong></span></p>
						</td><td class="No-Table-Style">
							<p>Designed to work with PGvector (https://github.com/pgvector/pgvector), an extension for PostgreSQL that allows vectors to be stored and embedded directly within <span class="No-Break">the database.</span></p>
						</td></tr></tbody></table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 7.1 – Different query engine modules available in LlamaIndex</p>
			<p>The list of advanced implementations has already become so long that it could probably be the subject <a id="_idIndexMarker750"></a>of a separate book. Consequently, I did not set out to give a detailed presentation of each method. Instead, I encourage you to consult the official project documentation on the subject and discover how these building blocks can be used in various <span class="No-Break">scenarios: </span><span class="No-Break">https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/modules.html</span><span class="No-Break">.</span></p>
			<p>There, you will find detailed explanations, use cases for each module, and, most importantly, code examples with which you can understand the operation and implementation of <span class="No-Break">each method.</span></p>
			<p>However, we cannot end this chapter without introducing you to at least a few essential modules in a RAG scenario. So, that’s what we are going to <span class="No-Break">cover next.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Implementing advanced routing with RouterQueryEngine" no2="7.5.2.1">7.5.2.1. Implementing advanced routing with RouterQueryEngine</h4>
			<p>Remember <a id="_idIndexMarker751"></a>when we talked <a id="_idIndexMarker752"></a>about routing retrievers in <a href="#_idTextAnchor131"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Querying Our Data, Part 1 – Context Retrieval</em>? It’s time to see a more advanced routing mechanism, this time implemented at the query <span class="No-Break">engine level.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.4</em> summarizes the operation <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">RouterQueryEngine</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/B21861_07_4.jpg" alt="Figure 7.4 – How RouterQueryEngine works" width="1650" height="482" data-type="figure" id="untitled_figure_51" title2="– How RouterQueryEngine works" no2="7.4">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – How RouterQueryEngine works</p>
			<p><strong class="source-inline">RouterQueryEngine</strong> is capable of choosing between different tools it has available. Depending <a id="_idIndexMarker753"></a>on the user <a id="_idIndexMarker754"></a>query, the router will decide which <strong class="source-inline">QueryEngineTool</strong> should be used to generate <span class="No-Break">an answer.</span></p>
			<p>Just like in the case of retrievers, we can use <strong class="source-inline">PydanticMultiSelector</strong> or <strong class="source-inline">PydanticSingleSelector</strong> to configure its behavior. The multi-selector combines multiple options and can handle a broader spectrum of <span class="No-Break">user queries.</span></p>
			<p class="callout-heading">Potential use case</p>
			<p class="callout">Imagine a real-life scenario where an organization has its knowledge split into multiple individual documents. Such a router would allow for general queries over the entire knowledge base, while still enabling and precisely pinpointing the source data used to generate <span class="No-Break">the answer.</span></p>
			<p>In the following example, we’re building a <strong class="source-inline">RouterQueryEngine</strong> engine that operates different query engine tools – each one built over a different document. Here’s <span class="No-Break">the code:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_195" title2="(no caption)" no2="">from llama_index.core.tools import QueryEngineTool
from llama_index.core.query_engine import RouterQueryEngine
from llama_index.core.selectors import PydanticMultiSelector
from llama_index.core import SummaryIndex, SimpleDirectoryReader
from llama_index.core.extractors import TitleExtractor
documents = SimpleDirectoryReader("files").load_data()</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The first part of the code handles the imports and ingests our sample data. As before, we are <a id="_idIndexMarker755"></a>using two simple <a id="_idIndexMarker756"></a>text files: one containing information about ancient Rome and another containing a generic text about dogs. In the next part, we’ll go through each document and use <strong class="source-inline">TitleExtractor</strong> to extract a title and store it as a <span class="No-Break"><strong class="source-inline">metadata</strong></span><span class="No-Break"> field:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_196" title2="(no caption)" no2="">title_extractor = TitleExtractor()
for doc in documents:
&nbsp;&nbsp;&nbsp;&nbsp;title_metadata = title_extractor.extract([doc])
&nbsp;&nbsp;&nbsp;&nbsp;doc.metadata.update(title_metadata[0])</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Once the files have been ingested and we have generated document titles, we can define <strong class="source-inline">SummaryIndex</strong>, <strong class="source-inline">QueryEngine</strong>, and <strong class="source-inline">QueryEngineTool</strong> for each of the documents. We use the document title to provide the selector with a description of <span class="No-Break">each tool:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_197" title2="(no caption)" no2="">indexes = []
query_engines = []
tools = []
for doc in documents:
&nbsp;&nbsp;&nbsp;&nbsp;document_title = doc.metadata['document_title']
&nbsp;&nbsp;&nbsp;&nbsp;index = SummaryIndex.from_documents([doc])
&nbsp;&nbsp;&nbsp;&nbsp;query_engine = index.as_query_engine(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;response_mode="tree_summarize",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;use_async=True,
&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;tool = QueryEngineTool.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;query_engine=query_engine,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;description=f"Contains data about {document_title}",
&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;indexes.append(index)
&nbsp;&nbsp;&nbsp;&nbsp;query_engines.append(query_engine)
&nbsp;&nbsp;&nbsp;&nbsp;tools.append(tool)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Now <a id="_idIndexMarker757"></a>that we have a list <a id="_idIndexMarker758"></a>of available tools, we can build our <strong class="source-inline">RouterQueryEngine</strong> based <span class="No-Break">on </span><span class="No-Break"><strong class="source-inline">PydanticMultiSelector</strong></span><span class="No-Break">.</span></p>
			<p>To do this, we must pass the query engine tools as an argument. These will be the options that are available for <span class="No-Break">the selector:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_198" title2="(no caption)" no2="">qe = RouterQueryEngine(
&nbsp;&nbsp;&nbsp;&nbsp;selector=PydanticMultiSelector.from_defaults(),
&nbsp;&nbsp;&nbsp;&nbsp;query_engine_tools=tools
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Depending on the query, the selector will decide which tools to use to gather responses. After each tool has responded, the query engine will synthesize and return a <span class="No-Break">final response:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_199" title2="(no caption)" no2="">response = qe.query(
&nbsp;&nbsp;&nbsp;&nbsp;"Tell me about Rome and dogs"
)
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>For relatively small documents, this method will probably work just fine. So long as the text is short enough to be properly summarized into a title, this query engine will handle most user queries pretty well. In a real-life scenario, though, it’s highly unlikely that we could <a id="_idIndexMarker759"></a>fully summarize <a id="_idIndexMarker760"></a>the whole content in a title. In that case, using a document summary instead of the title would <span class="No-Break">be preferable.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Querying multiple documents with SubQuestionQueryEngine" no2="7.5.2.2">7.5.2.2. Querying multiple documents with SubQuestionQueryEngine</h4>
			<p>In a real-life scenario involving multiple data sources, as in the previous example, users may <a id="_idIndexMarker761"></a>come up with more complex queries – for example, they may ask for comparisons between different subjects <a id="_idIndexMarker762"></a>documented in different files. For this kind of situation, we can use <strong class="source-inline">SubQuestionQueryEngine</strong>. It is designed to handle complex queries by breaking them down into <span class="No-Break">smaller sub-questions.</span></p>
			<p>Each sub-question is processed by its designated query engine and the individual responses are then combined. A response synthesizer is used to compile these into a coherent final response, effectively managing queries that require a multi-faceted approach. <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.5</em> describes <span class="No-Break">its operation:</span></p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B21861_07_5.jpg" alt="Figure 7.5 – How SubQuestionQueryEngine works" width="1650" height="676" data-type="figure" id="untitled_figure_52" title2="– How SubQuestionQueryEngine works" no2="7.5">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – How SubQuestionQueryEngine works</p>
			<p>Let’s have a look at the code. The first part is very similar to our previous example <span class="No-Break">regarding </span><span class="No-Break"><strong class="source-inline">RouterQueryEngine</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_200" title2="(no caption)" no2="">from llama_index.core.tools import QueryEngineTool
from llama_index.core.query_engine import RouterQueryEngine
from llama_index.core.query_engine import SubQuestionQueryEngine
from llama_index.core.selectors import PydanticMultiSelector
from llama_index.core.extractors import TitleExtractor
from llama_index.core import SummaryIndex, SimpleDirectoryReader</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>After <a id="_idIndexMarker763"></a>importing the necessary <a id="_idIndexMarker764"></a>modules, we load the files and extract <span class="No-Break">their titles:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_201" title2="(no caption)" no2="">documents = SimpleDirectoryReader("files/sample").load_data()
title_extractor = TitleExtractor()
for doc in documents:
&nbsp;&nbsp;&nbsp;&nbsp;title_metadata = title_extractor.extract([doc])
&nbsp;&nbsp;&nbsp;&nbsp;doc.metadata.update(title_metadata[0])
indexes = []
query_engines = []
tools = []</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>So far, we have completed the same steps that we did for <strong class="source-inline">RouterQueryEngine</strong>. One notable change in the next part is that we also extract <strong class="source-inline">file_name</strong> from the metadata and use it as a name for the corresponding tool. This way, we’ll be able to tell exactly where each answer is <span class="No-Break">coming from:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_202" title2="(no caption)" no2="">for doc in documents:
&nbsp;&nbsp;&nbsp;&nbsp;document_title = doc.metadata['document_title']
&nbsp;&nbsp;&nbsp;&nbsp;file_name = doc.metadata['file_name']
&nbsp;&nbsp;&nbsp;&nbsp;index = SummaryIndex.from_documents([doc])
&nbsp;&nbsp;&nbsp;&nbsp;query_engine = index.as_query_engine(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;response_mode="tree_summarize",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;use_async=True,
&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;tool = QueryEngineTool.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;query_engine=query_engine,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;name=file_name,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;description=f"Contains data about {document_title}",
&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;indexes.append(index)
&nbsp;&nbsp;&nbsp;&nbsp;query_engines.append(query_engine)
&nbsp;&nbsp;&nbsp;&nbsp;tools.append(tool)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Next, let’s <a id="_idIndexMarker765"></a>build <span class="No-Break">our </span><span class="No-Break"><strong class="source-inline">SubQuestionQueryEngine</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_203" title2="(no caption)" no2="">qe = SubQuestionQueryEngine.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;query_engine_tools=tools,
&nbsp;&nbsp;&nbsp;&nbsp;use_async=True
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>At this <a id="_idIndexMarker766"></a>point, we’re ready to generate <span class="No-Break">the output:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_204" title2="(no caption)" no2="">response = qe.query(
&nbsp;&nbsp;&nbsp;&nbsp;"Compare buildings from ancient Athens and ancient Rome"
)
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Along with the final response, we’ll be able to see each sub-question generated and its corresponding query engine tool name. In our case, the tool name will correspond to the filename of each <span class="No-Break">source text.</span></p>
			<p><strong class="source-inline">SubQuestionQueryEngine</strong> is particularly useful for complex queries that cannot be <a id="_idIndexMarker767"></a>addressed directly <a id="_idIndexMarker768"></a>in a single step. It produces great results in cases such as <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Comparative analysis</strong>: For queries that require comparing and contrasting different subjects, the engine can divide the query into smaller, focused sub-questions to gather detailed information about each subject before synthesizing a comparative response. Here’s a sample question: <em class="italic">Compare and contrast the economic policies of Country A and Country B in the </em><span class="No-Break"><em class="italic">last decade.</em></span></li>
				<li><strong class="bold">Multi-faceted questions</strong>: In cases where a query involves multiple aspects or criteria, this engine can break down the query into individual components, handle each separately, and then combine the results for a comprehensive answer. That means questions such as <em class="italic">What are the environmental, economic, and social impacts of deforestation in the </em><span class="No-Break"><em class="italic">Amazon rainforest?</em></span></li>
				<li><strong class="bold">Complex research tasks</strong>: For research-oriented queries that require information to be gathered from various sources or perspectives, this engine can efficiently handle the task by segmenting it into more manageable sub-questions. Here’s the type of query it could answer: <em class="italic">Investigate the historical development of renewable energy technologies and their adoption across </em><span class="No-Break"><em class="italic">different continents.</em></span></li>
			</ul>
			<p>Now that you’ve got a general understanding of how query engines work, I’ll let you explore the different possibilities and experiment with all the existing query <span class="No-Break">engine modules.</span></p>
			<p>In case you’re wondering whether you can create custom ones, that option is <span class="No-Break">also available.</span></p>
			<p>You can find an example <span class="No-Break">here: </span><span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/query_engine/custom_query_engine.html#option-1-ragqueryengine</span><span class="No-Break">.</span></p>
			<p>Now that we’ve got some fresh knowledge, it’s about time we built some new components into our <span class="No-Break">tutoring project.</span></p>
			<h2 id="f_11__idParaDest-178" data-type="sect1" class="sect1" title2="Hands-on – building quizzes in PITS" no2="7.6"><a id="_idTextAnchor177"></a>7.6. Hands-on – building quizzes in PITS</h2>
			<p>One of the features we are building in our PITS project is the ability to generate quizzes based on the learning material uploaded by <span class="No-Break">the user.</span></p>
			<p>These quizzes <a id="_idIndexMarker769"></a>will initially be used to gauge the overall knowledge of the user on the topic. Based on that assessment, the training slides and narration will be adjusted to the level of <span class="No-Break">the learner.</span></p>
			<p>The same mechanism can also be used to generate intermediate quizzes at the end of each section to test the user’s current knowledge. Let’s see how we can easily implement the quiz <span class="No-Break">builder feature.</span></p>
			<p>We’ll be using one of the LlamaIndex pre-packaged pydantic programs: the DataFrame Pydantic extractor. This is designed to extract tabular DataFrames from <span class="No-Break">raw text.</span></p>
			<p>Let’s have a look at the code <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">quiz_builder.py</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_205" title2="(no caption)" no2="">from llama_index.core import load_index_from_storage, StorageContext
from llama_index.program.evaporate.df import DFRowsProgram
from llama_index.program.openai import OpenAIPydanticProgram
from global_settings import INDEX_STORAGE, QUIZ_SIZE, QUIZ_FILE
import pandas as pd</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>First, we imported all the necessary modules, including our global variables defined <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">global_settings.py</strong></span><span class="No-Break">:</span></p>
			<ul>
				<li><strong class="source-inline">INDEX_STORAGE</strong>: The index’s <span class="No-Break">storage location</span></li>
				<li><strong class="source-inline">QUIZ_SIZE</strong>: The number of questions to be included in <span class="No-Break">a quiz</span></li>
				<li><strong class="source-inline">QUIZ_FILE</strong>: The path where the quiz will be saved as <span class="No-Break">a CSV</span></li>
			</ul>
			<p>We’re also importing the <strong class="source-inline">load_index_from_storage</strong> function, which we will use to fetch our indexes from storage to avoid the cost and time of <span class="No-Break">rebuilding them.</span></p>
			<p>Because we’re using DataFrames, we’ll also need to import the pandas library. If you don’t have it already installed in your environment, make sure you run <span class="No-Break">this first:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_206" title2="(no caption)" no2="">pip install pandas</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>OK – let’s build <a id="_idIndexMarker770"></a>our main function. The <strong class="source-inline">build_quiz</strong> function will be responsible for generating the quiz and saving the questions in a <strong class="source-inline">CSV</strong> file for <span class="No-Break">further use:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_207" title2="(no caption)" no2="">def build_quiz(topic):
&nbsp;&nbsp;&nbsp;&nbsp;df = pd.DataFrame({
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Question_no": pd.Series(dtype="int"),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Question_text": pd.Series(dtype="str"),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Option1": pd.Series(dtype="str"),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Option2": pd.Series(dtype="str"),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Option3": pd.Series(dtype="str"),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Option4": pd.Series(dtype="str"),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Correct_answer": pd.Series(dtype="str"),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Rationale": pd.Series(dtype="str"),
&nbsp;&nbsp;&nbsp;&nbsp;})</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<ol>
				<li>First, we set up a DataFrame to structure the quiz questions and their associated options and answers. This DataFrame will serve as the foundation for our quiz. It includes columns for the question number, question text, four answer options, the correct answer, and a rationale for the answer. The use of a pandas DataFrame will make handling and manipulating the quiz data <span class="No-Break">much easier.</span></li>
				<li>Next, we need to load our vector index from storage. To do this, we must define a <strong class="source-inline">StorageContext</strong> object while using the <strong class="source-inline">INDEX_STORAGE</strong> folder as <span class="No-Break">a parameter:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_208" title2="(no caption)" no2="">&nbsp;&nbsp;&nbsp;&nbsp;storage_context = StorageContext.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persist_dir=INDEX_STORAGE
&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;vector_index = load_index_from_storage(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;storage_context, index_id="vector"
&nbsp;&nbsp;&nbsp;&nbsp;)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>				<li>Here, we used <strong class="source-inline">index_id</strong> to identify the <em class="italic">vector</em> index because there’s also a <strong class="source-inline">TreeIndex</strong> index in that storage that we won’t be using for now. It’s time to initialize our <span class="No-Break"><strong class="source-inline">DataFrame</strong></span><span class="No-Break"> extractor:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_209" title2="(no caption)" no2="">&nbsp;&nbsp;&nbsp;&nbsp;df_rows_program = DFRowsProgram.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pydantic_program_cls=OpenAIPydanticProgram,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;df=df
&nbsp;&nbsp;&nbsp;&nbsp;)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>				<li>Now, we can <a id="_idIndexMarker771"></a>define our query engine and craft a prompt that will generate the <span class="No-Break">quiz questions:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_210" title2="(no caption)" no2="">&nbsp;&nbsp;&nbsp;&nbsp;query_engine = vector_index.as_query_engine()
&nbsp;&nbsp;&nbsp;&nbsp;quiz_query = (
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f"Create {QUIZ_SIZE} different quiz "
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"questions relevant for testing "
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"a candidate's knowledge about "
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f"{topic}. Each question will have 4 "
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"answer options. Questions must be "
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"general topic-related, not specific "
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"to the provided text. For each "
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"question, provide also the correct "
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"answer and the answer rationale. "
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"The rationale must not make any "
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"reference to the provided context, "
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"any exams or the topic name. Only "
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"one answer option should be correct."
&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;response = query_engine.query(quiz_query)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>				<li>Next, the prompt <a id="_idIndexMarker772"></a>is passed to the query engine, and the response is then processed by <strong class="source-inline">DFRowsProgram</strong> to convert it into a structured <span class="No-Break">DataFrame format:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_211" title2="(no caption)" no2="">&nbsp;&nbsp;&nbsp;&nbsp;result_obj = df_rows_program(input_str=response)
&nbsp;&nbsp;&nbsp;&nbsp;new_df = result_obj.to_df(existing_df=df)
&nbsp;&nbsp;&nbsp;&nbsp;new_df.to_csv(QUIZ_FILE, index=False)
&nbsp;&nbsp;&nbsp;&nbsp;return new_df</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>				<li>Finally, the new DataFrame containing the quiz questions is saved as a CSV file in the path defined by <strong class="source-inline">QUIZ_FILE</strong>. The function returns the new DataFrame for <span class="No-Break">further use.</span></li>
			</ol>
			<p>This serves as a simple demonstration of how to leverage a combination of LlamaIndex features, Pydantic programs, and DataFrame manipulation to create a dynamic quiz generator. We’ll continue working on the rest of the features in <span class="No-Break">future chapters.</span></p>
			<h2 id="f_11__idParaDest-179" data-type="sect1" class="sect1" title2="Summary" no2="7.7"><a id="_idTextAnchor178"></a>7.7. Summary</h2>
			<p>This chapter explored how to refine search results with various postprocessors, generate responses using different synthesizers, and ensure structured outputs with <span class="No-Break">specific parsers.</span></p>
			<p>We also explored how to construct query engines while integrating the various components that we discussed in the <span class="No-Break">previous chapters.</span></p>
			<p>This chapter also covered handling diverse data sources with <strong class="source-inline">RouterQueryEngine</strong> and decomposing complex queries with <strong class="source-inline">SubQuestionQueryEngine</strong>, and also demonstrated quiz creation in our <span class="No-Break">tutoring app.</span></p>
			<p>See you in the next chapter, where we’ll talk about chatbots, agents, and conversation tracking <span class="No-Break">with LlamaIndex.</span></p>
		</div>
<div id="f_12__idContainer094" data-type="chapter" class="chapter" file="B21861_08_xhtml" title2="Building Chatbots and Agents with LlamaIndex" no2="8">
			<h1 id="f_12__idParaDest-180" class="chapter-number"><a id="_idTextAnchor179"></a>8</h1>
			<h1 class="H1---Chapter" id="f_12__idParaDest-181"><a id="_idTextAnchor180"></a>Building Chatbots and Agents with LlamaIndex</h1>
<p style="font-style: italic;">As this ebook edition doesn't have fixed pagination, the page numbers below are hyperlinked for reference only, based on the printed edition of this book.</p>
			<p>This chapter provides an in-depth look at implementing chatbots and intelligent agents using the capabilities of LlamaIndex. We will explore the various chat engine modes available, from simple chatbots to more advanced context-aware and question-<strong class="bold">condensing engines</strong>. Then, we’ll dive into <strong class="bold">agent architectures</strong>, analyzing tools, <strong class="bold">reasoning loops</strong>, and parallel execution methods. You will gain practical knowledge so that you can build conversational interfaces powered by LLMs that can understand user needs and orchestrate responses or actions by utilizing tools and <span class="No-Break">data sources.</span></p>
			<p>Throughout this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Understanding chatbots <span class="No-Break">and agents</span></li>
				<li>Implementing agentic strategies in <span class="No-Break">our apps</span></li>
				<li>Hands-on – implementing conversation tracking <span class="No-Break">for PITS</span></li>
			</ul>
			<h2 id="f_12__idParaDest-182" data-type="sect1" class="sect1" title2="Technical requirements" no2="8.1"><a id="_idTextAnchor181"></a>8.1. Technical requirements</h2>
			<p>The following LlamaIndex integration packages will be required for the <span class="No-Break">sample code:</span></p>
			<ul>
				<li><em class="italic">Database </em><span class="No-Break"><em class="italic">Tool</em></span><span class="No-Break">: </span><a href="https://pypi.org/project/llama-index-tools-database/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pypi.org/project/llama-index-tools-database/</span></a></li>
				<li><em class="italic">OpenAI </em><span class="No-Break"><em class="italic">Agent</em></span><span class="No-Break">: </span><a href="https://pypi.org/project/llama-index-agent-openai/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pypi.org/project/llama-index-agent-openai/</span></a></li>
				<li><em class="italic">Wikipedia </em><span class="No-Break"><em class="italic">Reader</em></span><span class="No-Break">: </span><a href="https://pypi.org/search/?q=llama-index-readers-wikipedia" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pypi.org/search/?q=llama-index-readers-wikipedia</span></a></li>
				<li><em class="italic">LLM Compiler </em><span class="No-Break"><em class="italic">Agent</em></span><span class="No-Break">: </span><a href="https://pypi.org/project/llama-index-packs-agents-llm-compiler/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pypi.org/project/llama-index-packs-agents-llm-compiler/</span></a></li>
			</ul>
			<p>All the code samples in this chapter can be found in the <strong class="source-inline">ch8</strong> subfolder of this book’s GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex</span></a><span class="No-Break">.</span></p>
			<h2 id="f_12__idParaDest-183" data-type="sect1" class="sect1" title2="Understanding chatbots and agents" no2="8.2"><a id="_idTextAnchor182"></a>8.2. Understanding chatbots and agents</h2>
			<p>In the<a id="_idIndexMarker773"></a> modern business ecosystem, the role of <strong class="bold">chatbot systems</strong> is increasingly important. First appearing in the 1960s (<a href="https://en.wikipedia.org/wiki/ELIZA" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/ELIZA</a>), chatbots have always fascinated both developers and technology users alike. <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.1</em> shows the user interface of one of these <span class="No-Break">early systems:</span></p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/B21861_08_1.jpg" alt="Figure 8.1 – The ELIZA chatbot interface" width="750" height="334" data-type="figure" id="untitled_figure_53" title2="– The ELIZA chatbot interface" no2="8.1">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – The ELIZA chatbot interface</p>
			<p>While these <a id="_idIndexMarker774"></a>systems were rudimentary initially and seen as more of an experiment, with the advancement of NLP technologies, the experience they offer has become increasingly interesting and valuable <span class="No-Break">to users.</span></p>
			<p><strong class="bold">Chatbot-based support systems</strong> offer today’s consumers a self-service experience. For users, self-service support services have<a id="_idIndexMarker775"></a> two major advantages over <span class="No-Break">human support:</span></p>
			<ul>
				<li>They are available 24/7, even outside normal <span class="No-Break">working hours</span></li>
				<li>The user does not have to <em class="italic">hold the line</em> to <span class="No-Break">access them</span></li>
			</ul>
			<p>Even if there is some reluctance to use these systems at first, once they discover these advantages, users soon get used to interacting <span class="No-Break">with them.</span></p>
			<p>Don’t necessarily think of chatbots as a technology designed to replace human support and interaction entirely. Although they have made enormous progress in recent years, these technologies, while getting more and more advanced, still have <span class="No-Break">their shortcomings.</span></p>
			<p>Lacking real empathy and the human touch, even under ideal operating conditions, chatbot-based services are unlikely to replace human support completely. But that doesn’t mean they aren’t extremely valuable, both for organizations and <span class="No-Break">their users.</span></p>
			<p>Perhaps the greatest value they bring is when they work in a blended experience, where users can receive both human support and access to self-service platforms that are interfaced with chatbot technologies. Implemented strategically, these systems can vastly improve not only the support offered to end consumers but also the internal interactions between an <span class="No-Break">organization’s employees.</span></p>
			<p><strong class="bold">ChatOps</strong>, for example, is<a id="_idIndexMarker776"></a> a model<a id="_idIndexMarker777"></a> increasingly used by modern <span class="No-Break">organizations (</span><a href="https://www.ibm.com/blog/benefits-of-chatops/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://www.ibm.com/blog/benefits-of-chatops/</span></a><span class="No-Break">).</span></p>
			<p class="callout-heading">Definition</p>
			<p class="callout">ChatOps refers to the ability to integrate chat platforms with operational workflows, facilitating transparent collaboration among team members, processes, tools, and automated bots to enhance service dependability, accelerate recovery, and boost <span class="No-Break">collaborative productivity.</span></p>
			<p>Based on the<a id="_idIndexMarker778"></a> idea of <strong class="bold">conversation-driven collaboration</strong>, the ChatOps model combines <strong class="bold">DevOps</strong> principles (<a href="https://en.wikipedia.org/wiki/DevOps" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/DevOps</a>) by<a id="_idIndexMarker779"></a> simplifying and accelerating interactions between team members <span class="No-Break">using chatbots.</span></p>
			<p>Whether we use them for internal communication or in interactions with our users, chatbots can only be useful to the extent that they can solve real problems. This depends on how well they can understand the context of the interaction and how relevant the answers they <span class="No-Break">provide are.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.2</em> provides<a id="_idIndexMarker780"></a> a visual representation of the <span class="No-Break">ChatOps model:</span></p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/B21861_08_2.jpg" alt="Figure 8.2 – The ChatOps paradigm" width="1650" height="725" data-type="figure" id="untitled_figure_54" title2="– The ChatOps paradigm" no2="8.2">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – The ChatOps paradigm</p>
			<p>If, in the beginning, the main limitation of chatbots came from the <em class="italic">clumsy</em> way of interacting with the user, with the evolution of NLP technologies, the main shortcoming has become, more recently, the lack of integration with the organization’s <span class="No-Break">knowledge base.</span></p>
			<p>After all, what good is a natural communication experience if the answers given by the system aren’t useful in solving the <span class="No-Break">user’s requests?</span></p>
			<p>This brings us <span class="No-Break">to RAG.</span></p>
			<p>By now, I think it has become obvious that without being connected to an organization’s knowledge base, a chatbot can, at best, be considered a technology experiment. Even conversational engines based on powerful LLMs such as GPT-4 can, at best, provide generic answers that don’t always address the specific problems of each organization. Perhaps worse, not being anchored in validated documentation, they can <em class="italic">hallucinate</em> very convincingly, creating unpleasant or even potentially <span class="No-Break">dangerous experiences.</span></p>
			<p>As you’ve probably guessed by now, LlamaIndex<a id="_idIndexMarker781"></a> also offers RAG tools for implementing chatbot technologies. In this chapter, we will explore the options available to us and understand how we can implement very simple systems to advanced <span class="No-Break">chatbot mechanisms.</span></p>
			<p>But first, let’s see how this functionality is built <span class="No-Break">into LlamaIndex.</span></p>
			<h3 id="f_12__idParaDest-184" data-type="sect2" class="sect2" title2="Discovering ChatEngine" no2="8.2.1"><a id="_idTextAnchor183"></a>8.2.1. Discovering ChatEngine</h3>
			<p>In the previous chapters, we<a id="_idIndexMarker782"></a> saw how we can build a query engine to run queries based on our data. This mechanism allows us to integrate multiple types of indexes, retrievers, node postprocessors, and response synthesizers at the same time, thus being able to access our proprietary data in multiple ways. Unfortunately, the <strong class="source-inline">QueryEngine</strong> class <a id="_idIndexMarker783"></a>does not provide any mechanism to keep the history of a conversation. That means each query is a separate interaction and there is no contextual memory to allow a <span class="No-Break">true </span><span class="No-Break"><em class="italic">conversation</em></span><span class="No-Break">.</span></p>
			<p>For that purpose, however, we have <strong class="bold">ChatEngine</strong>. Unlike query engines, <strong class="source-inline">ChatEngine</strong> allows us to have an actual conversation, giving us both the context of our proprietary data and the history of the chat. To simplify this concept even further, imagine a <strong class="source-inline">QueryEngine</strong> class <span class="No-Break">with memory.</span></p>
			<p>In its simplest form, a chat engine can be initialized just as easily, based on <span class="No-Break">an index:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_212" title2="(no caption)" no2="">chat_engine = index.as_chat_engine()
response = chat_engine.chat("Hi, how are you?")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Once initialized, a chat engine can be queried using <span class="No-Break">various methods:</span></p>
			<ul>
				<li><strong class="source-inline">chat()</strong>: This method initiates a synchronous chat session, processing the user’s message and returning the <span class="No-Break">response immediately.</span></li>
				<li><strong class="source-inline">achat()</strong>: This method is similar to <strong class="source-inline">chat()</strong> but executes the query asynchronously, allowing multiple requests to be processed simultaneously. This can be useful, for example, in a web or mobile application where we want to avoid blocking the main thread during <span class="No-Break">server queries.</span></li>
				<li><strong class="source-inline">stream_chat()</strong>: This method opens a streaming chat session, where responses can be returned as they are generated, for more dynamic interaction. This is particularly useful for long or complex responses that require significant processing time, allowing the user to start seeing parts of the response before all processing <span class="No-Break">is complete.</span></li>
				<li><strong class="source-inline">astream_chat()</strong>: This method is an asynchronous version of <strong class="source-inline">stream_chat()</strong> that allows us to handle streaming interactions in an <span class="No-Break">asynchronous context.</span></li>
			</ul>
			<p>Another option is to<a id="_idIndexMarker784"></a> initiate a <strong class="bold">Read-Eval-Print</strong> (<strong class="bold">REPL</strong>) loop <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">ChatEngine</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_213" title2="(no caption)" no2="">chat_engine.chat_repl()</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>A REPL chat is akin to a <a id="_idIndexMarker785"></a>ChatGPT interface, where a user sends a message or question, the LLM processes the input, generates a response, and then immediately displays it to the user. This loop continues for as long as the user keeps providing input, creating an <span class="No-Break">interactive conversation.</span></p>
			<p>To reset a chat conversation, you can use the <span class="No-Break">following command:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_214" title2="(no caption)" no2="">chat_engine.reset()</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This is useful when you want to clear the history and begin a new <span class="No-Break">conversation thread.</span></p>
			<p>So, the basics are<a id="_idIndexMarker786"></a> very straightforward. Next, let’s talk about the different <strong class="bold">built-in chat modes</strong> available <span class="No-Break">in LlamaIndex.</span></p>
			<h3 id="f_12__idParaDest-185" data-type="sect2" class="sect2" title2="Understanding the different chat modes" no2="8.2.2"><a id="_idTextAnchor184"></a>8.2.2. Understanding the different chat modes</h3>
			<p>When <a id="_idIndexMarker787"></a>initializing a chat engine, we can use the <strong class="source-inline">chat_mode</strong> argument to invoke various chat engine types predefined in LlamaIndex. I will show you how each of these engines works. We will discuss them one by one and get a good understanding of the advantages and use cases best suited for each <span class="No-Break">of them.</span></p>
			<p>But first, let’s have a short introduction to how chat memory is managed <span class="No-Break">within LlamaIndex.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Understanding how chat memory works" no2="8.2.2.1">8.2.2.1. Understanding how chat memory works</h4>
			<p>The <strong class="source-inline">ChatMemoryBuffer</strong> class is a<a id="_idIndexMarker788"></a> specialized memory buffer that’s designed to store chat history efficiently while also managing the token limit imposed by different LLMs. This structure is important because we can pass it as an argument when initializing chat engines using the <strong class="source-inline">memory</strong> parameter. By saving and restoring this buffer from one session to another, we can implement persistence for <span class="No-Break">our conversations.</span></p>
			<p>There are two different storage options for the <span class="No-Break">chat store:</span></p>
			<ul>
				<li>The default <strong class="source-inline">SimpleChatStore</strong>, which<a id="_idIndexMarker789"></a> stores the conversation <span class="No-Break">in memory</span></li>
				<li>The more<a id="_idIndexMarker790"></a> advanced <strong class="source-inline">RedisChatStore</strong>, which stores the chat history in a Redis database, eliminating the need to manually persist and load the <span class="No-Break">chat history</span></li>
			</ul>
			<p>The <strong class="source-inline">chat_store</strong> attribute, which<a id="_idIndexMarker791"></a> is an<a id="_idIndexMarker792"></a> instance of the <strong class="source-inline">BaseChatStore</strong> class, is <a id="_idIndexMarker793"></a>used for the actual storage and retrieval of chat messages. This modular approach allows different storage implementations, such as a simple in-memory store or more complex <span class="No-Break">database-backed stores.</span></p>
			<p>We also have <a id="_idIndexMarker794"></a>the <strong class="source-inline">chat_store_key</strong> parameter, which is used to uniquely identify the chat session or conversation within the chat store. This is useful for retrieving the correct conversation history when there are multiple conversations stored in the <a id="_idIndexMarker795"></a>same chat store. Here’s a basic example of <strong class="bold">conversation history persistence</strong> <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">SimpleChatStore</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_215" title2="(no caption)" no2="">from llama_index.core.storage.chat_store import SimpleChatStore
from llama_index.core.chat_engine import SimpleChatEngine
from llama_index.core.memory import ChatMemoryBuffer</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>After importing the necessary libraries, we can try to load the previous conversation. If there is no previous conversation save file, we simply initialize an <span class="No-Break">empty </span><span class="No-Break"><strong class="source-inline">chat_store</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_216" title2="(no caption)" no2="">try:
&nbsp;&nbsp;&nbsp;&nbsp;chat_store = SimpleChatStore.from_persist_path(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persist_path="chat_memory.json"
&nbsp;&nbsp;&nbsp;&nbsp;)
except FileNotFoundError:
&nbsp;&nbsp;&nbsp;&nbsp;chat_store = SimpleChatStore()</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>It’s now time to initialize our memory buffer by using <strong class="source-inline">chat_store</strong> as an argument. Although not needed here, for a more detailed illustration, we will also customize <strong class="source-inline">token_limit</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">chat_store_key</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_217" title2="(no caption)" no2="">memory = ChatMemoryBuffer.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;token_limit=2000,
&nbsp;&nbsp;&nbsp;&nbsp;chat_store=chat_store,
&nbsp;&nbsp;&nbsp;&nbsp;chat_store_key="user_X"
&nbsp;&nbsp;&nbsp;&nbsp;)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>OK; we have all<a id="_idIndexMarker796"></a> the necessary pieces. Let’s put them together into a <strong class="source-inline">SimpleChatEngine</strong> class and create a <span class="No-Break">chat loop:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_218" title2="(no caption)" no2="">chat_engine = SimpleChatEngine.from_defaults(memory=memory)
while True:
&nbsp;&nbsp;&nbsp;&nbsp;user_message = input("You: ")
&nbsp;&nbsp;&nbsp;&nbsp;if user_message.lower() == 'exit':
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("Exiting chat...")
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;break
&nbsp;&nbsp;&nbsp;&nbsp;response = chat_engine.chat(user_message)
&nbsp;&nbsp;&nbsp;&nbsp;print(f"Chatbot: {response}")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Once the user types <strong class="source-inline">exit</strong> and we break the loop, we use the <strong class="source-inline">persist()</strong> method to store the current conversation for <span class="No-Break">future sessions:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_219" title2="(no caption)" no2="">chat_store.persist(persist_path="chat_memory.json")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In case you’re wondering why we haven’t used the <strong class="source-inline">chat_repl()</strong> method shown previously and created a chat loop instead, the answer is in the <span class="No-Break">following note.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">While the <strong class="source-inline">chat()</strong>, <strong class="source-inline">achat()</strong>, <strong class="source-inline">stream_chat()</strong>, and <strong class="source-inline">astream_chat()</strong> methods can benefit from loading and resuming previous conversations, by design, the <strong class="source-inline">chat_repl()</strong> method will reset the conversation history <span class="No-Break">during initialization.</span></p>
			<p><strong class="source-inline">ChatMemoryBuffer</strong> also plays an important role in ensuring that the conversation’s context remains within the token limits of the model being used. Among other parameters available for <strong class="source-inline">ChatMemoryBuffer</strong>, the <strong class="source-inline">token_limit</strong> attribute specifies the maximum number of tokens that can be stored in the memory buffer. This limit is essential to ensure we stay within the maximum context window size of the current LLM we <span class="No-Break">are using.</span></p>
			<p>When the<a id="_idIndexMarker797"></a> conversation exceeds the context limit, a sliding window method is applied. Older parts of the conversation are truncated to ensure that the most recent and relevant parts are retained and processed by the LLM within its <span class="No-Break">token constraints.</span></p>
			<p class="callout-heading">An analogy to better understand the sliding window method</p>
			<p class="callout">Imagine a conversation with an LLM as a train journey, where each piece of dialogue adds a carriage. However, the train can only be so long due to the tracks’ length limit, representing the model’s context window limit. To keep the journey going and add new carriages – in our case, messages – older ones need to be detached and left behind. This ensures the train can continue its journey, carrying the most recent and relevant parts of the conversation, while staying within the limits of the track. Just like in a train journey, where we might prioritize which carriages to keep based on their importance, the sliding window method prioritizes newer conversation parts, keeping the dialogue <span class="No-Break">flowing smoothly.</span></p>
			<p>Now that we understand how memory works, let’s talk about the different available <span class="No-Break">chat modes.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Simple mode" no2="8.2.2.2">8.2.2.2. Simple mode</h4>
			<p>This is<a id="_idIndexMarker798"></a> the most <strong class="bold">basic chat engine</strong> available. It allows for a simple, direct conversation with the LLM, without any connection to our proprietary data. <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.3</em> explains this chat <span class="No-Break">mode visually:</span></p>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/B21861_08_3.jpg" alt="Figure 8.3 – SimpleChatEngine" width="1495" height="632" data-type="figure" id="untitled_figure_55" title2="– SimpleChatEngine" no2="8.3">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – SimpleChatEngine</p>
			<p>The user’s experience<a id="_idIndexMarker799"></a> in this mode is defined by the inherent capabilities and limitations of the LLM, such<a id="_idIndexMarker800"></a> as its context window size and <span class="No-Break">overall performance.</span></p>
			<p>To initialize this mode, we can use the <span class="No-Break">following code:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_220" title2="(no caption)" no2="">from llama_index.core.chat_engine import SimpleChatEngine
chat_engine = SimpleChatEngine.from_defaults()
chat_engine.chat_repl()</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>If we want, we can customize the LLM using the <span class="No-Break"><strong class="source-inline">llm</strong></span><span class="No-Break"> argument:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_221" title2="(no caption)" no2="">from llama_index.llms.openai import OpenAI
llm = OpenAI(temperature=0.8, model="gpt-4")
chat_engine = SimpleChatEngine.from_defaults(llm=llm)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>As you probably won’t be using this mode too much in your RAG designs, let’s talk about the more advanced options that <span class="No-Break">are available.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Context mode" no2="8.2.2.3">8.2.2.3. Context mode</h4>
			<p><strong class="source-inline">ContextChatEngine</strong> is <a id="_idIndexMarker801"></a>designed to enhance chat interactions by<a id="_idIndexMarker802"></a> leveraging our proprietary knowledge. It works by retrieving relevant text from an index based on the user’s input, integrating this retrieved information into the system prompt to provide context, and then generating a response with the help of <span class="No-Break">the LLM.</span></p>
			<p>Have a look at <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.4</em> for a visual representation of this <span class="No-Break">chat mode:</span></p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/B21861_08_4.jpg" alt="Figure 8.4 – ContextChatEngine" width="1650" height="971" data-type="figure" id="untitled_figure_56" title2="– ContextChatEngine" no2="8.4">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – ContextChatEngine</p>
			<p>There are <a id="_idIndexMarker803"></a>several <a id="_idIndexMarker804"></a>parameters that <a id="_idIndexMarker805"></a>we can customize for this <span class="No-Break">chat engine:</span></p>
			<ul>
				<li><strong class="source-inline">retriever</strong>: The actual retriever that’s used to retrieve relevant text from the index based on the user’s message. When the chat engine is initialized directly from the index, it will use the default retriever for that particular <span class="No-Break">index type</span></li>
				<li><strong class="source-inline">llm</strong>: An instance of an LLM, which will be used for <span class="No-Break">generating responses</span></li>
				<li><strong class="source-inline">memory</strong>: A <strong class="source-inline">ChatMemoryBuffer</strong> object, which is used to store and manage the <span class="No-Break">chat history</span></li>
				<li><strong class="source-inline">chat_history</strong>: This is an optional list of <strong class="source-inline">ChatMessage</strong> instances representing the history of the conversation. It can be used to maintain continuity in a conversation. This history includes all messages that have been exchanged in the chat session, including both user and chatbot messages. For instance, it can be used to continue a conversation from a certain point. A <strong class="source-inline">ChatMessage</strong> object contains <span class="No-Break">three attributes:</span><ul><li><strong class="source-inline">role</strong>: This defaults <span class="No-Break">to </span><span class="No-Break"><em class="italic">user</em></span></li><li><strong class="source-inline">content</strong>: The <span class="No-Break">actual message</span></li><li>Any optional arguments provided <span class="No-Break">via </span><span class="No-Break"><strong class="source-inline">additional_kwargs</strong></span></li></ul></li>
				<li><strong class="source-inline">prefix_messages</strong>: A list of <strong class="source-inline">ChatMessage</strong> instances that may be used as predefined messages or prompts before the actual user message. This can be useful for setting a particular tone or context for <span class="No-Break">the chat</span></li>
				<li><strong class="source-inline">node_postprocessors</strong>: An optional list of <strong class="source-inline">BaseNodePostprocessor</strong> instances for further processing the nodes retrieved by the retriever. This can be used to <a id="_idIndexMarker806"></a>implement guardrails, scrub sensitive information from the<a id="_idIndexMarker807"></a> context, or make any other adjustments to the retrieved nodes <span class="No-Break">if required</span></li>
				<li><strong class="source-inline">context_template</strong>: A string template that can be used to format the prompt that feeds the context to <span class="No-Break">the LLM</span></li>
				<li><strong class="source-inline">callback_manager</strong>: An optional <strong class="source-inline">CallbackManager</strong> instance for managing callbacks during the chat process. This is useful for tracing and <span class="No-Break">debugging purposes</span></li>
				<li><strong class="source-inline">system_prompt</strong>: An optional string that’s used as a system prompt, providing initial context or instructions for <span class="No-Break">the chatbot</span></li>
				<li><strong class="source-inline">service_context</strong>: An optional <strong class="source-inline">ServiceContext</strong> instance, which can be used to make additional customizations to the <span class="No-Break">chat engine</span></li>
			</ul>
			<p>To implement <strong class="source-inline">ContextChatEngine</strong>, we <a id="_idIndexMarker808"></a>must load our data and build an index, then optionally configure the chat engine with different parameters <span class="No-Break">as needed.</span></p>
			<p>Here’s a quick example based on our sample data files, which can be found in the <strong class="source-inline">ch8/files</strong> subfolder in this book’s <span class="No-Break">GitHub repository:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_222" title2="(no caption)" no2="">from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
docs = SimpleDirectoryReader(input_dir="files").load_data()
index = VectorStoreIndex.from_documents(docs)
chat_engine = index.as_chat_engine(
&nbsp;&nbsp;&nbsp;&nbsp;chat_mode="context",
&nbsp;&nbsp;&nbsp;&nbsp;system_prompt=(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"You're a chatbot, able to talk about "
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"general topics, as well as answering specific "
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"questions about ancient Rome."
&nbsp;&nbsp;&nbsp;&nbsp;),
)
chat_engine.chat_repl()</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In this<a id="_idIndexMarker809"></a> example, we initialized <strong class="source-inline">chat_engine</strong> from the index. Alternatively, we could have defined it standalone, providing a retriever as an argument, <span class="No-Break">like this:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_223" title2="(no caption)" no2="">retriever = index.as_retriever(retriever_mode='default')
chat_engine = ContextChatEngine.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;retriever=retriever
&nbsp;&nbsp;&nbsp;&nbsp;)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Overall, this <a id="_idIndexMarker810"></a>chat mode is particularly effective for queries that relate to the knowledge contained within our data, supporting both general conversations and more specific discussions based on the <span class="No-Break">indexed content.</span></p>
			<p>Because the engine first retrieves context from the index and uses it to generate responses, this approach makes the chat experience a lot more useful and natural for users seeking specific information from the <span class="No-Break">indexed data.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Condense question mode" no2="8.2.2.4">8.2.2.4. Condense question mode</h4>
			<p><strong class="source-inline">CondenseQuestionChatEngine</strong> streamlines <a id="_idIndexMarker811"></a>the <a id="_idIndexMarker812"></a>user interaction by first <strong class="bold">condensing the conversation</strong> and the latest user message into a standalone question with the help of the LLM. This standalone question, which tries to capture the essential elements of the conversation, is then sent to a query engine built on our proprietary data to generate <span class="No-Break">a response.</span></p>
			<p>The main benefit of using this approach is that it maintains the conversation focused on the topic, preserving the essential points of the entire dialogue throughout every interaction. And it<a id="_idIndexMarker813"></a> always responds in the context of our <span class="No-Break">proprietary data.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.5</em> describes the operation of this particular <span class="No-Break">chat mode:</span></p>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/B21861_08_5.jpg" alt="Figure 8.5 – CondenseQuestionChatEngine" width="1650" height="382" data-type="figure" id="untitled_figure_57" title2="– CondenseQuestionChatEngine" no2="8.5">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – CondenseQuestionChatEngine</p>
			<p>The fact that the final response comes from our retrieved proprietary data and not directly from the LLM can also be a disadvantage sometimes. This chat mode may struggle with more general questions, such as inquiries about previous interactions, due to its reliance<a id="_idIndexMarker814"></a> on querying the knowledge base for <span class="No-Break">every response.</span></p>
			<p>Let’s look at some of<a id="_idIndexMarker815"></a> the key parameters <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">CondenseQuestionChatEngine</strong></span><span class="No-Break">:</span></p>
			<ul>
				<li><strong class="source-inline">query_engine</strong>: This is a <strong class="source-inline">BaseQueryEngine</strong> instance that’s used to query the condensed question. Any type of query engine may be used here, including complex constructs with <span class="No-Break">routing functionality</span></li>
				<li><strong class="source-inline">condense_question_prompt</strong>: This is a <strong class="source-inline">BasePromptTemplate</strong> instance that’s used for condensing the conversation and user message into a single, <span class="No-Break">standalone question</span></li>
				<li><strong class="source-inline">Memory</strong>: A <strong class="source-inline">ChatMemoryBuffer</strong> instance that’s used to manage and store the <span class="No-Break">chat history</span></li>
				<li><strong class="source-inline">llm</strong>: A language model instance for generating the <span class="No-Break">condensed question</span></li>
				<li><strong class="source-inline">verbose</strong>: A Boolean flag for printing verbose logs <span class="No-Break">during operation</span></li>
				<li><strong class="source-inline">callback_manager</strong>: An optional <strong class="source-inline">CallbackManager</strong> instance for <span class="No-Break">managing callbacks</span></li>
			</ul>
			<p>To implement<a id="_idIndexMarker816"></a> this chat<a id="_idIndexMarker817"></a> engine, we typically initialize it with a query engine and, optionally, configure it with custom parameters. The conversation is condensed into a question using a predefined template that can be customized using the <strong class="source-inline">condense_question_prompt</strong> parameter. The resulting question is then sent to the <span class="No-Break">query engine.</span></p>
			<p>Here’s a brief<a id="_idIndexMarker818"></a> <span class="No-Break">implementation example:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_224" title2="(no caption)" no2="">from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.chat_engine import CondenseQuestionChatEngine
from llama_index.core.llms import ChatMessage
documents = SimpleDirectoryReader("files").load_data()
index = VectorStoreIndex.from_documents(documents)
query_engine=index.as_query_engine()
chat_history = [
&nbsp;&nbsp;&nbsp;&nbsp;ChatMessage(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;role="user",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;content="Arch of Constantine is a famous"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"building in Rome"
&nbsp;&nbsp;&nbsp;&nbsp;),
&nbsp;&nbsp;&nbsp;&nbsp;ChatMessage(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;role="user",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;content="The Pantheon should not be "
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"regarded as a famous building"
&nbsp;&nbsp;&nbsp;&nbsp;),
]</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In the first part of the code, we ingested our sample files, created an index, and then created a simple query engine. Next, we introduced a previous conversation context by creating a<a id="_idIndexMarker819"></a> chat history consisting of two <strong class="source-inline">ChatMessage</strong> objects. Specifically, we instructed the chat engine not to consider the Pantheon as a <span class="No-Break">famous building.</span></p>
			<p>Now, let’s create our chat engine and <span class="No-Break">query it:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_225" title2="(no caption)" no2="">chat_engine = CondenseQuestionChatEngine.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;query_engine=query_engine,
&nbsp;&nbsp;&nbsp;&nbsp;chat_history=chat_history
)
response = chat_engine.chat(
&nbsp;&nbsp;&nbsp;&nbsp;"What are two of the most famous structures in ancient Rome?"
)
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Let’s see what <a id="_idIndexMarker820"></a>happened in <span class="No-Break">the background:</span></p>
			<ol>
				<li><strong class="source-inline">CondenseQuestionChatEngine</strong> took the user’s message, along with the provided chat history, and condensed them into a standalone question. This process involved using the LLM and <strong class="source-inline">condense_question_prompt</strong> to generate a question that encapsulates the essence of the conversation context and the user’s <span class="No-Break">latest query.</span></li>
				<li>Then, the engine forwarded this condensed question to the query engine, which searched the indexed data for <span class="No-Break">relevant information.</span></li>
				<li>The query engine, having access to the information from <strong class="source-inline">VectorStoreIndex</strong>, processed the question and returned an answer. This answer reflects the collective context of the previous conversation and the specific query about famous structures in <span class="No-Break">ancient Rome.</span></li>
			</ol>
			<p>Without the added chat history, the output of the sample would have been similar to <span class="No-Break">the following:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_226" title2="(no caption)" no2="">The Colosseum and the Pantheon.</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This is because the two buildings are explicitly mentioned in our <span class="No-Break">sample data.</span></p>
			<p>However, once we add the new conversational context, the output looks <span class="No-Break">like this:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_227" title2="(no caption)" no2="">The Colosseum and the Arch of Constantine are two famous buildings in ancient Rome.</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Another way of initializing this chat engine would be directly from the index, <span class="No-Break">like this:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_228" title2="(no caption)" no2="">index.as_chat_engine(chat_mode="condense_question")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This <a id="_idIndexMarker821"></a>chat mode is particularly useful for complex<a id="_idIndexMarker822"></a> conversations where the context and nuances of previous exchanges play a crucial role in understanding and accurately responding to the latest query. It ensures that the chatbot remains aware of the conversation’s history, thus making the interaction more coherent and <span class="No-Break">contextually relevant.</span></p>
			<p>The next chat mode we’ll talk about uses a mix of two <span class="No-Break">other approaches.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Condense and context mode" no2="8.2.2.5">8.2.2.5. Condense and context mode</h4>
			<p><strong class="source-inline">CondensePlusContextChatEngine</strong> offers<a id="_idIndexMarker823"></a> an even more comprehensive chat interaction<a id="_idIndexMarker824"></a> by combining the benefits of condensed questions and <span class="No-Break">context retrieval.</span></p>
			<p>While the previous chat engine we discussed is more straightforward and focuses on simplifying the conversation into a question for response generation, <strong class="source-inline">CondensePlusContextChatEngine</strong> takes an extra step to enrich the conversation with additional context from the indexed data, leading to more detailed and context-aware responses. The trade-off here is an increase in response generation time due to the additional step performed. Let’s explore how it works under the hood by looking at <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/B21861_08_6.jpg" alt="Figure 8.6 – CondensePlusContextChatEngine" width="1650" height="572" data-type="figure" id="untitled_figure_58" title2="– CondensePlusContextChatEngine" no2="8.6">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – CondensePlusContextChatEngine</p>
			<p>First, this engine<a id="_idIndexMarker825"></a> condenses a conversation and the latest user message into a standalone question. Then, it retrieves relevant context from the index using this condensed question. Finally, it uses both the retrieved context and the condensed question to <a id="_idIndexMarker826"></a>generate a response with <span class="No-Break">the LLM.</span></p>
			<p>Here are some of the<a id="_idIndexMarker827"></a> key parameters <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">CondensePlusContextChatEngine</strong></span><span class="No-Break">:</span></p>
			<ul>
				<li><strong class="source-inline">retriever</strong>: Used to fetch context based on the <span class="No-Break">condensed question</span></li>
				<li><strong class="source-inline">llm</strong>: The LLM that’s used to generate the condensed question and the <span class="No-Break">final response</span></li>
				<li><strong class="source-inline">memory</strong>: A <strong class="source-inline">ChatMemoryBuffer</strong> instance for storing and managing <span class="No-Break">chat history</span></li>
				<li><strong class="source-inline">context_prompt</strong>: A prompt template for formatting the context in the <span class="No-Break">system prompt</span></li>
				<li><strong class="source-inline">condense_prompt</strong>: A prompt for condensing the conversation into a <span class="No-Break">standalone question</span></li>
				<li><strong class="source-inline">system_prompt</strong>: A prompt with instructions for <span class="No-Break">the chatbot</span></li>
				<li><strong class="source-inline">skip_condense</strong>: A Boolean flag to bypass the condensation step <span class="No-Break">if desired</span></li>
				<li><strong class="source-inline">node_postprocessors</strong>: An optional list of <strong class="source-inline">BaseNodePostprocessors</strong> for additional processing of <span class="No-Break">retrieved nodes</span></li>
				<li><strong class="source-inline">callback_manager</strong>: As usual, this can be used for <span class="No-Break">managing callbacks</span></li>
				<li><strong class="source-inline">verbose</strong>: A Boolean flag for enabling verbose logging <span class="No-Break">during operation</span></li>
			</ul>
			<p>To build this <a id="_idIndexMarker828"></a>particular chat engine from an index, we can use the <span class="No-Break">following command:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_229" title2="(no caption)" no2="">index.as_chat_engine(chat_mode="condense_plus_context")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This chat mode is ideal in scenarios where both the context of the conversation and specific information from the indexed data are crucial for generating accurate and relevant responses. It enhances the chat experience by ensuring the responses are both contextually relevant and enriched with specific details from the <span class="No-Break">indexed content.</span></p>
			<p>OK. It’s time to discover the more advanced <span class="No-Break">chat modes.</span></p>
			<h2 id="f_12__idParaDest-186" data-type="sect1" class="sect1" title2="Implementing agentic strategies in our apps" no2="8.3"><a id="_idTextAnchor185"></a>8.3. Implementing agentic strategies in our apps</h2>
			<p><em class="italic">The name is Bot. </em><span class="No-Break"><em class="italic">Chat Bot</em></span><span class="No-Break">.</span></p>
			<p>At the<a id="_idIndexMarker829"></a> beginning of this chapter, we talked about the growing popularity of the ChatOps model. This model is based on the interaction between groups of human operators and AI agents, who can understand the context of discussions to provide answers to questions but also to perform certain functions, thus playing the role of virtual assistants for the group <span class="No-Break">they serve.</span></p>
			<p>You probably realize, however, that the chat engine models we have discussed so far can only answer questions and cannot execute functions or interact in ways other than read-only with <span class="No-Break">backend data.</span></p>
			<p>For these use cases, we <a id="_idIndexMarker830"></a><span class="No-Break">need </span><span class="No-Break"><strong class="bold">agents</strong></span><span class="No-Break">.</span></p>
			<p>The major difference between an agent and a simple chat engine is that an agent operates based<a id="_idIndexMarker831"></a> on a <strong class="bold">reasoning loop</strong> and has several tools at its disposal. After all, who would be Bond without the gadgets that Q <span class="No-Break">always provides?</span></p>
			<p>Unlike a simple chatbot, which can – at best – answer questions, either directly with the help of an LLM or by extracting proprietary data from a knowledge base, agents are much more powerful and can handle far more complex scenarios. This gives them a lot more utility in a business context, where human interactions augmented by AI are becoming <span class="No-Break">increasingly prevalent.</span></p>
			<p>Let’s understand the core components of an agent: the tools and the <span class="No-Break">reasoning loop.</span></p>
			<h3 id="f_12__idParaDest-187" data-type="sect2" class="sect2" title2="Building tools and ToolSpec classes for our agents" no2="8.3.1"><a id="_idTextAnchor186"></a>8.3.1. Building tools and ToolSpec classes for our agents</h3>
			<p>We<a id="_idIndexMarker832"></a> briefly discussed tools in <a href="#_idTextAnchor131"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>,<em class="italic"> Querying Our Data, Part 1 – Context Retrieval.</em> However, because the main topic of <a href="#_idTextAnchor131"><span class="No-Break"><em class="italic">Chapter 6</em></span></a> was data querying, I only showed you how different query engines or retrievers can be wrapped in tools and then become components of a router. In many ways, you can think of a router as a very simple type of agent. It uses LLM reasoning to decide which query engine or retriever should be used, depending on their specified purpose and the actual <span class="No-Break">user query.</span></p>
			<p>But tools can be a lot <span class="No-Break">more useful.</span></p>
			<p>A tool can also be a wrapper for any kind of user-defined function, capable of reading or writing data, calling functions from external APIs, or executing any kind of code. This means that tools come in two <span class="No-Break">different flavors:</span></p>
			<ul>
				<li><strong class="source-inline">QueryEngineTool</strong>: This<a id="_idIndexMarker833"></a> can encapsulate any existing query engine. This is the kind we covered during <a href="#_idTextAnchor131"><span class="No-Break"><em class="italic">Chapter 6</em></span></a> and it can only provide read-only access to <span class="No-Break">our data</span></li>
				<li><strong class="source-inline">FunctionTool</strong>: This<a id="_idIndexMarker834"></a> enables any user-defined function to be transformed into a tool. This is a universal type of tool as it allows any type of operation to <span class="No-Break">be executed</span></li>
			</ul>
			<p>Because we have already seen examples of how <strong class="source-inline">QueryEngineTool</strong> works, let’s focus on <span class="No-Break"><strong class="source-inline">FunctionTool</strong></span><span class="No-Break"> instead.</span></p>
			<p>Here’s an example of how we can <span class="No-Break">define one:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_230" title2="(no caption)" no2="">from llama_index.core.tools import FunctionTool
def calculate_average(*values):
&nbsp;&nbsp;&nbsp;&nbsp;"""
&nbsp;&nbsp;&nbsp;&nbsp;Calculates the average of the provided values.
&nbsp;&nbsp;&nbsp;&nbsp;"""
&nbsp;&nbsp;&nbsp;&nbsp;return sum(values) / len(values)
average_tool = FunctionTool.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;fn=calculate_average)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>To <a id="_idIndexMarker835"></a>enable agents to assimilate our functions as tools, they must contain descriptive docstrings, just like in the previous example. LlamaIndex relies on <a id="_idIndexMarker836"></a>these <strong class="bold">docstrings</strong> to provide agents with an <em class="italic">understanding</em> of the purpose and proper usage of a particular tool wrapping a <span class="No-Break">user-defined function.</span></p>
			<p class="callout-heading">Definition</p>
			<p class="callout">In Python, a docstring is a string literal that occurs as the first statement in a module, function, class, or method definition. It is used to document the purpose and usage of the code block it describes. Docstrings can be accessed from the code at runtime using the <strong class="source-inline">__doc__</strong> attribute on the object they describe, and they are also the primary way that documentation is generated <span class="No-Break">in Python.</span></p>
			<p>This description will be used by the reasoning loop of an agent to determine which particular tool is fit for solving a specific task, allowing the agent to decide the <span class="No-Break">execution path.</span></p>
			<p>However, competent agents are usually able to handle more than just <span class="No-Break">one tool.</span></p>
			<p>For this purpose, LlamaIndex also provides <a id="_idIndexMarker837"></a>the <strong class="source-inline">ToolSpec</strong> class. Akin to a collection of individual tools, <strong class="source-inline">ToolSpec</strong> specifies a full set of tools for a particular service. It’s like equipping our agent with a complete API for a particular type <span class="No-Break">of technology.</span></p>
			<p>We can build custom <strong class="source-inline">ToolSpec</strong> classes but there is also a growing number of them already available on LlamaHub: <a href="https://llamahub.ai/?tab=tools" target="_blank" rel="noopener noreferrer">https://llamahub.ai/?tab=tools</a>. They cover different types of service integrations, such as Gmail, Slack, SalesForce, Shopify, and <span class="No-Break">many others.</span></p>
			<p class="callout-heading">The LlamaHub agent tool repository</p>
			<p class="callout">The LlamaHub agent tool repository is a key addition to LlamaHub, providing a curated collection of tool specs that enable agents to interact with and extend the functionality of a range of services. This repository simplifies the agent design process for various APIs and includes numerous practical examples in its notebooks for easy integration <span class="No-Break">and use.</span></p>
			<p>Let’s take<a id="_idIndexMarker838"></a> the <strong class="source-inline">DatabaseToolSpec</strong> class available on LlamaHub as <span class="No-Break">an example.</span></p>
			<p>This <strong class="source-inline">ToolSpec</strong> class can be found here: <a href="https://llamahub.ai/l/tools-database?from=tools" target="_blank" rel="noopener noreferrer">https://llamahub.ai/l/tools-database?from=tools</a>. First, let’s have a look at <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.7</em> to understand <span class="No-Break">its structure:</span></p>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/B21861_08_7.jpg" alt="Figure 8.7 – DatabaseToolSpec" width="1650" height="481" data-type="figure" id="untitled_figure_59" title2="– DatabaseToolSpec" no2="8.7">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – DatabaseToolSpec</p>
			<p>Built<a id="_idIndexMarker839"></a> on top of the<a id="_idIndexMarker840"></a> SQLAlchemy library (<a href="https://www.sqlalchemy.org/" target="_blank" rel="noopener noreferrer">https://www.sqlalchemy.org/</a>) this tool collection can access many types of databases while providing three <span class="No-Break">simple tools:</span></p>
			<ul>
				<li><strong class="source-inline">list_tables</strong>: A tool that lists the tables in the <span class="No-Break">database schema</span></li>
				<li><strong class="source-inline">describe_tables</strong>: A tool that describes the schema of <span class="No-Break">a table</span></li>
				<li><strong class="source-inline">load_data</strong>: A tool that accepts a SQL query as input and returns the <span class="No-Break">resulting data</span></li>
			</ul>
			<p class="callout-heading">Quick note</p>
			<p class="callout">SQLAlchemy is a powerful and versatile toolkit for Python that allows developers to work with various databases, such as Microsoft SQL Server, OracleDB, MySQL, and others, in a more Pythonic way, abstracting away many of the complexities of database interaction and <span class="No-Break">query construction.</span></p>
			<p>Because this is not a LlamaIndex core component but comes as an integration package instead, it must be installed in our <span class="No-Break">environment first:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_231" title2="(no caption)" no2="">pip install llama-index-tools-database</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Next, to initialize this <strong class="source-inline">ToolSpec</strong>, all we have to do is <span class="No-Break">import it:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_232" title2="(no caption)" no2="">from llama_index.tools.database import DatabaseToolSpec</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Then, we must configure our database access, <span class="No-Break">like this:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_233" title2="(no caption)" no2="">db_tools = DatabaseToolSpec(&lt;db_specific_configuration&gt;)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Once<a id="_idIndexMarker841"></a> the <strong class="source-inline">ToolSpec</strong> class<a id="_idIndexMarker842"></a> has been built, if we want to initialize an agent with it, we have to convert it into a list of tools using the <strong class="source-inline">to_tool_list()</strong> method. This is because agents expect a list of tools as <span class="No-Break">an argument.</span></p>
			<p>Here’s how we can easily convert the <strong class="source-inline">ToolSpec</strong> class into a list of <span class="No-Break">tool objects:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_234" title2="(no caption)" no2="">tool_list = db_tools.to_tool_list()</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>At this point, we can pass <strong class="source-inline">tool_list</strong> as an argument when initializing any type of agent. Our agent will now be capable of <em class="italic">understanding</em> the schema of the database and extracting any required information from its tables. You can find a full example of how to use this <strong class="source-inline">ToolSpec</strong> class later in this chapter in the <em class="italic">OpenAIAgent</em> section. Next, let’s see how reasoning <span class="No-Break">loops work.</span></p>
			<h3 id="f_12__idParaDest-188" data-type="sect2" class="sect2" title2="Understanding reasoning loops" no2="8.3.2"><a id="_idTextAnchor187"></a>8.3.2. Understanding reasoning loops</h3>
			<p>Having so<a id="_idIndexMarker843"></a> many specialized tools already<a id="_idIndexMarker844"></a> available for our agents is a great advantage. But unfortunately, a box full of some of the best-quality instruments is not always enough. Our agents also need to know <em class="italic">when</em> to use each of <span class="No-Break">these tools.</span></p>
			<p>Specifically, the RAG applications we build need to decide – as autonomously as possible – which tool to use, depending on the specific user query and the dataset they are operating on. Any hard-coded solution will only deliver good results in a limited number of scenarios. This is where reasoning loops <span class="No-Break">come in.</span></p>
			<p>The reasoning loop is a fundamental aspect of agents, enabling them to intelligently decide which tools to use in different scenarios. This aspect is important because, in complex, real-world applications, the requirements can vary significantly and a static approach would limit the <span class="No-Break">agent’s effectiveness.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.8</em> presents a visual representation of the reasoning <span class="No-Break">loop concept:</span></p>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/B21861_08_8.jpg" alt="Figure 8.8 – The reasoning loop in an agent" width="1650" height="759" data-type="figure" id="untitled_figure_60" title2="– The reasoning loop in an agent" no2="8.8">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – The reasoning loop in an agent</p>
			<p>The<a id="_idIndexMarker845"></a> reasoning loop is responsible for the <a id="_idIndexMarker846"></a>decision-making process. It evaluates the context, understands the requirements of the task at hand, and then selects the appropriate tools from its arsenal to accomplish the task. This dynamic approach allows agents to adapt to various scenarios, making them versatile <span class="No-Break">and efficient.</span></p>
			<p>In LlamaIndex, the implementation of the reasoning loop is tailored to the type of agent. For instance, <strong class="source-inline">OpenAIAgent</strong> uses the Function API to make decisions, while <strong class="source-inline">ReActAgent</strong> relies on chat or text completion endpoints for its <span class="No-Break">reasoning process.</span></p>
			<p>This loop is not just about selecting the right tool, though; it’s also about determining the sequence in which the tools should be used and the specific parameters that should be applied. It’s the brain of the agent, orchestrating the tools to work together seamlessly, much like a skilled craftsman uses a combination of tools to create something greater than the sum of <span class="No-Break">its parts.</span></p>
			<p>This ability to intelligently interact with various tools and data sources, and read and modify data dynamically, sets agents apart from simpler chat engines and makes them invaluable in a business context where adaptability and intelligence <span class="No-Break">are key.</span></p>
			<p>The remaining types of chat modes that I’m going to describe over the next few pages are not simple chat engines but agents at their core. They all operate using a list of tools but implement the reasoning loop in <span class="No-Break">different ways.</span></p>
			<h3 id="f_12__idParaDest-189" data-type="sect2" class="sect2" title2="OpenAIAgent" no2="8.3.3"><a id="_idTextAnchor188"></a>8.3.3. OpenAIAgent</h3>
			<p>This <a id="_idIndexMarker847"></a>specialized agent leverages the capabilities <a id="_idIndexMarker848"></a>of OpenAI models, particularly those supporting the function calling API. It works with OpenAI models that have been designed to support the function calling API. They can interpret and execute function calls as part of <span class="No-Break">their capabilities.</span></p>
			<p class="callout-heading">Quick note</p>
			<p class="callout">These models are designed to interpret prompts and context to determine when a function call is appropriate. They respond with outputs that adhere to the defined structure of the function, based on the patterns they’ve learned during training. For more information on this topic and a list of supported models, you may consult the official OpenAI <span class="No-Break">documentation: </span><a href="https://platform.openai.com/docs/guides/function-calling" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://platform.openai.com/docs/guides/function-calling</span></a><span class="No-Break">.</span></p>
			<p>The key advantage of this agent type is that the tool selection logic is implemented directly on the model itself. When a task is provided by the user to <strong class="bold">OpenAIAgent</strong>, along with any previous chat history, the function API will analyze the context and decide whether another tool needs to be invoked or if a final response can be returned. If it determines that another tool is required, the function API will output the name of that tool. <strong class="source-inline">OpenAIAgent</strong> will then execute the tool, passing the tool’s response back into the chat history. This cycle continues until the API returns a final message, indicating the reasoning loop <span class="No-Break">is complete.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.9</em> explains this <span class="No-Break">process visually:</span></p>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="image/B21861_08_9.jpg" alt="Figure 8.9 – The simplified workflow of OpenAIAgent" width="1650" height="856" data-type="figure" id="untitled_figure_61" title2="– The simplified workflow of OpenAIAgent" no2="8.9">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9 – The simplified workflow of OpenAIAgent</p>
			<p>With the <a id="_idIndexMarker849"></a>model handling the complex logic of tool <a id="_idIndexMarker850"></a>selection and chaining, <strong class="source-inline">OpenAIAgent</strong> is a great solution for tool orchestration. One tradeoff is less flexibility compared to other architectures as the tool selection logic is hard-coded into <span class="No-Break">the LLM.</span></p>
			<p>However, for many use cases, the pre-trained capabilities of the function API model are sufficient to enable effective tool orchestration and <span class="No-Break">task completion.</span></p>
			<p>Before proceeding to the next example, make sure you install the required <span class="No-Break">integration package:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_235" title2="(no caption)" no2="">pip install llama-index-agent-openai</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>To <a id="_idIndexMarker851"></a>implement OpenAIAgent, we must define the available tools and then initialize the agent with these components, adding any other custom parameters we desire. The best way to explain how they work is through <span class="No-Break">an example.</span></p>
			<p>For the following example, we are using an SQLite database containing a single table called <em class="italic">Employees</em>. This table contains some randomly chosen salary data for 10 employees from different departments. <em class="italic">Table 8.1</em> displays the contents of the <span class="No-Break"><em class="italic">Employees</em></span><span class="No-Break"> table:</span></p>
			<table id="table001-3" class="No-Table-Style _idGenTablePara-1" data-type="table" title2="– The sample Employees table from the Employees.db file" no2="8.1"><colgroup><col><col><col><col><col></colgroup><thead><tr class="No-Table-Style"><th class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">ID</strong></span></p>
						</th><th class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Name</strong></span></p>
						</th><th class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Department</strong></span></p>
						</th><th class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Salary</strong></span></p>
						</th><th class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Email</strong></span></p>
						</th></tr></thead><tbody><tr class="No-Table-Style"><th class="No-Table-Style">
							<p>1</p>
						</th><th class="No-Table-Style">
							<p><span class="No-Break">Alice</span></p>
						</th><th class="No-Table-Style">
							<p><span class="No-Break">IT</span></p>
						</th><th class="No-Table-Style">
							<p><span class="No-Break">36420.77</span></p>
						</th><th class="No-Table-Style">
							<p><span class="No-Break">Alice_IT@org.com</span></p>
						</th></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p>2</p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">Karen</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">Finance</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">57705.06</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">Alice_Finance@org.com</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p>3</p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">Helen</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">IT</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">52612.51</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">Helen_IT@org.com</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p>4</p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">Jackie</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">Finance</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">61374.58</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">Jack_Finance@org.com</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p>5</p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">David</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">Finance</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">32242.72</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">David_Finance@org.com</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p>6</p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">Cora</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">HR</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">62040.53</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">Alice_HR@org.com</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p>7</p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">Ingrid</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">IT</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">70821.96</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">Alice_IT@org.com</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p>8</p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">Jack</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">IT</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">57268.89</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">Jack_IT@org.com</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p>9</p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">Bob</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">Finance</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">76868.23</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">Bob_Finance@org.com</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break">10</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">Bill</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">HR</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">74161.45</span></p>
						</td><td class="No-Table-Style">
							<p><span class="No-Break">Bob_HR@org.com</span></p>
						</td></tr></tbody></table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 8.1 – The sample Employees table from the Employees.db file</p>
			<p>The <a id="_idIndexMarker852"></a>database file itself can be found in the <strong class="source-inline">ch8/files/database</strong> subfolder of this book’s GitHub repository. Let’s have a<a id="_idIndexMarker853"></a> look at <span class="No-Break">the code:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_236" title2="(no caption)" no2="">from llama_index.tools.database import DatabaseToolSpec
from llama_index.core.tools import FunctionTool
from llama_index.agent.openai import OpenAIAgent
from llama_index.llms.openai import OpenAI</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The first part is responsible for <span class="No-Break">the imports.</span></p>
			<p>Next, it’s time to define a simple function that’s going to become a custom tool for our agent. This simple tool will allow us to save files in the local folder. Notice the detailed docstring that we are providing to <span class="No-Break">the agent:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_237" title2="(no caption)" no2="">def write_text_to_file(text, filename):
&nbsp;&nbsp;&nbsp;&nbsp;"""
&nbsp;&nbsp;&nbsp;&nbsp;Writes the text to a file with the specified filename.
&nbsp;&nbsp;&nbsp;&nbsp;Args:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text (str): The text to be written to the file.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;filename (str): File name to write the text into.
&nbsp;&nbsp;&nbsp;&nbsp;Returns: None
&nbsp;&nbsp;&nbsp;&nbsp;"""
&nbsp;&nbsp;&nbsp;&nbsp;with open(filename, 'w') as file:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;file.write(text)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Once the <a id="_idIndexMarker854"></a>function has been defined, we must wrap it into a new tool <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">save_tool</strong></span><span class="No-Break">.</span></p>
			<p>We also <a id="_idIndexMarker855"></a>initialize an entire <strong class="source-inline">ToolSpec</strong> class from the imported <strong class="source-inline">DatabaseToolSpec</strong>. We need these tools because the agent will have to read data from our SQLite database to solve <span class="No-Break">the task:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_238" title2="(no caption)" no2="">save_tool = FunctionTool.from_defaults(fn=write_text_to_file)
db_tools = DatabaseToolSpec(uri="sqlite:///files//database//employees.db")
tools = [save_tool]+db_tools.to_tool_list()</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Once we’ve created <strong class="source-inline">db_tools</strong>, we must join it with <strong class="source-inline">save_tool</strong> and put them into a single list called <strong class="source-inline">tools</strong>. We’ll use this list as an argument for initializing <span class="No-Break">the agent.</span></p>
			<p>Now, let’s build our agent. Notice that we’re not using the default LLM in this case; instead, we’re configuring our agent to use GPT-4 for <span class="No-Break">more accuracy:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_239" title2="(no caption)" no2="">llm = OpenAI(model="gpt-4")
agent = OpenAIAgent.from_tools(
&nbsp;&nbsp;&nbsp;&nbsp;tools=tools,
&nbsp;&nbsp;&nbsp;&nbsp;llm=llm,
&nbsp;&nbsp;&nbsp;&nbsp;verbose=True,
&nbsp;&nbsp;&nbsp;&nbsp;max_function_calls=20
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In the preceding <a id="_idIndexMarker856"></a>code, we initialized our agent using the list of tools we prepared. The <strong class="source-inline">verbose</strong> argument will make the agent display every execution step for better visibility of <a id="_idIndexMarker857"></a>the reasoning process. We also set <strong class="source-inline">max_function_calls</strong> to a larger value because, for complex tasks, the default value may not be enough to allow the agent to complete <span class="No-Break">the task.</span></p>
			<p class="callout-heading">A quick note on the max_function_calls parameter</p>
			<p class="callout">It may be <a id="_idIndexMarker858"></a>tempting to simply set this to a very large value to avoid exhausting the function calls and increase the chances for the agent to solve the task. Keep in mind, however, that every function call incurs costs, and sometimes, agents have the bad habit of entering infinite loops. I call them <em class="italic">rogue agents</em> when<a id="_idIndexMarker859"></a> they do that. Chances are that if your agent implementation requires a lot of LLM calls to solve even simple tasks, you’re probably doing something wrong when defining or describing the <span class="No-Break">underlying tools.</span></p>
			<p>Let’s continue with our code. It’s time to dispatch the task to <span class="No-Break">our agent:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_240" title2="(no caption)" no2="">response = agent.chat(
&nbsp;&nbsp;&nbsp;&nbsp;"For each IT department employee with a salary lower "
&nbsp;&nbsp;&nbsp;&nbsp;"than the average organization salary, write an email,"
&nbsp;&nbsp;&nbsp;&nbsp;"announcing a 10% raise and then save all emails into "
&nbsp;&nbsp;&nbsp;&nbsp;"a file called 'emails.txt'")
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>As you can see, the task we provided is relatively complex. Multiple steps will be required to solve it. As we are not providing too many details in the query, our agent will have to figure out the structure of the database and then craft a SQL query to extract the average salary in the organization and the list of employees from the IT department who are paid below <span class="No-Break">the average.</span></p>
			<p>Since the <strong class="source-inline">verbose</strong> argument is set to <strong class="source-inline">True</strong>, running this sample will show you the entire reasoning logic and steps performed by <span class="No-Break">the agent.</span></p>
			<p>Notice <a id="_idIndexMarker860"></a>how, in each step, the agent incorporates <a id="_idIndexMarker861"></a>outputs from the tools into its ongoing reasoning process. Once it has the list of employees, it will compose an email for each one. The final step of the task is to use our custom-created tool and save the results in a <span class="No-Break">local file.</span></p>
			<p>This is just a simple example. In a more complex implementation, instead of saving the text locally, for example, we <a id="_idIndexMarker862"></a>could import <strong class="source-inline">GmailToolSpec</strong> from LlamaHub and create email drafts that can be manually reviewed later and sent by the user. Unfortunately, that would have made the example much longer as <strong class="source-inline">GmailToolSpec</strong> requires stored credentials for the Google API, but I leave it to you to experiment with that <strong class="source-inline">ToolSpec</strong> class (<a href="https://llamahub.ai/l/tools-gmail?from=tools" target="_blank" rel="noopener noreferrer">https://llamahub.ai/l/tools-gmail?from=tools</a>) and all the other tools available <span class="No-Break">on LlamaHub.</span></p>
			<p>The customizable <a id="_idIndexMarker863"></a>parameters of <strong class="source-inline">OpenAIAgent</strong> are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="source-inline">tools</strong>: A list of <strong class="source-inline">BaseTool</strong> instances that the agent can utilize during the chat session. These tools can range from specialized query engines to custom processing modules or collections of tools extracted from <span class="No-Break"><strong class="source-inline">ToolSpec</strong></span><span class="No-Break"> classes</span></li>
				<li><strong class="source-inline">llm</strong>: Any OpenAI model that supports the function calling API. The default model that’s used <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">gpt-3.5-turbo-0613</strong></span></li>
				<li><strong class="source-inline">memory</strong>: Just like with any chat engine, this is a <strong class="source-inline">ChatMemoryBuffer</strong> instance that can be used for storing and managing the <span class="No-Break">chat history</span></li>
				<li><strong class="source-inline">prefix_messages</strong>: A list of <strong class="source-inline">ChatMessage</strong> instances that serve as pre-configured messages or prompts at the start of the <span class="No-Break">chat session</span></li>
				<li><strong class="source-inline">max_function_calls</strong>: The maximum number of function calls that can be made to the OpenAI model during a single chat interaction. The default <span class="No-Break">is 5</span></li>
				<li><strong class="source-inline">default_tool_choice</strong>: A string indicating the default choice of tool to be used when multiple tools are available. This is useful for coercing the agent into using a <span class="No-Break">specific tool</span></li>
				<li><strong class="source-inline">callback_manager</strong>: An optional <strong class="source-inline">CallbackManager</strong> instance for managing callbacks <a id="_idIndexMarker864"></a>during the chat process, aiding in tracing, <span class="No-Break">and debugging</span></li>
				<li><strong class="source-inline">system_prompt</strong>: An optional initial system prompt that provides context or instructions for <span class="No-Break">the agent</span></li>
				<li><strong class="source-inline">verbose</strong>: A Boolean flag to enable detailed logging <span class="No-Break">during operation</span></li>
			</ul>
			<p>Overall, <strong class="source-inline">OpenAIAgent</strong> stands <a id="_idIndexMarker865"></a>out from other chat engines due to its ability to execute complex function calls, on top of contextually rich conversations. This makes it particularly suitable for scenarios where advanced functionalities, such as integrating external tools or processing user queries in more sophisticated ways, are required. <strong class="source-inline">OpenAIAgent</strong> provides a versatile and powerful platform for creating engaging and intelligent <span class="No-Break">chat experiences.</span></p>
			<p>But wait – there are other types of <span class="No-Break">agents too.</span></p>
			<h3 id="f_12__idParaDest-190" data-type="sect2" class="sect2" title2="ReActAgent" no2="8.3.4"><a id="_idTextAnchor189"></a>8.3.4. ReActAgent</h3>
			<p>In contrast<a id="_idIndexMarker866"></a> to <strong class="source-inline">OpenAIAgent</strong>, <strong class="bold">ReActAgent</strong> uses <a id="_idIndexMarker867"></a>more generic text completion endpoints that can be driven by any LLM. It operates based on a <strong class="bold">ReAct</strong> loop within a chat mode built on top of a set <span class="No-Break">of tools.</span></p>
			<p>This loop<a id="_idIndexMarker868"></a> involves deciding whether to use any of the available tools, potentially using it and observing its output, and then deciding whether to repeat the process or provide a final response. This flexibility allows it to choose between using tools or relying solely on the LLM. However, this also means that its performance is heavily dependent on the quality of the LLM, often requiring more nuanced prompting to ensure accurate knowledge base queries, rather than relying on potentially inaccurate <span class="No-Break">model-generated responses.</span></p>
			<p>The input prompt for <strong class="source-inline">ReActAgent</strong> is carefully designed to guide the model in tool selection, using a format inspired by the ReAct paper by Yao, S., et al. (2022), <em class="italic">ReAct: Synergizing Reasoning and Acting in Language </em><span class="No-Break"><em class="italic">Models</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/abs/2210.03629" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://arxiv.org/abs/2210.03629</span></a><span class="No-Break">).</span></p>
			<p>It presents a list of available tools and asks the model to select one and provide the required parameters in JSON format. This explicit prompt is critical to the agent’s decision-making process. After selecting a tool, the agent executes it and integrates the response into <a id="_idIndexMarker869"></a>the chat history. This cycle of prompting, execution, and response integration continues until a satisfactory response is achieved. For an overall visual representation of the workflow, you may review the diagram that was presented for <strong class="source-inline">OpenAIAgent</strong> in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.9</em></span><span class="No-Break">.</span></p>
			<p>Unlike <strong class="source-inline">OpenAIAgent</strong>, which uses a function calling API with a model capable of selecting and chaining together multiple tools, the <strong class="source-inline">ReActAgent</strong> class’s logic must be fully encoded through <span class="No-Break">its prompts.</span></p>
			<p><strong class="source-inline">ReActAgent</strong> uses<a id="_idIndexMarker870"></a> a predefined loop with a maximum number of iterations, along with strategic prompting, to mimic a reasoning loop. Nevertheless, with strategic prompt engineering, <strong class="source-inline">ReActAgent</strong> can achieve effective tool orchestration and chained execution, similar to the output of the OpenAI <span class="No-Break">Function API.</span></p>
			<p>The key difference is that whereas the logic of the OpenAI Function API is embedded in the model, <strong class="source-inline">ReActAgent</strong> relies on the structure of its prompts to induce the desired tool selection behavior. This approach offers considerable flexibility as it can adapt to different language model backends, allowing for different implementations <span class="No-Break">and applications.</span></p>
			<p>In this case, we have the usual customizable parameters that we discussed for <strong class="source-inline">OpenAIAgent</strong>: <strong class="source-inline">tools</strong>, <strong class="source-inline">llm</strong>, <strong class="source-inline">memory</strong>, <strong class="source-inline">callback_manager</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">verbose</strong></span><span class="No-Break">.</span></p>
			<p>In addition, <strong class="source-inline">ReActAgent</strong> comes with<a id="_idIndexMarker871"></a> a few <span class="No-Break">specific parameters:</span></p>
			<ul>
				<li><strong class="source-inline">max_iterations</strong>: Similar to <strong class="source-inline">max_function_calls</strong>, this parameter sets the maximum number of iterations the ReAct loop can execute. This limit ensures that the agent does not enter an endless loop <span class="No-Break">of processing</span></li>
				<li><strong class="source-inline">react_chat_formatter</strong>: This formats the chat history into a structured list of <strong class="source-inline">ChatMessages</strong>, alternating between user and assistant roles, based on the provided tools, chat history, and reasoning steps. This helps maintain clarity and consistency in the <span class="No-Break">reasoning loop</span></li>
				<li><strong class="source-inline">output_parser</strong>: An optional instance of the <strong class="source-inline">ReActOutputParser</strong> class. This parser processes the outputs generated by the agent, helping in interpreting, and formatting <span class="No-Break">them appropriately</span></li>
				<li><strong class="source-inline">tool_retriever</strong>: An optional instance of <strong class="source-inline">ObjectRetriever</strong> for <strong class="source-inline">BaseTool</strong>. This <a id="_idIndexMarker872"></a>retriever can be used to dynamically fetch tools based on certain criteria. Similar to how we index nodes, there is also an option to create an <strong class="source-inline">ObjectIndex</strong> index to index a set of tools. This can be especially useful when we have to work with a large number of tools. You can find more information about this feature in the official <span class="No-Break">documentation: </span><a href="https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/usage_pattern.html#function-retrieval-agents" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/usage_pattern.html#function-retrieval-agents</span></a></li>
				<li><strong class="source-inline">context</strong>: An optional string providing initial instructions for <span class="No-Break">the agent</span></li>
			</ul>
			<p>Initializing <a id="_idIndexMarker873"></a>and using <strong class="source-inline">ReActAgent</strong> is done the same as with the OpenAI one, except this time, you won’t need to install any integration packages first – this type of agent is part of the core <span class="No-Break">LlamaIndex components:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_241" title2="(no caption)" no2="">from llama_index.agent.react import ReActAgent
agent = ReActAgent.from_tools(tools)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Overall, <strong class="source-inline">ReActAgent</strong> stands out for its flexibility as it can use any LLM to drive its unique ReAct loop, enabling it to smartly choose and use various tools. It’s like having a virtual assistant that not only answers questions but also intelligently decides when to consult external sources, making the conversation more contextually relevant and improving the <span class="No-Break">user experience.</span></p>
			<h3 id="f_12__idParaDest-191" data-type="sect2" class="sect2" title2="How do we interact with agents?" no2="8.3.5"><a id="_idTextAnchor190"></a>8.3.5. How do we interact with agents?</h3>
			<p>There<a id="_idIndexMarker874"></a> are two main methods that we can use to interact with an agent: <strong class="source-inline">chat()</strong> and <strong class="source-inline">query()</strong>. The first method utilizes stored conversation history to provide context-informed responses, making it <a id="_idIndexMarker875"></a>suitable for <span class="No-Break">ongoing dialogues.</span></p>
			<p>On the other hand, the former method operates in a stateless mode, treating each call independently without reference to past interactions. This is better suited for <span class="No-Break">standalone requests.</span></p>
			<h3 id="f_12__idParaDest-192" data-type="sect2" class="sect2" title2="Enhancing our agents with the help of utility tools" no2="8.3.6"><a id="_idTextAnchor191"></a>8.3.6. Enhancing our agents with the help of utility tools</h3>
			<p>To improve <a id="_idIndexMarker876"></a>the capabilities of the existing tools, LlamaIndex also provides two very useful so-called <em class="italic">utility tools</em> – <strong class="source-inline">OnDemandLoaderTool</strong> and <strong class="source-inline">LoadAndSearchToolSpec</strong>. They are universal and can be used with any type of agent to augment the standard tool functionality in <span class="No-Break">certain scenarios.</span></p>
			<p>One common<a id="_idIndexMarker877"></a> issue when interacting with an API is that we might receive a very long response in return. Our agents may not always be able to handle such <span class="No-Break">large outputs.</span></p>
			<p>Problems may arise because they may overflow the context window of the LLM or sometimes, key context may be diluted by a large amount of data, decreasing the accuracy of the agent’s <span class="No-Break">reasoning logic.</span></p>
			<p>A good way to understand this issue is by looking at our previous example for <strong class="source-inline">OpenAIAgent</strong>. In that case, we used a collection of tools called <strong class="source-inline">DatabaseToolSpec</strong> to retrieve data from our sample <em class="italic">Employees</em> table. If you’ve run that particular agent with the <strong class="source-inline">Verbose</strong> parameter set to <strong class="source-inline">True</strong>, then you’ve probably noticed that the outputs produced by the <strong class="source-inline">load_data</strong> tool are in the form of LlamaIndex document objects, as we can see in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.10</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/B21861_08_10.jpg" alt="Figure 8.10 – Sample output for the OpenAIAgent code example" width="1300" height="259" data-type="figure" id="untitled_figure_62" title2="– Sample output for the OpenAIAgent code example" no2="8.10">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10 – Sample output for the OpenAIAgent code example</p>
			<p>This means that whenever the agent calls the <strong class="source-inline">load_data</strong> tool, using a SQL query to interrogate the database, instead of simply receiving the output of the query, it gets a whole document <a id="_idIndexMarker878"></a>in return – along with a bunch of additional data, such as the ID of the document, metadata fields, hashes, and so on. The agent has to extract the actual query results from that data using the LLM, hence the aforementioned <span class="No-Break">potential issues.</span></p>
			<p>So, what if we want to extract <em class="italic">only</em> the result of the query, without all the additional data on top of it? That is the job <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">LoadAndSearchToolSpec</strong></span><span class="No-Break">.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Understanding the LoadAndSearchToolSpec utility" no2="8.3.6.1">8.3.6.1. Understanding the LoadAndSearchToolSpec utility</h4>
			<p>This <a id="_idIndexMarker879"></a>utility tool is designed to help the <a id="_idIndexMarker880"></a>agent handle large volumes of data from API endpoints, as demonstrated in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.11</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B21861_08_11.jpg" alt="Figure 8.11 – Visualization of a direct API call versus interaction via LoadAndSearchToolSpec" width="1650" height="354" data-type="figure" id="untitled_figure_63" title2="– Visualization of a direct API call versus interaction via LoadAndSearchToolSpec" no2="8.11">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.11 – Visualization of a direct API call versus interaction via LoadAndSearchToolSpec</p>
			<p>It takes an existing tool and generates two separate tools: one for loading and indexing data – by default, using a vector index – and another for conducting searches on this indexed data. The agent will now use the <em class="italic">Load</em> tool to ingest the data, and, similar to a caching mechanism, it will store it in an index. In the next step, the agent will use the <em class="italic">Search</em> tool to extract only the needed information using a built-in <span class="No-Break">query engine.</span></p>
			<p>Let’s see how that translates into code. We will adapt the previous <strong class="source-inline">OpenAIAgent</strong> example so that it <span class="No-Break">uses </span><span class="No-Break"><strong class="source-inline">LoadAndSearchToolSpec</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_242" title2="(no caption)" no2="">from llama_index.core.tools.tool_spec.load_and_search.base import (
&nbsp;&nbsp;&nbsp;&nbsp;LoadAndSearchToolSpec)
from llama_index.tools.database import DatabaseToolSpec
from llama_index.agent.openai import OpenAIAgent
from llama_index.llms.openai import OpenAI
db_tools = DatabaseToolSpec(
&nbsp;&nbsp;&nbsp;&nbsp;uri="sqlite:///files//database//employees.db")
tool_list = db_tools.to_tool_list()
tools=LoadAndSearchToolSpec.from_defaults(
tool_list[0]
).to_tool_list()</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Once we<a id="_idIndexMarker881"></a> finished with the imports, we<a id="_idIndexMarker882"></a> initialized our <strong class="source-inline">DatabaseToolSpec</strong> utility, which points to the same sample SQLite database as in the previous example. However, this time, we didn’t add any additional tools since we’ll only run a simple query. For that reason, we only pass the first tool from <strong class="source-inline">ToolSpec</strong> – that is, <strong class="source-inline">tool_list[0]</strong> – as an argument to <strong class="source-inline">LoadAndSearchToolSpec</strong>. That’s the <strong class="source-inline">load_data</strong> function, by the way. We don’t need the other two functions available in the database’s <strong class="source-inline">ToolSpec</strong> <span class="No-Break">this time.</span></p>
			<p>From this point on, the code is very <span class="No-Break">much straightforward:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_243" title2="(no caption)" no2="">llm = OpenAI(model="gpt-4")
agent = OpenAIAgent.from_tools(
&nbsp;&nbsp;&nbsp;&nbsp;tools=tools,
&nbsp;&nbsp;&nbsp;&nbsp;llm=llm,
&nbsp;&nbsp;&nbsp;&nbsp;verbose=True
)
response = agent.chat(
&nbsp;&nbsp;&nbsp;&nbsp;"Who has the highest salary in the Employees table?'")
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>If you look at the output – presented in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.12</em> – you’ll notice the reduced amount of data the agent has to deal with <span class="No-Break">this time:</span></p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/B21861_08_12.jpg" alt="Figure 8.12 – Sample agent output when LoadAndSearchToolSpec is used" width="957" height="392" data-type="figure" id="untitled_figure_64" title2="– Sample agent output when LoadAndSearchToolSpec is used" no2="8.12">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.12 – Sample agent output when LoadAndSearchToolSpec is used</p>
			<p>Instead<a id="_idIndexMarker883"></a> of receiving an entire document <a id="_idIndexMarker884"></a>as a response, the first call returns just a confirmation message that the data has been loaded and indexed, while the second extracts the final response using a query. We’ll talk about another utility <span class="No-Break">tool next.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Understanding OnDemandLoaderTool" no2="8.3.6.2">8.3.6.2. Understanding OnDemandLoaderTool</h4>
			<p>Another<a id="_idIndexMarker885"></a> important utility is <strong class="source-inline">OnDemandLoaderTool</strong>. This<a id="_idIndexMarker886"></a> utility is designed to make the process of loading, indexing, and querying data seamless and efficient within an agent’s workflow, particularly when dealing with large volumes of data from <span class="No-Break">various sources.</span></p>
			<p>It simplifies the process of using data loaders for agents by allowing them to trigger the loading, indexing, and querying of data through a single <span class="No-Break">tool call.</span></p>
			<p>The normal approach in a RAG workflow would be to ingest all data at the start of our application, then chunk it, index it, and build a query engine on it. But that may not always be the most <span class="No-Break">efficient method.</span></p>
			<p>Let’s say we have a large number of data sources. Ingesting and indexing all of them during startup would take a very long time, negatively affecting the user experience. And what if the user asks a question that cannot be answered by the agent based on the ingested data sources alone? That’s where a feature like this <span class="No-Break">becomes useful.</span></p>
			<p><strong class="source-inline">OnDemandLoaderTool</strong> is especially useful in scenarios where data requirements are dynamic and unpredictable. Instead of pre-loading a vast amount of data at startup, which may not all be relevant to the user’s current needs, this tool enables an agent to fetch, index, and query data on demand. This approach significantly enhances efficiency as it allows the agent to focus only on the relevant data at any given time, rather than handling large datasets that may not be <span class="No-Break">immediately necessary.</span></p>
			<p>How does it work? It <a id="_idIndexMarker887"></a>takes any existing data loader and wraps it into a tool that can be used by the<a id="_idIndexMarker888"></a> agent as required. Before running the code, make sure you install the Wikipedia <span class="No-Break">integration package:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_244" title2="(no caption)" no2="">pip install llama-index-readers-wikipedia</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Here’s the sample code. We’ll start with <span class="No-Break">the imports:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_245" title2="(no caption)" no2="">from llama_index.agent.openai import OpenAIAgent
from llama_index.core.tools.ondemand_loader_tool import(
&nbsp;&nbsp;&nbsp;&nbsp;OnDemandLoaderTool)
from llama_index.readers.wikipedia import WikipediaReader</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Next, let’s define an on-demand tool for our agent, based <span class="No-Break">on </span><span class="No-Break"><strong class="source-inline">WikipediaReader</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_246" title2="(no caption)" no2="">tool = OnDemandLoaderTool.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;WikipediaReader(),
&nbsp;&nbsp;&nbsp;&nbsp;name="WikipediaReader",
&nbsp;&nbsp;&nbsp;&nbsp;description="args: {'pages': [&lt;list of pages&gt;],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'query_str': &lt;query&gt;}"
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Notice how I provided usage instructions in the description argument. These should help the agent better <em class="italic">understand</em> how to properly use the tool, although it might still take a few tries to get it right. Now, it’s time to initialize <span class="No-Break">the agent:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_247" title2="(no caption)" no2="">agent = OpenAIAgent.from_tools(
&nbsp;&nbsp;&nbsp;&nbsp;tools=[tool],
&nbsp;&nbsp;&nbsp;&nbsp;verbose=True
)
response = agent.chat(
&nbsp;&nbsp;&nbsp;&nbsp;"What were some famous buildings in ancient Rome?")
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p class="callout-heading">Important side note</p>
			<p class="callout">One big advantage of using this approach is that once data has been loaded into the index, it’s also cached. Therefore, subsequent queries on the same topic will <span class="No-Break">run faster.</span></p>
			<p>In<a id="_idIndexMarker889"></a> addition, <strong class="source-inline">OnDemandLoaderTool</strong> can <a id="_idIndexMarker890"></a>be chained together with other, regular tools, allowing the agent to handle more <span class="No-Break">complex scenarios.</span></p>
			<p>With that, we’ve covered the basics. Now, let’s have a look at more advanced types <span class="No-Break">of agents.</span></p>
			<h3 id="f_12__idParaDest-193" data-type="sect2" class="sect2" title2="Using the LLMCompiler agent for more advanced scenarios" no2="8.3.7"><a id="_idTextAnchor192"></a>8.3.7. Using the LLMCompiler agent for more advanced scenarios</h3>
			<p>I saved the best <span class="No-Break">for last.</span></p>
			<p>While<a id="_idIndexMarker891"></a> they tend to perform well in many scenarios, both OpenAI and ReAct agents have<a id="_idIndexMarker892"></a> some drawbacks. Because current LLMs are not very good at long-term planning, they can sometimes get stuck in an infinite loop without finding the desired solution. At other times, their attention can be distracted by certain outputs they receive during execution, and this can cause them to stop before solving the <span class="No-Break">given task.</span></p>
			<p>But probably the biggest drawback of these types of agents is their serialized way of working. In other words, the execution of the steps is done in sequence. These agents wait for the output generated by one step to trigger the next step. This is a very inefficient approach in many practical scenarios. Often, a series of steps can be executed in parallel, significantly improving application performance and user experience. Based on these premises, I will now present a more advanced form <span class="No-Break">of agent.</span></p>
			<p>Inspired by the paper by Kim, S., et al. (2023), <em class="italic">An LLM Compiler for Parallel Function Calling</em> (<a href="https://arxiv.org/abs/2312.04511" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2312.04511</a>), this agent implementation offers outstanding performance and scalability. The concept is based on the ability of LLMs to execute multiple functions in parallel and draws inspiration from classical compilers to efficiently orchestrate <span class="No-Break">multi-function execution.</span></p>
			<p>The <strong class="bold">LLMCompiler agent</strong> orchestrates these parallel function calls using a three-part system that plans, dispatches, and executes tasks, resulting in faster and more accurate multi-function calls compared to sequential methods. Just as compilers transform and optimize<a id="_idIndexMarker893"></a> code to run efficiently, LLMCompiler transforms natural language queries into optimized sequences of function calls that can be executed in parallel when dependencies allow. This makes calling multiple tools with LLMs faster, cheaper, and potentially more accurate. An additional advantage is that it works with any kind of LLM, including both open source and closed <span class="No-Break">source models.</span></p>
			<p>Under the hood, an LLMCompileraAgent has three <span class="No-Break">main components:</span></p>
			<ul>
				<li><strong class="bold">LLM planner</strong>: Formulates <a id="_idIndexMarker894"></a>execution strategies and dependencies from user input <span class="No-Break">and examples</span></li>
				<li><strong class="bold">Task-fetching unit</strong>: Sends<a id="_idIndexMarker895"></a> and updates function-calling tasks based on <span class="No-Break">the dependencies</span></li>
				<li><strong class="bold">Executor</strong>: Executes<a id="_idIndexMarker896"></a> tasks in parallel using <span class="No-Break">associated tools</span></li>
			</ul>
			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.13</em> explains <a id="_idIndexMarker897"></a>the structure of the LLMCompiler <span class="No-Break">agent visually:</span></p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/B21861_08_13.jpg" alt="Figure 8.13 – An overview of the LLMCompiler agent’s architecture" width="1650" height="572" data-type="figure" id="untitled_figure_65" title2="– An overview of the LLMCompiler agent’s architecture" no2="8.13">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.13 – An overview of the LLMCompiler agent’s architecture</p>
			<p>The <em class="italic">LLM planner</em> determines the order of function calls and their interdependencies according to<a id="_idIndexMarker898"></a> user input. Next, the <em class="italic">task-fetching unit</em> initiates parallel execution of these functions, replacing variables with the outputs from prior tasks. The <em class="italic">executor</em> then carries out these function calls with the relevant tools. Combined, these elements enhance the efficiency of parallel function calling <span class="No-Break">in LLMs.</span></p>
			<p>The <strong class="bold">directed acyclic graph</strong> (<strong class="bold">DAG</strong>) of <a id="_idIndexMarker899"></a>tasks is a key data structure created by the <em class="italic">LLM planner</em> from user inputs and examples. This planning graph captures task dependencies <a id="_idIndexMarker900"></a>and enables optimized parallel <span class="No-Break">execution (</span><a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://en.wikipedia.org/wiki/Directed_acyclic_graph</span></a><span class="No-Break">).</span></p>
			<p>The DAG facilitates the simultaneous execution of tasks that do not depend on each other. Should one task rely on the completion of another, the prerequisite task must finish before the dependent task can commence. Independent tasks, on the other hand, are capable of being executed concurrently without any <span class="No-Break">dependency constraints.</span></p>
			<p class="callout-heading">Quick note</p>
			<p class="callout">While OpenAI has already introduced parallel function calling into their API, the LLMCompiler is still superior in its approach because it manifests fault tolerance in case of wrong LLM decisions and can replan, depending on the <span class="No-Break">outputs generated.</span></p>
			<p>To understand how we can<a id="_idIndexMarker901"></a> implement an agent using the LLMCompiler, let’s have a look at a simple example. But first, to run the example, you’ll need to install the necessary <span class="No-Break">integration package:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_248" title2="(no caption)" no2="">pip install llama-index-packs-agents-llm-compiler</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Here’s <span class="No-Break">the code:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_249" title2="(no caption)" no2="">from llama_index.tools.database import DatabaseToolSpec
from llama_index.packs.agents_llm_compiler import LLMCompilerAgentPack
db_tools = DatabaseToolSpec(
&nbsp;&nbsp;&nbsp;&nbsp;uri="sqlite:///files//database//employees.db")
agent = LLMCompilerAgentPack(db_tools.to_tool_list())</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>After importing <strong class="source-inline">LLMCompilerAgentPack</strong> and <strong class="source-inline">DatabaseToolSpec</strong>, we initialized the<a id="_idIndexMarker902"></a> database tools and used the tool list to initialize the agent. It’s now time to interact with the agent, this time using the <span class="No-Break"><strong class="source-inline">run()</strong></span><span class="No-Break"> method:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_250" title2="(no caption)" no2="">response = agent.run(
&nbsp;&nbsp;&nbsp;&nbsp;"Using only the available tools, "
&nbsp;&nbsp;&nbsp;&nbsp;"List the HR department employee "
&nbsp;&nbsp;&nbsp;&nbsp;"with the highest salary "
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.14</em> shows the output of the <span class="No-Break">preceding code:</span></p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/B21861_08_14.jpg" alt="Figure 8.14 – Sample output of the LLMCompiler agent" width="1335" height="443" data-type="figure" id="untitled_figure_66" title2="– Sample output of the LLMCompiler agent" no2="8.14">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.14 – Sample output of the LLMCompiler agent</p>
			<p>Looking<a id="_idIndexMarker903"></a> at the output, we can see both the execution plan generated by the agent and the actual steps performed. Quite neat, <span class="No-Break">isn’t it?</span></p>
			<p>In conclusion, LLMCompiler-based agents represent a leap forward in addressing the limitations of serial execution found in traditional agents, pushing the boundaries of what’s possible in terms of chatbot implementations and <span class="No-Break">user interaction.</span></p>
			<h3 id="f_12__idParaDest-194" data-type="sect2" class="sect2" title2="Using the low-level Agent Protocol API" no2="8.3.8"><a id="_idTextAnchor193"></a>8.3.8. Using the low-level Agent Protocol API</h3>
			<p>Taking<a id="_idIndexMarker904"></a> inspiration from<a id="_idIndexMarker905"></a> the <strong class="bold">Agent Protocol</strong> (<a href="https://agentprotocol.ai/" target="_blank" rel="noopener noreferrer">https://agentprotocol.ai/</a>) and several research papers, the LlamaIndex community also created a <a id="_idIndexMarker906"></a>more granular way to control the agents. This provides enhanced control and flexibility for executing user queries. It enables users to manage the agent’s actions with finer detail, facilitating the development of more sophisticated <span class="No-Break">agentic systems.</span></p>
			<p>The entire concept is based on two main components, <strong class="source-inline">AgentRunner</strong> and <strong class="source-inline">AgentWorker</strong>, and works as described in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.15</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="image/B21861_08_15.jpg" alt="Figure 8.15 – The AgentRunner and AgentWorker orchestration model" width="1650" height="530" data-type="figure" id="untitled_figure_67" title2="– The AgentRunner and AgentWorker orchestration model" no2="8.15">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.15 – The AgentRunner and AgentWorker orchestration model</p>
			<p>We use <strong class="bold">agent runners</strong> to <a id="_idIndexMarker907"></a>orchestrate tasks and store conversational memory. <strong class="bold">Agent workers</strong> control <a id="_idIndexMarker908"></a>the execution of each task step by step without storing the state themselves. The agent runner manages the overall process and integrates <span class="No-Break">the results.</span></p>
			<p>In terms of <a id="_idIndexMarker909"></a>benefits, there are multiple reasons to use agents like this. Firstly, it allows for a clear separation of concerns: agent runners manage the task’s overall orchestration and memory, while agent workers focus only on executing specific steps of a task. This division enhances the maintainability and scalability of <span class="No-Break">the system.</span></p>
			<p>Moreover, the architecture promotes enhanced visibility and control over the agent’s decision-making process. We can observe and intervene at each step, with very good insight into the agent’s operation. This is particularly useful for debugging and refining our <span class="No-Break">agent’s behavior.</span></p>
			<p>Another key benefit is the flexibility it provides. We can tailor the behavior of agents according to the specific needs of the application. We can modify or extend the functionality of agent workers, or integrate custom logic within the agent runner, making the system highly adaptable. This setup also supports modular development. We can build or update individual components without affecting the entire system, facilitating easier updates <span class="No-Break">and iterations.</span></p>
			<p>Here’s a sample<a id="_idIndexMarker910"></a> implementation that takes one of our previous examples and applies this more granular approach. We’ll implement <strong class="source-inline">OpenAIAgent</strong> in a low-level fashion by using <strong class="source-inline">AgentRunner</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">OpenAIAgentWorker</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_251" title2="(no caption)" no2="">from llama_index.core.agent import AgentRunner
from llama_index.agent.openai import OpenAIAgentWorker
from llama_index.tools.database import DatabaseToolSpec
db_tools = DatabaseToolSpec(
&nbsp;&nbsp;&nbsp;&nbsp;uri="sqlite:///files//database//employees.db"
)
tools = db_tools.to_tool_list()</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Here, we’ve<a id="_idIndexMarker911"></a> imported the necessary components and prepared the tool list for the agent. We’re using the same <strong class="source-inline">employees.db</strong> database as before. Next, we’ll define the <span class="No-Break">agent worker:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_252" title2="(no caption)" no2="">step_engine = OpenAIAgentWorker.from_tools(
&nbsp;&nbsp;&nbsp;&nbsp;tools,
&nbsp;&nbsp;&nbsp;&nbsp;verbose=True
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>It’s time to initialize our agent runner and prepare the input that will contain <span class="No-Break">the task:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_253" title2="(no caption)" no2="">agent = AgentRunner(step_engine)
input =&nbsp;&nbsp;(
&nbsp;&nbsp;&nbsp;&nbsp;"Find the highest paid HR employee and write "
&nbsp;&nbsp;&nbsp;&nbsp;"them an email announcing a bonus"
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>There are two distinct methods to engage with our agent now. Let’s take <span class="No-Break">a look.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Option A – the end-to-end interaction, using the chat() method" no2="8.3.8.1">8.3.8.1. Option A – the end-to-end interaction, using the chat() method</h4>
			<p>The <strong class="source-inline">chat()</strong> method <a id="_idIndexMarker912"></a>offers a seamless, end-to-end interaction, executing the task without requiring intervention at each <span class="No-Break">reasoning step:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_254" title2="(no caption)" no2="">response = agent.chat(input)
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>It’s very straightforward: just two lines of code, at which point we wait for the agent to solve the task and provide a final response when all the steps <span class="No-Break">are completed.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Option B – the step-by-step interaction, using the create_task() method" no2="8.3.8.2">8.3.8.2. Option B – the step-by-step interaction, using the create_task() method</h4>
			<p>For more <a id="_idIndexMarker913"></a>granular control, we could leverage the agent runner and use a step-by-step method that allows us to create a task, run each step individually, and then finalize <span class="No-Break">the response:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_255" title2="(no caption)" no2="">task = agent.create_task(input)
step_output = agent.run_step(task.task_id)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In the first part, we created a new task for the agent runner and executed the first step of the task. Because this method provides manual control of the execution of each step, we have to manually implement a loop in our code. We will repeatedly call <strong class="source-inline">run_step()</strong> until the output indicates all steps <span class="No-Break">are complete:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_256" title2="(no caption)" no2="">while not step_output.is_last:
&nbsp;&nbsp;&nbsp;&nbsp;step_output = agent.run_step(task.task_id)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The previous loop will run until the last step is completed. Then, it’s time to synthesize and display the <span class="No-Break">final answer:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_257" title2="(no caption)" no2="">response = agent.finalize_response(task.task_id)
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This<a id="_idIndexMarker914"></a> allows us to execute and observe each reasoning step individually. The <strong class="source-inline">create_task()</strong> method initializes a new task, <strong class="source-inline">run_step()</strong> executes each step, returning an output, and <strong class="source-inline">finalize_response()</strong> generates the final response once all steps <span class="No-Break">are complete.</span></p>
			<p>Overall, this option is particularly useful when you need to monitor the agent’s decisions closely or when you want to step in at certain points to guide the process or to <span class="No-Break">handle exceptions.</span></p>
			<p>Now, it’s time to apply this fresh knowledge and add some chat features to our <span class="No-Break">PITS project.</span></p>
			<h2 id="f_12__idParaDest-195" data-type="sect1" class="sect1" title2="Hands-on – implementing conversation tracking for PITS" no2="8.4"><a id="_idTextAnchor194"></a>8.4. Hands-on – implementing conversation tracking for PITS</h2>
			<p>In this<a id="_idIndexMarker915"></a> practical section, we’ll use some of our newfound knowledge to further improve our personal tutoring project. Like any professional tutor, eager to teach students and answer their questions, PITS should have a proper conversational engine at its core. It should be able to understand the topic, be aware of the current context, and keep track of the entire interaction with the student. Because the learning process will probably take place through multiple sessions, PITS must be able to persist the entire conversation and resume the interaction when a new session is initiated. We’ll implement all these features in <strong class="source-inline">coversation_engine.py</strong>. This <a id="_idIndexMarker916"></a>module is not meant to be used directly in our app architecture. Instead, it will provide three callable functions that we will later import and use in<a id="_idIndexMarker917"></a> the <span class="No-Break"><strong class="source-inline">training_UI.py</strong></span><span class="No-Break"> module:</span></p>
			<ul>
				<li><strong class="source-inline">load_chat_store</strong>: This<a id="_idIndexMarker918"></a> function is responsible for retrieving the chatbot conversation from previous sessions. We’re using a generic <strong class="source-inline">chat_store_key="0"</strong> key. In a multi-user scenario, this key could be used to store chat conversations for different users in the same <span class="No-Break">chat store.</span></li>
				<li><strong class="source-inline">initialize_chatbot</strong>: This<a id="_idIndexMarker919"></a> function is responsible for loading the training material vector index from storage, defining a query engine tool on the index, and then initializing <strong class="source-inline">OpenAIAgent</strong> using this tool as an argument. It also provides the agent with a system prompt that contains context information describing the purpose of the agent, the username and study topic, as well as the current slide content. The function returns the initialized agent, which will then be used by <strong class="source-inline">chat_interface</strong> to implement the <span class="No-Break">actual conversation.</span></li>
				<li><strong class="source-inline">chat_interface</strong>: This <a id="_idIndexMarker920"></a>function implements the ongoing conversation by taking <a id="_idIndexMarker921"></a>the user input and generating an answer from the agent. It also persists the conversation after each interaction. If the user ends the current session, on resume, the conversation will be continued from <span class="No-Break">that point.</span></li>
			</ul>
			<p>Once implemented in the main training interface, this chat should look similar to what’s shown in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.16</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="image/B21861_08_16.jpg" alt="Figure 8.16 – Screenshot from the PITS training UI" width="1209" height="494" data-type="figure" id="untitled_figure_68" title2="– Screenshot from the PITS training UI" no2="8.16">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.16 – Screenshot from the PITS training UI</p>
			<p>Let’s have a look at the code. The first part contains all the <span class="No-Break">necessary imports:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_258" title2="(no caption)" no2="">import os
import json
import streamlit as st
from openai import OpenAI
from llama_index.core import load_index_from_storage
from llama_index.core import StorageContext
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.tools import QueryEngineTool, ToolMetadata
from llama_index.agent.openai import OpenAIAgent
from llama_index.core.storage.chat_store import SimpleChatStore
from global_settings import INDEX_STORAGE, CONVERSATION_FILE</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>You’ll notice<a id="_idIndexMarker922"></a> that in the first part of the code, we imported a lot of components. The <strong class="source-inline">os</strong> and <strong class="source-inline">json</strong> modules will be used for the chat persistence feature. The specific LlamaIndex elements will be used to implement the agent with all its <span class="No-Break">required components.</span></p>
			<p>We also imported the <strong class="source-inline">INDEX_STORAGE</strong> and <strong class="source-inline">CONVERSATION_FILE</strong> locations from the <strong class="source-inline">global_settings.py</strong> module. Because the chat conversation will be implemented using Streamlit, we also have to import the <span class="No-Break"><strong class="source-inline">streamlit</strong></span><span class="No-Break"> library.</span></p>
			<p>Next, let’s have a look at the <strong class="source-inline">load_chat_store</strong> function, which is responsible for resuming the previous conversation by loading the chat history from the local storage file specified <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">CONVERSATION_FILE</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_259" title2="(no caption)" no2="">def load_chat_store():
&nbsp;&nbsp;&nbsp;&nbsp;try:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chat_store = SimpleChatStore.from_persist_path(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CONVERSATION_FILE
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;except FileNotFoundError:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chat_store = SimpleChatStore()
&nbsp;&nbsp;&nbsp;&nbsp;return chat_store</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>As we can <a id="_idIndexMarker923"></a>see, the <strong class="source-inline">load_chat_store</strong> function tries to retrieve the conversation history from the local storage file. If the storage file does not exist, a new empty <strong class="source-inline">chat_store</strong> is created. The function <span class="No-Break">returns </span><span class="No-Break"><strong class="source-inline">chat_store</strong></span><span class="No-Break">.</span></p>
			<p>The next function is responsible for displaying the entire conversation history in the <span class="No-Break">Streamlit interface:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_260" title2="(no caption)" no2="">def display_messages(chat_store, container):
&nbsp;&nbsp;&nbsp;&nbsp;with container:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for message in chat_store.get_messages(key="0"):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with st.chat_message(message.role):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st.markdown(message.content)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The <strong class="source-inline">display_messages</strong> function takes a chat store and a Streamlit container as arguments. It extracts all messages from the chat store using <strong class="source-inline">get_messages()</strong>. The function iterates over and displays each message from the chat store, assigning appropriate roles – <em class="italic">user</em> or <em class="italic">assistant</em> – <span class="No-Break">to each.</span></p>
			<p>The messages are displayed in the Streamlit container using Streamlit’s <strong class="source-inline">chat_message()</strong> method, which has the advantage of automatically adding a corresponding icon for <span class="No-Break">each role.</span></p>
			<p>The next function is responsible for initializing the agent. This function takes <span class="No-Break">five arguments:</span></p>
			<ul>
				<li><strong class="source-inline">user_name</strong>: The name of the user – to enable a more <span class="No-Break">personal experience.</span></li>
				<li><strong class="source-inline">study_subject</strong>: The topic covered by the <span class="No-Break">study materials.</span></li>
				<li><strong class="source-inline">chat_store</strong>: Used to initialize the <span class="No-Break">conversation history.</span></li>
				<li><strong class="source-inline">container</strong>: This is the Streamlit container where the chat conversation will be displayed. It’s not used by this function itself and instead passed further to the <span class="No-Break"><strong class="source-inline">display_messages</strong></span><span class="No-Break"> function.</span></li>
				<li><strong class="source-inline">context</strong>: This is the content of the current slide being displayed in the training interface. This context will be fed into the agent’s system prompt to ground any answer on the current context of <span class="No-Break">the user.</span></li>
			</ul>
			<p>Let’s see <a id="_idIndexMarker924"></a>the first part of <span class="No-Break">the function:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_261" title2="(no caption)" no2="">def initialize_chatbot(user_name, study_subject,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chat_store, container, context):
&nbsp;&nbsp;&nbsp;&nbsp;memory = ChatMemoryBuffer.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;token_limit=3000,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chat_store=chat_store,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chat_store_key="0"
&nbsp;&nbsp;&nbsp;&nbsp;)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Here, we have defined a <strong class="source-inline">ChatMemoryBuffer</strong> object for the agent, specifying the <strong class="source-inline">chat_store</strong> attribute containing the conversation history. We used the same <strong class="source-inline">chat_store_key</strong> as before. This is important to allow the agent to correctly retrieve the <span class="No-Break">chat history.</span></p>
			<p>Next, we’ll prepare the tools for <span class="No-Break">the agent:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_262" title2="(no caption)" no2="">&nbsp;&nbsp;&nbsp;&nbsp;storage_context = StorageContext.from_defaults(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persist_dir=INDEX_STORAGE
&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;index = load_index_from_storage(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;storage_context, index_id="vector"
&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;study_materials_engine = index.as_query_engine(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;similarity_top_k=3
&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;study_materials_tool = QueryEngineTool(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;query_engine=study_materials_engine,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata=ToolMetadata(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;name="study_materials",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;description=(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f"Provides official information about "
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f"{study_subject}. Use a detailed plain "
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f"text question as input to the tool."
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Here, we first retrieved our vector index by using a <strong class="source-inline">StorageContext</strong> object and the <strong class="source-inline">load_index_from_storage()</strong> method. We had to specify the <em class="italic">ID</em> of the index – <em class="italic">vector</em> – because in our case, the storage contains more than <span class="No-Break">one index.</span></p>
			<p>After loading <a id="_idIndexMarker925"></a>the index, we created a simple query engine configured with <strong class="source-inline">similarity_top_k=3</strong> and then created a <strong class="source-inline">QueryEngineTool</strong> utility, providing a proper description in its metadata so that the agent can <em class="italic">understand</em> its purpose and usage. The top-k similarity parameter is set to <strong class="source-inline">3</strong> to retrieve the three most relevant pieces of information from <span class="No-Break">the index.</span></p>
			<p>The next part will <span class="No-Break">initialize </span><span class="No-Break"><strong class="source-inline">OpenAIAgent</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_263" title2="(no caption)" no2="">&nbsp;&nbsp;&nbsp;&nbsp;agent = OpenAIAgent.from_tools(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tools=[study_materials_tool],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;memory=memory,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;system_prompt=(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f"Your name is PITS, a personal tutor. Your "
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f"purpose is to help {user_name} study and "
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f"better understand the topic of: "
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f"{study_subject}. We are now discussing the "
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f"slide with the following content: {context}"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;display_messages(chat_store, container)
&nbsp;&nbsp;&nbsp;&nbsp;return agent</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In the preceding code, we initialized <strong class="source-inline">OpenAIAgent</strong> while providing <strong class="source-inline">QueryEngineTool</strong>, <strong class="source-inline">memory</strong>, and <strong class="source-inline">system_prompt</strong> as arguments. This prompt is used to provide the LLM with background information to contextualize its responses, ensuring they are relevant to the current discussion topic and the user’s <span class="No-Break">study needs.</span></p>
			<p>As you can<a id="_idIndexMarker926"></a> see, I’ve tried to keep the code as simple as possible. Many things could be improved in this implementation. After initializing the agent, we call <strong class="source-inline">display_messages</strong> to display the <span class="No-Break">existing conversation.</span></p>
			<p>Our last function is responsible for handling the actual conversation. It takes <span class="No-Break">three arguments:</span></p>
			<ul>
				<li><strong class="source-inline">agent</strong>: The agent engine that will be used to run <span class="No-Break">the chat</span></li>
				<li><strong class="source-inline">chat_store</strong>: The <strong class="source-inline">chat_store</strong> argument that will be used to persist <span class="No-Break">the conversation</span></li>
				<li><strong class="source-inline">container</strong>: The Streamlit container where the messages will <span class="No-Break">be displayed</span></li>
			</ul>
			<p>Let’s have a look at <span class="No-Break">the code:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_264" title2="(no caption)" no2="">def chat_interface(agent, chat_store, container):
&nbsp;&nbsp;&nbsp;&nbsp;prompt = st.chat_input("Type your question here:")
&nbsp;&nbsp;&nbsp;&nbsp;if prompt:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with container:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with st.chat_message("user"):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st.markdown(prompt)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;response = str(agent.chat(prompt))
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with st.chat_message("assistant"):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st.markdown(response)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chat_store.persist(CONVERSATION_FILE)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This <strong class="source-inline">chat_interface</strong> function<a id="_idIndexMarker927"></a> displays a chat input widget using Streamlit’s <strong class="source-inline">chat_input()</strong> method. Upon <a id="_idIndexMarker928"></a>receiving input, it does <span class="No-Break">the following:</span></p>
			<ul>
				<li>Adds the user’s question to the chat interface in the <span class="No-Break">specified container</span></li>
				<li>Calls the chat method of <strong class="source-inline">OpenAIAgent</strong> to process the question and generate <span class="No-Break">a response</span></li>
				<li>Adds the chatbot’s response to the chat interface in the <span class="No-Break">specified container</span></li>
				<li>Persists the new conversation to <strong class="source-inline">CONVERSATION_FILE</strong> using the chat store’s persist method to ensure continuity <span class="No-Break">across sessions</span></li>
			</ul>
			<p>That’s it for now. We’ll talk about more of the features of PITS in the next <span class="No-Break">few chapters.</span></p>
			<h2 id="f_12__idParaDest-196" data-type="sect1" class="sect1" title2="Summary" no2="8.5"><a id="_idTextAnchor195"></a>8.5. Summary</h2>
			<p>This chapter provided an in-depth exploration of building chatbots and agents with LlamaIndex. We covered <strong class="source-inline">ChatEngine</strong> for conversation tracking and different built-in chat modes, such as simple, context, condense question, and condense <span class="No-Break">plus context.</span></p>
			<p>Then, we explored different agent architectures and strategies using <strong class="source-inline">OpenAIAgent</strong>, <strong class="source-inline">ReActAgent</strong>, and the more advanced LLMCompiler agent. Key concepts such as tools, tool orchestration, reasoning loops, and parallel execution <span class="No-Break">were explained.</span></p>
			<p>We concluded this chapter with a hands-on implementation of conversation tracking for the PITS <span class="No-Break">tutoring application.</span></p>
			<p>Overall, you should now have a comprehensive understanding of leveraging LlamaIndex capabilities to create useful and engaging <span class="No-Break">conversational interfaces.</span></p>
			<p>Throughout the next chapter, we’ll discover how to customize our RAG pipeline and provide a straightforward guide to deploying it with Streamlit. We’ll also explore advanced tracing methods for seamless debugging and unravel strategies for evaluating <span class="No-Break">our applications.</span></p>
		</div>
<div id="f_13__idContainer095" class="part" data-type="part" file="B21861_Part_4_xhtml" title2="Customization, Prompt Engineering, and Final Words" no2="4">
			<h1 id="f_13__idParaDest-197" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor196"></a>Part 4: Customization, Prompt Engineering, and Final Words</h1>
		</div>
<div id="f_14__idContainer110" data-type="chapter" class="chapter" file="B21861_09_xhtml" title2="Customizing and Deploying Our LlamaIndex Project" no2="9">
			<h1 id="f_14__idParaDest-198" class="chapter-number"><a id="_idTextAnchor197"></a>9</h1>
			<h1 id="f_14__idParaDest-199"><a id="_idTextAnchor198"></a>Customizing and Deploying Our LlamaIndex Project</h1>
			<p>Customizing <strong class="bold">Retrieval-Augmented Generation</strong> (<strong class="bold">RAG</strong>) components and optimizing performance<a id="_idIndexMarker929"></a> is critical to building robust, production-ready applications with LlamaIndex. This chapter explores methods for leveraging<a id="_idIndexMarker930"></a> open source models, intelligent routing across <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>), and using community-built modules to increase flexibility and cost-effectiveness. Advanced tracing, evaluation methods, and deployment options are explored to gain deep insight, ensure reliable operation, and streamline the development <span class="No-Break">life cycle.</span></p>
			<p>Throughout this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Customizing our <span class="No-Break">RAG components</span></li>
				<li>Using advanced tracing and <span class="No-Break">evaluation techniques</span></li>
				<li>Introduction to deployment <span class="No-Break">with Streamlit</span></li>
				<li>Hands-on – a step-by-step <span class="No-Break">deployment guide</span></li>
			</ul>
			<h2 id="f_14__idParaDest-200" data-type="sect1" class="sect1" title2="Technical requirements" no2="9.1"><a id="_idTextAnchor199"></a>9.1. Technical requirements</h2>
			<p>For this chapter, you will need to install the following package in <span class="No-Break">your environment:</span></p>
			<ul>
				<li><em class="italic">Arize AI </em><span class="No-Break"><em class="italic">Phoenix</em></span><span class="No-Break">: </span><a href="https://pypi.org/project/arize-phoenix/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pypi.org/project/arize-phoenix/</span></a></li>
			</ul>
			<p>Three additional integration packages are required in order to run the <span class="No-Break">sample code:</span></p>
			<ul>
				<li><em class="italic">Hugging Face </em><span class="No-Break"><em class="italic">embeddings</em></span><span class="No-Break">: </span><a href="https://pypi.org/project/llama-index-embeddings-huggingface/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pypi.org/project/llama-index-embeddings-huggingface/</span></a></li>
				<li><em class="italic">Zephyr query </em><span class="No-Break"><em class="italic">engine</em></span><span class="No-Break">: </span><a href="https://pypi.org/project/llama-index-packs-zephyr-query-engine/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pypi.org/project/llama-index-packs-zephyr-query-engine/</span></a></li>
				<li><em class="italic">Neutrino </em><span class="No-Break"><em class="italic">LLM</em></span><span class="No-Break">: </span><a href="https://pypi.org/project/llama-index-llms-neutrino/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://pypi.org/project/llama-index-llms-neutrino/</span></a></li>
			</ul>
			<p>All code samples from this chapter can be found in the <strong class="source-inline">ch9</strong> subfolder of the book’s <span class="No-Break">GitHub repository:</span></p>
			<p><a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex</span></a></p>
			<h2 id="f_14__idParaDest-201" data-type="sect1" class="sect1" title2="Customizing our RAG components" no2="9.2"><a id="_idTextAnchor200"></a>9.2. Customizing our RAG components</h2>
			<p>For starters, let’s talk about which components<a id="_idIndexMarker931"></a> of a RAG workflow can be customized in LlamaIndex. The short answer is <em class="italic">pretty much all of them, as we have seen already in the previous chapters</em>. The fact that the framework itself is flexible and allows customization of all the core components is a definite advantage. But leaving aside the framework itself, the core of a RAG workflow is actually the LLM and the embedding model it uses. In all the examples given so far, we have used the default configuration of LlamaIndex – which is based on OpenAI models. But, as we already briefly discussed in <a href="#_idTextAnchor045"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Kickstarting Your Journey with LlamaIndex</em>, there are both good reasons and enough options available to choose other models – both commercial variants offered by established companies in this market, and open source models, which can be hosted locally, offering private alternatives, and substantially reducing the costs of a <span class="No-Break">large-scale implementation.</span></p>
			<p>But first, <span class="No-Break">some background.</span></p>
			<h3 id="f_14__idParaDest-202" data-type="sect2" class="sect2" title2="How LLaMA and LLaMA 2 changed the open source landscape" no2="9.2.1"><a id="_idTextAnchor201"></a>9.2.1. How LLaMA and LLaMA 2 changed the open source landscape</h3>
			<p>In early 2023, Meta AI introduced the <strong class="bold">Large Language Model Meta AI</strong> (<strong class="bold">LLaMA</strong>) family, offering<a id="_idIndexMarker932"></a> a notable leap in accessibility for LLMs<a id="_idIndexMarker933"></a> by releasing model weights<a id="_idIndexMarker934"></a> to the research community. Following this, LLaMA 2 was launched in July 2023, with improvements such as increased data for training and expanded model sizes, alongside models fine-tuned for dialogue under less restrictive commercial use conditions. Meta developed and launched three versions of LLaMA 2 with 7, 13, and 70 billion parameters, respectively. While the basic structure of these models stayed similar to the original LLaMA versions, they were trained with 40% additional data compared to the original models, in order to enhance their <span class="No-Break">foundational capabilities.</span></p>
			<p>Despite some controversy regarding its open source status, the initiative marked a significant contribution to the open source ecosystem, triggering a new wave of community-based research and application development. The model consistently showcased competitive performance in tests against other leading LLMs, proving its <span class="No-Break">advanced capabilities.</span></p>
			<p>Further down the line, these releases have led to the creation of tools such as <em class="italic">llama.cpp</em> by Georgi Gerganov (<a href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener noreferrer">https://github.com/ggerganov/llama.cpp</a>), enabling the operation of these sophisticated models on more modest hardware, thus democratizing access to cutting-edge <span class="No-Break">AI technologies.</span></p>
			<p class="callout-heading">Quick note</p>
			<p class="callout"><em class="italic">llama.cpp</em> is an efficient C/C++ implementation<a id="_idIndexMarker935"></a> of Meta’s LLaMA architecture for LLM inference. Hugely popular in the open source community, with more than 43,000 stars on GitHub and over 930 releases, this foundational framework has sparked the development of many other similar tools and services such as Ollama, Local.AI, and others. These updates and advances signaled that AI research was changing, focusing more on making information freely available and making sure AI models can run on simpler computers and other edge devices. This opened <a id="_idIndexMarker936"></a>up more possibilities for using <strong class="bold">generative AI</strong> (<strong class="bold">GenAI</strong>) and encouraged new ideas and <span class="No-Break">improvements everywhere.</span></p>
			<p>I won’t go into a detailed discussion<a id="_idIndexMarker937"></a> of all the currently available<a id="_idIndexMarker938"></a> tools for running local LLMs. This is because there is already a plethora of available methods by which various open source models can be run on the local system. And not just local LLMs: there’s also an increasing number of service providers offering access either to their own proprietary AI models or providing cloud-hosted access to open source models, and the good news is that LlamaIndex already provides built-in support for many of them. You can always consult the official documentation of the framework for a detailed overview of the supported models, along with examples<a id="_idIndexMarker939"></a> of how they can be <span class="No-Break">used: </span><a href="https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules.html" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules.html</span></a><span class="No-Break">.</span></p>
			<p>Instead, I will try to offer you an alternative that I personally find very convenient for two important reasons: it is very easy to implement, and your existing code can be reused with only a few minimal changes. For beginner coders and tinkerers wanting to quickly experiment with an idea<a id="_idIndexMarker940"></a> or build simple<a id="_idIndexMarker941"></a> prototypes, this may be one of the <span class="No-Break">best solutions.</span></p>
			<h3 id="f_14__idParaDest-203" data-type="sect2" class="sect2" title2="Running a local LLM using LM Studio" no2="9.2.2"><a id="_idTextAnchor202"></a>9.2.2. Running a local LLM using LM Studio</h3>
			<p>Built on top<a id="_idIndexMarker942"></a> of the <strong class="source-inline">llama.cpp</strong> library, <strong class="bold">LM Studio</strong> (<a href="https://lmstudio.ai/" target="_blank" rel="noopener noreferrer">https://lmstudio.ai/</a>) provides a very user-friendly <a id="_idIndexMarker943"></a>graphical interface<a id="_idIndexMarker944"></a> for LLMs. It allows us to download, configure, and locally run almost any open source model available on Hugging Face. A great resource, especially for non-technical users, LM Studio offers two ways of interacting with a local LLM: through a chat UI similar to OpenAI’s ChatGPT or via an OpenAI-compatible local server. This second option makes it particularly useful because we can easily adapt any LlamaIndex application natively designed to use OpenAI’s LLMs with very few modifications. We’ll get to that in a moment, but first, let’s see how to get things started with <span class="No-Break">LM Studio.</span></p>
			<p>To start using this tool, you’ll first have to download and install the right version, depending on your operating system. Releases are available for Mac, Windows, and Linux. The installation steps are self-explanatory and well documented on <span class="No-Break">their website.</span></p>
			<p>Once installed, the LM Studio GUI starts with a <strong class="bold">Model Discovery</strong> screen where you can type any model or model family name and get a list of matching<a id="_idIndexMarker945"></a> model builds available for download. We’ll use the popular <strong class="bold">Zephyr-7B</strong> model for our example (<a href="https://huggingface.co/HuggingFaceH4/zephyr-7b-beta" target="_blank" rel="noopener noreferrer">https://huggingface.co/HuggingFaceH4/zephyr-7b-beta</a>). I have specifically chosen Zephyr because, albeit a compact model, it demonstrates the effectiveness of distilling<a id="_idIndexMarker946"></a> an LLM into a more manageable size. Derived from <strong class="bold">Mistral-7B</strong>, Zephyr-7B establishes a new benchmark for chat models with 7 billion<a id="_idIndexMarker947"></a> parameters, surpassing the performance of <strong class="bold">LLAMA2-CHAT-70B</strong> on the Hugging<a id="_idIndexMarker948"></a> Face <em class="italic">LMSYS Chatbot Arena Leaderboard </em> (<a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard" target="_blank" rel="noopener noreferrer">https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard</a>). <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.1</em> shows a typical output when searching for the <span class="No-Break"><strong class="source-inline">zephyr-7b</strong></span><span class="No-Break"> keyword:</span></p>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/B21861_09_1.jpg" alt="Figure 9.1 – LM Studio screenshot displaying search results" width="1650" height="994" data-type="figure" id="untitled_figure_69" title2="– LM Studio screenshot displaying search results" no2="9.1">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – LM Studio screenshot displaying search results</p>
			<p>In the search results screen, you’ll see <span class="No-Break">two panels:</span></p>
			<ul>
				<li>The one on the left contains all models that match your search query. In our case, these are different builds of the <span class="No-Break">Zephyr-7B model</span></li>
				<li>The right panel lists all the <strong class="bold">Generative Pre-trained Transformer-Generated Unified Format</strong> (<strong class="bold">GGUF</strong>) file versions available<a id="_idIndexMarker949"></a> <span class="No-Break">for download</span></li>
			</ul>
			<p class="callout-heading">About GGUF files</p>
			<p class="callout">GGUF is a specific file format<a id="_idIndexMarker950"></a> used for storing models for inference. Enhancing model sharing and usage efficiency, this format has quickly become a popular way of storing and distributing models throughout the open <span class="No-Break">source community.</span></p>
			<p>For most models, you’ll get an entire<a id="_idIndexMarker951"></a> list of GGUF files <a id="_idIndexMarker952"></a>available. Each one will have its own characteristics, but probably<a id="_idIndexMarker953"></a> the most important characteristic is the <span class="No-Break"><strong class="bold">quantization </strong></span><span class="No-Break">level.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Understanding LLM quantization" no2="9.2.2.1">9.2.2.1. Understanding LLM quantization</h4>
			<p>Running an open source LLM<a id="_idIndexMarker954"></a> on typical consumer hardware can prove challenging mainly because of its large memory footprint and high computational requirements. While some consumer-grade GPUs can aid in this regard, they may not be as effective as enterprise-level hardware in handling the demands of LLMs. That’s why we need quantization. The goal of applying quantization – a post-training optimization technique  – to an AI model is to optimize it for better performance and efficiency, particularly in terms of speed and memory usage, without significantly compromising its accuracy or <span class="No-Break">output quality.</span></p>
			<p>The quantization process achieves this by converting the model’s parameters – typically stored as 32-bit floating-point<a id="_idIndexMarker955"></a> numbers – to lower-bit representations, such as <strong class="bold">16-bit floating-point</strong> (<strong class="source-inline">FP16</strong>), <strong class="bold">8-bit integers</strong> (<strong class="source-inline">INT8</strong>), or even lower. It’s a kind<a id="_idIndexMarker956"></a> of approximation process that works by reducing the numerical precision used to represent the model’s parameters, combined with complex techniques to maintain as much accuracy as possible. Modern quantization techniques are designed to minimize accuracy loss, often resulting in models that are nearly as accurate as their <span class="No-Break">full-precision counterparts.</span></p>
			<p class="callout-heading">A simple analogy to help you better understand the concept</p>
			<p class="callout">Imagine you have a recipe that calls for very precise measurements, such as <strong class="source-inline">1.4732</strong> cups of flour. In practice, you might round this to 1.5 cups, as the difference is negligible in most cases and the difference will not affect the end result. This is similar to quantization, where we reduce the precision of the model’s parameters to make the model more efficient while maintaining acceptable accuracy. But instead of cups of flour, we reduce the numerical precision of the model’s parameters. Instead of using 16 bits to store a parameter as 23.7, we could quantize it into 8 bits as 23. This directly translates to less memory usage and faster processing times. However, there is a trade-off between model size, speed, <span class="No-Break">and accuracy.</span></p>
			<p>With an acceptable loss of accuracy, this process can significantly reduce the size of the model and the computational resources required for both training and inference phases, making it more feasible to deploy these models on consumer hardware. Generally, the lower the bit representation (such as <strong class="source-inline">INT4</strong> or even binary), the smaller and faster the model becomes, but at a higher risk of <span class="No-Break">accuracy loss.</span></p>
			<p>Being built on top of llama.cpp, LM Studio can take advantage of any compatible GPUs that could be used during the inference process. This feature is commonly called <em class="italic">GPU offloading</em> and means that computing operations<a id="_idIndexMarker957"></a> can be partially or even entirely transferred from the CPU to the GPU. Given the fact that a modern GPU is capable of handling highly parallel computing tasks more efficiently than CPUs, this can dramatically speed up the inference process. It also reduces the load on the CPU, thus providing an overall balanced improvement of system performance. The main limitation when attempting GPU offloading is the amount of video memory available on your GPU. In order to run efficiently, the GPU must load the model in the video <span class="No-Break">memory first.</span></p>
			<p>Because of this, apart from the quantization<a id="_idIndexMarker958"></a> level, the GGUF files in the right panel will also have a flag showing three possible compatibility scenarios, each represented by a <span class="No-Break">different color:</span></p>
			<ul>
				<li><strong class="bold">Green</strong>: This means your GPU has enough video memory to load the model and execute the inference. In most cases, this is the <span class="No-Break">ideal scenario</span></li>
				<li><strong class="bold">Blue</strong>: Not ideal, but still provides a considerable uplift <span class="No-Break">in performance</span></li>
				<li><strong class="bold">Gray</strong>: This may or may not work depending on the <span class="No-Break">model architecture</span></li>
				<li><strong class="bold">Red</strong>: Unfortunately, this means you won’t be able to run this version on your machine, the most probable reason being that its size exceeds your total <span class="No-Break">system memory</span></li>
			</ul>
			<p class="callout-heading">Pro tip</p>
			<p class="callout">A very handy tool for approximating the required VRAM for a particular model given a particular quantization level<a id="_idIndexMarker959"></a> can be found on the Hugging Face <span class="No-Break">website: </span><a href="https://huggingface.co/spaces/hf-accelerate/model-memory-usage" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://huggingface.co/spaces/hf-accelerate/model-memory-usage</span></a></p>
			<h4 data-type="sect3" class="sect3" title2="So, which model should you choose?" no2="9.2.2.2">9.2.2.2. So, which model should you choose?</h4>
			<p>The general rule of thumb<a id="_idIndexMarker960"></a> is that with a lower quantization level, less memory will be required and the inference process will be faster. The trade-off is decreased accuracy. For example, a 3-bit quantization will always result in less accuracy than a <span class="No-Break">6-bit quantization.</span></p>
			<p>Once you’ve made a decision on the exact model version, the next step is to download the model on your machine. But first, make sure you have the necessary space on your hard drive. There’s a handy status <a id="_idIndexMarker961"></a>bar on the bottom of the UI to monitor the status of <span class="No-Break">the download.</span></p>
			<p>After the download is complete, moving to the <strong class="bold">Chats</strong> screen will display something similar to <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="image/B21861_09_2.jpg" alt="Figure 9.2 – LM Studio’s chat UI" width="1334" height="809" data-type="figure" id="untitled_figure_70" title2="– LM Studio’s chat UI" no2="9.2">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – LM Studio’s chat UI</p>
			<p>This is the interaction method that I mentioned at the beginning of this section – the one resembling the ChatGPT interface. In this screen, you’ll be able to do <span class="No-Break">the following:</span></p>
			<ol>
				<li>Select the desired AI model from a list of all downloaded ones. To choose your model, use the <em class="italic">model selector</em> on top of the screen. You’ll have to wait for a few moments until the model is loaded <span class="No-Break">into memory.</span></li>
				<li>Configure any available parameters of the model using the <em class="italic">configuration panel </em>on the right side. We’ll talk in more detail about that in <span class="No-Break">a moment.</span></li>
				<li>See a list of previous chats on the <span class="No-Break">left side.</span></li>
				<li>Chat with the model using a familiar interface inspired <span class="No-Break">by ChatGPT.</span></li>
			</ol>
			<p>There are a number of parameters that you can tweak in the configuration panel. The most important ones are <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Preset</strong>: Some models come with predefined configurations that you can load from presets. For an easy start, I would recommend selecting the model’s specific preset from the list. For example, there is a Zephyr preset that can be used with all <span class="No-Break">Zephyr-based models</span></li>
				<li><strong class="bold">System Prompt</strong>: This prompt will set the initial context of <span class="No-Break">the conversation</span></li>
				<li><strong class="bold">GPU Offload</strong>: Allows you to configure the number of model layers to be offloaded to the GPU. Depending on the model you’re using and your available GPU, you may want to gradually experiment with increasing values while checking for model stability. Higher values can sometimes produce errors. If you feel confident, use -1 to offload all the model’s layers to <span class="No-Break">the GPU</span></li>
				<li><strong class="bold">Context Length</strong>: Allows you to define the maximum context window to <span class="No-Break">be used</span></li>
			</ul>
			<p>Changing some of these parameters may trigger a model reload, so you’ll have to be patient until it completes<a id="_idIndexMarker962"></a> the process. Once you have customized everything, the floor is yours – enjoy chatting with your <span class="No-Break">local LLM.</span></p>
			<h4 data-type="sect3" class="sect3" title2="So far, so good, but where’s the RAG part in all this?" no2="9.2.2.3">9.2.2.3. So far, so good, but where’s the RAG part in all this?</h4>
			<p>For that, we’ll have to go<a id="_idIndexMarker963"></a> to the <strong class="bold">Local Inference Server</strong> screen, which you can do by pressing the double-arrow icon on the left-side menu. You’ll be presented with a UI similar to <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="image/Image96391.jpg" alt="Figure 9.3 – The local Inference Server interface in LM Studio" width="1326" height="802" data-type="figure" id="untitled_figure_71" title2="– The local Inference Server interface in LM Studio" no2="9.3">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – The local Inference Server interface in LM Studio</p>
			<p>The configuration options from the right-side panel<a id="_idIndexMarker964"></a> are almost identical to the ones in the <strong class="bold">Chat</strong> screen. In the beginning, you can leave the <em class="italic">server configuration</em> options as default. The <em class="italic">usage</em> section tells you how to interact with the API. One of the great aspects of LM Studio is that it emulates the OpenAI API. That means your already existing code will need very few changes to work with a local LLM hosted through <span class="No-Break">LM Studio.</span></p>
			<p>All you have to do at this point is to click the <strong class="bold">Start Server</strong> button, and you’re good <span class="No-Break">to go.</span></p>
			<p class="callout-heading">Quick note</p>
			<p class="callout">Please keep in mind that while the API server is running, the chat UI will be disabled, so you won’t be able to use both at the <span class="No-Break">same time.</span></p>
			<p>Let’s see exactly what we need to change in our code if we want to port it to a local LLM using this method. If we look at the recommendation in the <em class="italic">usage</em> section, we’ll see that a single change <span class="No-Break">is necessary:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_265" title2="(no caption)" no2="">client = OpenAI(base_url="http://localhost:1234/v1")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>However, because LlamaIndex has its own implementation of the OpenAI API client, in our case, we’ll have to use the <strong class="source-inline">api_base</strong> parameter <span class="No-Break">like this:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_266" title2="(no caption)" no2="">from llama_index.llms.openai import OpenAI
llm = OpenAI(
&nbsp;&nbsp;&nbsp;&nbsp;api_base='http://localhost:1234/v1',
&nbsp;&nbsp;&nbsp;&nbsp;temperature=0.7
)
print(llm.complete('Who is Lionel Messi?'))</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>As you can see, the only real change<a id="_idIndexMarker965"></a> we have to make is pointing the <strong class="source-inline">llm</strong> instance toward our local server instead of the OpenAI one. The rest of the code remains unchanged. After running this example, you’ll see actual requests coming from our code and responses coming from the API in LM Studio’s log screen. If you want to permanently reconfigure the LLM in the entire code, you’ll have to define a <strong class="source-inline">Settings</strong> object and use it to configure global settings, as I showed you in <a href="#_idTextAnchor045"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>,<em class="italic"> Kickstarting Your Journey with LlamaIndex</em>, in the <em class="italic">Customizing the LLM used by </em><span class="No-Break"><em class="italic">LlamaIndex</em></span><span class="No-Break"> section.</span></p>
			<p>Neat, isn’t it? Our data is now completely private, and we don’t have to pay for using an AI model in our RAG workflows anymore. Of course, there’s still a cost, albeit in electricity rather than tokens. The capability to run local models on modest hardware unlocks numerous possibilities that extend beyond mere text generation. This includes the opportunity to embrace<a id="_idIndexMarker966"></a> fully multimodal experiences with models such as <strong class="bold">LLaVa</strong>  (<a href="https://huggingface.co/docs/transformers/main/en/model_doc/llava" target="_blank" rel="noopener noreferrer">https://huggingface.co/docs/transformers/main/en/model_doc/llava</a>), allowing for a wider range of applications: a wonderful tool that serves as an excellent resource for rapid prototyping or exploring <span class="No-Break">diverse ideas.</span></p>
			<p>However, keep in mind that LM Studio<a id="_idIndexMarker967"></a> is governed by a licensing model, which restricts its use to personal, non-commercial purposes. To utilize LM Studio for commercial applications, obtaining permission from the developers <span class="No-Break">is necessary.</span></p>
			<h3 id="f_14__idParaDest-204" data-type="sect2" class="sect2" title2="Routing between LLMs using services such as Neutrino or OpenRouter" no2="9.2.3"><a id="_idTextAnchor203"></a>9.2.3. Routing between LLMs using services such as Neutrino or OpenRouter</h3>
			<p>Sometimes, a single LLM <a id="_idIndexMarker968"></a>may not be ideal for every single<a id="_idIndexMarker969"></a> interaction. In complex RAG scenarios, finding the best mix between cost, latency, and precision could prove to be a difficult task when forced to choose a single LLM for everything. But what if we could find a way to mix different LLMs in the same app and dynamically choose which one to use for each individual interaction? That<a id="_idIndexMarker970"></a> is the exact purpose of third-party services such as <strong class="bold">Neutrino</strong> (<a href="https://www.neutrinoapp.com/" target="_blank" rel="noopener noreferrer">https://www.neutrinoapp.com/</a>) and <strong class="bold">OpenRouter</strong> (<a href="https://openrouter.ai/" target="_blank" rel="noopener noreferrer">https://openrouter.ai/</a>). These types of services<a id="_idIndexMarker971"></a> can significantly enhance a RAG workflow by providing intelligent routing capabilities for queries across <span class="No-Break">different LLMs.</span></p>
			<p>Neutrino’s smart model router, for<a id="_idIndexMarker972"></a> instance, allows you to intelligently route queries to the most suited LLM for the prompt, optimizing both response quality and cost efficiency. This can be particularly useful in a RAG workflow where different types of queries may require different LLM strengths or specialties. For example, one model might be more effective at understanding and parsing the initial user query, while another might be better suited for generating responses based on retrieved documents. By employing a router, we can dynamically select the most suitable model for each task without hardcoding model choices into our application, thus enhancing flexibility and potentially improving the overall performance of our RAG system. <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.4</em> describes the working mechanism of a <span class="No-Break">Neutrino router:</span></p>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/B21861_09_4.jpg" alt="Figure 9.4 – A diagram of the Neutrino smart routing feature" width="1642" height="1013" data-type="figure" id="untitled_figure_72" title2="– A diagram of the Neutrino smart routing feature" no2="9.4">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 – A diagram of the Neutrino smart routing feature</p>
			<p>The great news is that both Neutrino<a id="_idIndexMarker973"></a> and OpenRouter are supported as integration packages<a id="_idIndexMarker974"></a> in LlamaIndex. Let’s have a look at a simple example that uses a custom<a id="_idIndexMarker975"></a> Neutrino router to dynamically<a id="_idIndexMarker976"></a> choose between different LLMs depending on the user query. To run this example, make sure you first install the Neutrino integration package by running the <span class="No-Break">following command:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_267" title2="(no caption)" no2="">pip install llama-index-llms-neutrino</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Once the package is installed, you should first sign up for an account and obtain an API key on the Neutrino website. The next step is to create an LLM router by selecting your desired LLMs as well as a <em class="italic">fallback</em> LLM. The fallback model will be used by default in case of errors or whenever the router cannot determine which LLM to use. During the router setup, you will also have the option of choosing to use Neutrino as a provider for the AI models or utilize your own API keys for each LLM. The last step in the router setup process requires you to provide a <em class="italic">router ID</em>. This ID will be used in the code to specify the router used by <span class="No-Break">the service.</span></p>
			<p>Here is how we can use the Neutrino router <span class="No-Break">in LlamaIndex:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_268" title2="(no caption)" no2="">from llama_index.core.llms import ChatMessage
from llama_index.llms.neutrino import Neutrino
llm = Neutrino(
&nbsp;&nbsp;&nbsp;&nbsp;api_key="&lt;your-Neutrino_API_key&gt;",
&nbsp;&nbsp;&nbsp;&nbsp;router="&lt;Neutrino-router_ID&gt;"
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The code first initializes the Neutrino router in the form of a LlamaIndex <strong class="source-inline">llm</strong> object, for which you’ll need to provide your Neutrino API key and the ID of the router you have defined. Next, it runs in a loop, continually taking questions from the user until the <strong class="source-inline">'exit'</strong> keyword <span class="No-Break">is received:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_269" title2="(no caption)" no2="">while True:
&nbsp;&nbsp;&nbsp;&nbsp;user_message = input("Ask a question: ")
&nbsp;&nbsp;&nbsp;&nbsp;if user_message.lower() == 'exit':
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("Exiting chat...")
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;break
&nbsp;&nbsp;&nbsp;&nbsp;response = llm.complete(user_message)
&nbsp;&nbsp;&nbsp;&nbsp;print(f"LLM answer: {response}")
&nbsp;&nbsp;&nbsp;&nbsp;print(f"Answered by: {response.raw['model']}")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The questions are submitted<a id="_idIndexMarker977"></a> to the Neutrino router, and, in return, the script<a id="_idIndexMarker978"></a> not only prints<a id="_idIndexMarker979"></a> the answer <a id="_idIndexMarker980"></a>but also the name of the LLM that was chosen by the router to generate the answer. You can play around and experiment with different types of questions. Based on whichever models you selected when you defined the router, you’ll see that it will send the questions to different LLMs, depending on their capabilities. Another, more general approach in using such a router would be to use the <strong class="source-inline">Settings</strong> class to create a global configuration using that <span class="No-Break"><strong class="source-inline">llm</strong></span><span class="No-Break"> object:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_270" title2="(no caption)" no2="">from llama_index.core import Settings
Settings.llm = llm</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>This has the advantage that it configures every subsequent LlamaIndex component in our code to use the <span class="No-Break">Neutrino router.</span></p>
			<p class="callout-heading">Pro tip</p>
			<p class="callout">If you’re not entirely satisfied with the decisions made by the router, Neutrino also gives you the ability to fine-tune your defined router by uploading a list of examples on which the router can be <span class="No-Break">trained: </span><a href="https://platform.neutrinoapp.com/training-studio" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://platform.neutrinoapp.com/training-studio</span></a></p>
			<p>And Neutrino is just one example. OpenRouter works in a similar way, but it’s mostly focused on optimizing the cost of LLM calls, not necessarily <span class="No-Break">the quality.</span></p>
			<p>There are also other providers<a id="_idIndexMarker981"></a> offering similar<a id="_idIndexMarker982"></a> services, and the concept<a id="_idIndexMarker983"></a> is bound to become more and more popular as hundreds of new AI models<a id="_idIndexMarker984"></a> emerge every week. The ability to use LLM routing services enhances the RAG workflow by abstracting the complexity of model selection and management. As a result, we can focus on building and optimizing our applications instead of managing the underlying <span class="No-Break">AI models.</span></p>
			<h3 id="f_14__idParaDest-205" data-type="sect2" class="sect2" title2="What about customizing embedding models?" no2="9.2.4"><a id="_idTextAnchor204"></a>9.2.4. What about customizing embedding models?</h3>
			<p>Another important component<a id="_idIndexMarker985"></a> that can be considered for customization in a RAG scenario is the underlying embedding model. Intensively used in scenarios where vector store indexes are employed, the embedding model can also be a source of concern regarding cost and privacy. That is why we may sometimes prefer using a local model in our RAG workflow. Again, the good news is that LlamaIndex provides out-of-the-box support for more than 30 embedding models. They can be used by installing embedding <a id="_idIndexMarker986"></a>integration packages, documented on the <em class="italic">LlamaHub</em> <span class="No-Break">website: </span><a href="https://llamahub.ai/?tab=embeddings" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://llamahub.ai/?tab=embeddings</span></a><span class="No-Break">.</span></p>
			<p>You can find a very simple example of how to configure LlamaIndex to use a local embedding model from Hugging Face in <a href="#_idTextAnchor104"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Indexing with LlamaIndex</em>, in the<em class="italic"> Understanding </em><span class="No-Break"><em class="italic">embeddings</em></span><span class="No-Break"> section.</span></p>
			<h3 id="f_14__idParaDest-206" data-type="sect2" class="sect2" title2="Leveraging the Plug and Play convenience of using Llama Packs" no2="9.2.5"><a id="_idTextAnchor205"></a>9.2.5. Leveraging the Plug and Play convenience of using Llama Packs</h3>
			<p>The fact that LlamaIndex<a id="_idIndexMarker987"></a> offers us such a rich framework<a id="_idIndexMarker988"></a> of low-level elements and methods for RAG is a double-edged sword. On the one hand, it is extremely useful to have a tool available for almost any practical problem you have to solve. On the other hand, to successfully implement these tools, we must first spend a fair amount of time familiarizing ourselves with each one. Then comes the fine-tuning<a id="_idIndexMarker989"></a> and optimization phase<a id="_idIndexMarker990"></a> for each component. We are already talking about a significant effort in the development and optimization process. Sometimes, in order to be able to test an idea with a rapid prototype, it would be preferable if we already had some advanced ready-made modules. Imagine some <em class="italic">Lego</em> pieces already structured into functional sub-assemblies: a roof, a window, a bus stop, and so on. Well, we have that <span class="No-Break">to hand.</span></p>
			<p>Created and continually improved by the flourishing LlamaIndex community, <strong class="bold">Llama Packs</strong> are pre-packaged modules that can be used to quickly build LLM applications. Just like some pre-built Lego structures, they provide reusable components such as LLMs, embedding models, and vector indexes that have been preconfigured to work together for various use cases in building a RAG pipeline. They are ready-to-use modules that can be downloaded and initialized with parameters to achieve a specific goal outside of <span class="No-Break">the box.</span></p>
			<p class="callout-heading">Example</p>
			<p class="callout">A pack could contain a full RAG pipeline to enable semantic search over text or an entire agent construct that could be immediately invoked in <span class="No-Break">our app.</span></p>
			<p>Llama Packs act as templates that can be inspected, customized, and extended as needed. The code for each pack is available, so developers can modify it or take inspiration to build their own applications. The beauty of this concept is that it provides <strong class="bold">Plug and Play</strong> (<strong class="bold">PnP</strong>) solutions without bloating the main code base of the framework. You can still use various integration packages together with the core components of LlamaIndex, and you can definitely customize any of these packs according to <span class="No-Break">your needs.</span></p>
			<p>You’ll find a collection of all the published Llama Packs, together with all the other integration packages, available on LlamaHub (<a href="https://llamahub.ai/?tab=llama_packs" target="_blank" rel="noopener noreferrer">https://llamahub.ai/?tab=llama_packs</a>). There’s a <em class="italic">README</em> file for each pack that provides details about its usage, and most of them also have detailed examples that you can follow and <span class="No-Break">experiment with.</span></p>
			<p>Using them is very straightforward. Because, in this section, we talk about customizations in general and, among other options, moving our RAG workflows to local, open source models, I’m going to show you an example in the same line. We’ll explore a Llama Pack that allows for the creation of a query engine that relies entirely on locally hosted AI models. The pack implements <strong class="source-inline">HuggingFaceH4/zephyr-7b-beta</strong> as the LLM used for inference and <strong class="source-inline">BAAI/bge-base-en-v1.5</strong> as the embedding model. The pack<a id="_idIndexMarker991"></a> is called Zephyr Query Engine Pack, and<a id="_idIndexMarker992"></a> you can find it <span class="No-Break">here: </span><a href="https://llamahub.ai/l/llama_packs-zephyr_query_engine" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://llamahub.ai/l/llama_packs-zephyr_query_engine</span></a><span class="No-Break">.</span></p>
			<p>In a similar way to how LM Studio works, this pack can leverage existing GPUs to accelerate the inference process. Let’s see how <span class="No-Break">it works.</span></p>
			<p>The first step in using any Llama Pack<a id="_idIndexMarker993"></a> is to download the actual modules<a id="_idIndexMarker994"></a> on your local environment. This can be accomplished in three <span class="No-Break">different ways:</span></p>
			<ul>
				<li>By installing the corresponding integration package. In our example, that would be accomplished with the <span class="No-Break">following command:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="terminal" id="untitled_terminal_8" title2="(no caption)" no2=""><strong class="bold">pip install llama-index-packs-zephyr-query-engine</strong></pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div><p class="list-inset">This method is simple and permanently installs the required pack into your local environment. Its only disadvantage is that you cannot inspect and modify the pack code. For that purpose, the other two methods <span class="No-Break">are recommended.</span></p></li>				<li>By using the <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>). Here’s <span class="No-Break">an </span><span class="No-Break"><a id="_idIndexMarker995"></a></span><span class="No-Break">example:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="terminal" id="untitled_terminal_9" title2="(no caption)" no2=""><strong class="bold">llamaindex-cli download-llamapack ZephyrQueryEnginePack --download-dir ./zephyr_pack</strong></pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div><p class="list-inset">We’ll discuss the CLI tool in more detail in the <span class="No-Break">next section.</span></p></li>				<li>Directly in the code, using the <strong class="source-inline">download_llama_pack()</strong> method and specifying a download location <span class="No-Break">like this:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_271" title2="(no caption)" no2="">from llama_index.llama_pack import download_llama_pack
download_llama_pack(
&nbsp;&nbsp;&nbsp;&nbsp;"ZephyrQueryEnginePack", "./zephyr_pack"
)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>			</ul>
			<p>Once downloaded into your local environment, the pack contents will be stored in a subfolder called <strong class="source-inline">zephyr_pack</strong>. You can inspect and modify anything in the code, adjusting it to your own needs. You will also need to install the Hugging Face <strong class="source-inline">embeddings</strong> integration package before running <span class="No-Break">the example:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_272" title2="(no caption)" no2="">pip install llama-index-embeddings-huggingface</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Here’s a simple example of how to use this pack <span class="No-Break">after downloading:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_273" title2="(no caption)" no2="">from zephyr_pack.base import ZephyrQueryEnginePack
from llama_index.readers import SimpleDirectoryReader
reader = SimpleDirectoryReader('files')
documents = reader.load_data()
zephyr_qe = ZephyrQueryEnginePack(documents)
response=zephyr_qe.run(
&nbsp;&nbsp;&nbsp;&nbsp;"Enumerate famous buildings in ancient Rome"
&nbsp;&nbsp;&nbsp;&nbsp;)
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Notice that we’re using the <strong class="source-inline">run()</strong> method, which, in this case, is a wrapper for the <strong class="source-inline">query()</strong> method used by the regular <span class="No-Break">query engine.</span></p>
			<p>This is just one of the more than 50 packs already available on LlamaHub at this moment. And the number keeps growing. The great news is that all of them are well documented and follow pretty much the same implementation model. So, next time you’re faced with a practical scenario that needs combining low-level components into more advanced elements, instead of reinventing the wheel, I encourage you to spend some time browsing LlamaHub for a potential ready-made solution for your problem. Llama Packs accelerates LLM app development<a id="_idIndexMarker996"></a> by letting developers tap into pre-built components tailored for common<a id="_idIndexMarker997"></a> use cases. Both ready-made solutions and customizable templates are available to <span class="No-Break">kickstart projects.</span></p>
			<h3 id="f_14__idParaDest-207" data-type="sect2" class="sect2" title2="Using the Llama CLI" no2="9.2.6"><a id="_idTextAnchor206"></a>9.2.6. Using the Llama CLI</h3>
			<p>Another very useful tool in the LlamaIndex<a id="_idIndexMarker998"></a> arsenal is the <strong class="source-inline">llamaindex-cli</strong> utility. Installed together with the LlamaIndex libraries, the tool can be accessed very easily from the command line and can be used for various purposes, including <span class="No-Break">the following:</span></p>
			<ul>
				<li>Downloading Llama Packs, as seen in the previous section. The syntax to download a Llama Pack is given <span class="No-Break">as follows:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="terminal" id="untitled_terminal_10" title2="(no caption)" no2=""><strong class="bold">llamaindex-cli download-llamapack &lt;pack_name&gt; --download-dir &lt;target_location&gt;</strong></pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>				<li>Upgrading source code from versions older than LlamaIndex v.<strong class="source-inline">0.10</strong>. Due to the fact that version 0.10 brought many changes related to the code structure and how to use certain modules in the framework, the authors of LlamaIndex provided developers with this automatic upgrade tool. Basically, it automatically modifies the code written on older versions and updates it to the new structure introduced with v0.10 for an easier transition. The syntax used for this feature is the following to process all sources in a given <span class="No-Break">folder simultaneously:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="terminal" id="untitled_terminal_11" title2="(no caption)" no2=""><strong class="bold">llamaindex-cli upgrade &lt;target_directory&gt;</strong></pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div><p class="list-inset">Or execute the following command to upgrade a <span class="No-Break">single file:</span></p><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="terminal" id="untitled_terminal_12" title2="(no caption)" no2=""><strong class="bold">llamaindex-cli upgrade-file &lt;target_file&gt;</strong></pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>				<li>By far the most interesting capability is enabled by using the <strong class="source-inline">rag</strong> argument. This feature allows you to build a RAG workflow directly from the command line without having to write any code. By default, the command-line RAG mode uses local storage for embeddings based on a Chroma DB database and OpenAI’s GPT-3.5 Turbo model as LLM. For privacy reasons, keep in mind that this means that all data we upload will be sent to OpenAI <span class="No-Break">by default.</span></li>
			</ul>
			<h4 data-type="sect3" class="sect3" title2="How RAG works in the command line" no2="9.2.6.1">9.2.6.1. How RAG works in the command line</h4>
			<p>Before we can use the RAG<a id="_idIndexMarker999"></a> mode from the command<a id="_idIndexMarker1000"></a> line, we must first install the ChromaDB library in our <span class="No-Break">local environment:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_274" title2="(no caption)" no2="">pip install chromadb</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The <strong class="source-inline">llamaindex-cli</strong> utility offers a variety of command-line parameters that enable users to interact with language models and manage local files efficiently. Here are descriptions of the most important <span class="No-Break">command-line parameters:</span></p>
			<ul>
				<li><strong class="source-inline">--help</strong>: Displays a help message, providing an overview of available commands and <span class="No-Break">their usage.</span></li>
				<li><strong class="source-inline">--files &lt;FILES&gt;</strong>: Defines the name of the file or directory from where the tool will ingest our proprietary data. The contents will be ingested and embedded into the local vector database, enabling the RAG CLI tool to index the specified files and later retrieve context from them at <span class="No-Break">query time.</span></li>
				<li><strong class="source-inline">--question &lt;QUESTION&gt;</strong>: Specifies the question you want to ask about the ingested files. Used for querying indexed content, leveraging the power of the LLM to extract information from our <span class="No-Break">proprietary data.</span></li>
				<li><strong class="source-inline">--chat</strong>: Opens a chat <strong class="bold">read-eval-print loop</strong> (<strong class="bold">REPL</strong>) for an interactive Q&amp;A session within<a id="_idIndexMarker1001"></a> the terminal. This provides a conversational interface to query the <span class="No-Break">ingested documents.</span></li>
				<li><strong class="source-inline">--verbose</strong>: Enables verbose output during execution, offering detailed information about the tool’s operations that can be useful for troubleshooting and understanding the tool’s <span class="No-Break">inner workings.</span></li>
				<li><strong class="source-inline">--clear</strong>: Clears out all currently embedded data from the local vector database. Because a Chroma database is used to store the embeddings, these will persist across the sessions. The <strong class="source-inline">--clear</strong> command is the equivalent of <span class="No-Break">a reset.</span></li>
				<li><strong class="source-inline">--create-llama</strong>: Initiates the creation of a LlamaIndex application based on the selected files. This parameter extends the tool’s functionality beyond simple Q&amp;A, enabling the development of full-stack applications with a backend and frontend, leveraging the ingested data. You’ll find a complete<a id="_idIndexMarker1002"></a> example of how to use it <span class="No-Break">here: </span><a href="https://www.npmjs.com/package/create-llama#example" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://www.npmjs.com/package/create-llama#example</span></a><span class="No-Break">.</span></li>
			</ul>
			<p>Talking about examples, let’s have a look at a simple way to have a conversation with our files using the CLI RAG feature. We’ll use the contents of the <strong class="source-inline">ch9\files</strong> folder from our GitHub repository. So, make sure you’re running this script from inside that folder, which should contain some <span class="No-Break">sample files:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_275" title2="(no caption)" no2="">llamaindex-cli rag --files files -q "What can you tell me about ancient Rome?" --verbose</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Alternatively, once the files have been ingested, for an interactive chat session with the data, you can use the <span class="No-Break">following command:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_276" title2="(no caption)" no2="">llamaindex-cli rag --chat</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>And just in case you need<a id="_idIndexMarker1003"></a> to customize the mechanics<a id="_idIndexMarker1004"></a> of the CLI RAG, a complete example can be found in the official documentation<a id="_idIndexMarker1005"></a> of the framework, <span class="No-Break">here: </span><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html</span></a><span class="No-Break">.</span></p>
			<p>Next, it’s time to dive deeper into the logic of our <span class="No-Break">LlamaIndex applications.</span></p>
			<h2 id="f_14__idParaDest-208" data-type="sect1" class="sect1" title2="Using advanced tracing and evaluation techniques" no2="9.3"><a id="_idTextAnchor207"></a>9.3. Using advanced tracing and evaluation techniques</h2>
			<p>The process of building<a id="_idIndexMarker1006"></a> an LLM-based application<a id="_idIndexMarker1007"></a> using a tool such as LlamaIndex is very developer-friendly since the framework abstracts away a lot of technical stuff. But at the same time, this complicates things, for the very same reason. When things don’t work as planned, developers need to have effective ways of understanding why. They have to peel back all these layers of abstraction in order to pinpoint the root causes. In other words, we need to be able to see the inner mechanics of our code, understand how different components interact, and be able to identify underlying issues. That’s where tracing becomes a really important feature. On the other hand, because we have so many tools available and so many ways of building our solution, we need a way to benchmark different combinations and determine the best mix of tools and orchestrations. That’s where evaluation comes into play. Evaluation is essential for comparing various tool and method combinations until we find the right configuration for our specific needs. Together, tracing and evaluation form the backbone of a successful RAG development process, ensuring both transparency and <span class="No-Break">optimal performance.</span></p>
			<p>In <a href="#_idTextAnchor045"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Kickstarting Your Journey with LlamaIndex</em>, we already discussed simple logging methods that we can use to better understand what’s happening under the hood of our LlamaIndex apps. Now, it’s time to discover a much more advanced way in which we can understand and evaluate RAG applications. In this section, I will explain advanced tracing<a id="_idIndexMarker1008"></a> and evaluation using the <strong class="bold">Phoenix framework</strong> developed by Arize AI (<a href="https://phoenix.arize.com/" target="_blank" rel="noopener noreferrer">https://phoenix.arize.com/</a>). Integrating LlamaIndex with specialized tracing and evaluation tools provides a sophisticated approach to understanding and optimizing RAG applications. Phoenix provides the necessary instrumentation together with a great visualization UI, making our RAG execution workflow really simple <span class="No-Break">to understand.</span></p>
			<p>To make use of the advanced<a id="_idIndexMarker1009"></a> capabilities<a id="_idIndexMarker1010"></a> of the Phoenix framework, we must first install some necessary libraries in <span class="No-Break">our environment:</span></p>
			<h3 id="f_14__idParaDest-209" data-type="sect2" class="sect2" title2="Tracing our RAG workflows using Phoenix" no2="9.3.1"><a id="_idTextAnchor208"></a>9.3.1. Tracing our RAG workflows using Phoenix</h3>
			<p>In Phoenix, tracing<a id="_idIndexMarker1011"></a> is built on<a id="_idIndexMarker1012"></a> the concept of <strong class="bold">spans</strong> and <strong class="bold">traces</strong>, which are fundamental for capturing the detailed execution flow of applications. A span represents a specific operation or unit of work within the application, tracking the start and end times, along with metadata that provides context about the operation. These spans are nested within traces, which aggregate multiple spans to depict the entire journey of a request through the application. This hierarchical structure allows developers to drill down into specific operations, understanding how each component contributes to the overall process. Phoenix’s tracing capabilities are designed to seamlessly integrate with LlamaIndex, enabling developers to instrument their RAG applications with <span class="No-Break">minimal effort.</span></p>
			<p>Because it features a client-server architecture, Phoenix is able to gather traces both locally and remotely. We can automatically collect telemetry data about each operation, including data ingestion, indexing, retrieval, processing, and any subsequent LLM calls. In the background, this data is collected by the Phoenix server, where it can be visualized and analyzed in <span class="No-Break">real time.</span></p>
			<p>Once the necessary requirements have been installed, using Phoenix is really easy. There are many advanced capabilities that you can explore with this framework, but I will show you the most simple and straightforward way to use it for tracing the execution of a LlamaIndex application. We’ll make use of a special method called <strong class="source-inline">set_global_handler</strong>, which conveniently configures LlamaIndex to use a certain tracing tool for every operation – in our case, the <span class="No-Break">Phoenix framework.</span></p>
			<p>Make sure you install<a id="_idIndexMarker1013"></a> the required packages before running<a id="_idIndexMarker1014"></a> <span class="No-Break">the example:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="console" data-type="example" id="untitled_example_277" title2="(no caption)" no2="">pip install "arize-phoenix[llama-index]" llama-hub html2text</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Here is <span class="No-Break">the code:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_278" title2="(no caption)" no2="">from llama_index.core import (
&nbsp;&nbsp;&nbsp;&nbsp;SimpleDirectoryReader,
&nbsp;&nbsp;&nbsp;&nbsp;VectorStoreIndex,
&nbsp;&nbsp;&nbsp;&nbsp;set_global_handler
)
import phoenix as px</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Apart from the obvious imports that will provide our basic RAG functionality, we’re also importing <strong class="source-inline">set_global_handler</strong> and the Phoenix library. The next part will be responsible for starting the Phoenix server and configuring LlamaIndex to use it as a global <span class="No-Break">callback handler:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_279" title2="(no caption)" no2="">px.launch_app()
set_global_handler("arize_phoenix")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>From now on, every single operation performed by our app will generate traces that will get collected by the Phoenix server. Let’s build a simple query engine based on a <strong class="source-inline">VectorStoreIndex</strong> index and run a <span class="No-Break">random query:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_280" title2="(no caption)" no2="">documents = SimpleDirectoryReader('files').load_data()
index = VectorStoreIndex.from_documents(documents)
qe = index.as_query_engine()
response = qe.query("Tell me about ancient Rome")
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Because we need the server to be live in order to visualize the trace, we keep the script running with <span class="No-Break">this line:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_281" title2="(no caption)" no2="">input("Press &lt;ENTER&gt; to exit...")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Now, with the script still running in the background, we can access the Phoenix UI at this URL: http://localhost:6006/. <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.5</em> shows what you’ll find in the Phoenix <span class="No-Break">server UI:</span></p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/B21861_09_5.jpg" alt="Figure 9.5 – A screenshot from the Phoenix server UI depicting our tracing output" width="1650" height="519" data-type="figure" id="untitled_figure_73" title2="– A screenshot from the Phoenix server UI depicting our tracing output" no2="9.5">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 – A screenshot from the Phoenix server UI depicting our tracing output</p>
			<p>Looking at this screenshot, we can see<a id="_idIndexMarker1015"></a> that the Phoenix server UI<a id="_idIndexMarker1016"></a> helps us visualize a complete trace of our code, divided into multiple spans. If you have successfully executed the previous sample code, our trace should consist of three different spans, each displayed on a separate line. </p>
			<p>Let’s talk about the columns in <span class="No-Break">the screenshot:</span></p>
			<ul>
				<li>The first column, <strong class="source-inline">kind</strong>, contains the type of the span. It can be <strong class="source-inline">chain</strong>, <strong class="source-inline">retriever</strong>, <strong class="source-inline">re-ranker</strong>, <strong class="source-inline">llm</strong>, <strong class="source-inline">embedding</strong>, <strong class="source-inline">tool</strong>, or <strong class="source-inline">agent</strong>. We are already familiar with what these concepts represent in LlamaIndex, except for a chain. In Phoenix, a chain can be either the starting point for a series of operations in an LLM application or a connector linking different steps within the application workflow. In our example, the screenshot contains three spans: two chains and an embedding. They are displayed in the reverse order of their operation, beginning with the <span class="No-Break">last one.</span></li>
				<li>The second column, <em class="italic">name</em>, provides a more detailed description of the span. We can see that in our example, the first span represents a <em class="italic">query</em>, the second one is an <em class="italic">embedding</em>, and the third is a <em class="italic">Node-parsing</em> operation. The logic of our code is now clear: it first parsed the ingested documents into Nodes, then created a vector index by embedding the Nodes, and the final step was to run a query against <span class="No-Break">that index.</span></li>
				<li>The next two columns, <em class="italic">input,</em> and <em class="italic">output</em>, show exactly what went as an input into the span and what was the final output produced by it. In our example, we only have values in these fields for the query span as this does not apply to the <span class="No-Break">other ones.</span></li>
				<li>The <em class="italic">evaluations</em> column displays the results of the evaluation for each span. For now, that column should be empty as we have not yet executed any evaluation. We’ll cover this topic in the <span class="No-Break">next section.</span></li>
				<li><em class="italic">start time</em> provides an exact timestamp for <span class="No-Break">each span.</span></li>
				<li><em class="italic">latency</em> measures the total execution time for each span. This is really useful when trying to optimize our code for <span class="No-Break">increased performance.</span></li>
				<li>As the name implies, <em class="italic">total tokens</em> count the total number of tokens used by the <span class="No-Break">corresponding operation.</span></li>
				<li>The last column, <em class="italic">status</em>, indicates whether the operation was completed successfully <span class="No-Break">or not.</span></li>
			</ul>
			<p>Here comes the best<a id="_idIndexMarker1017"></a> part. If we now<a id="_idIndexMarker1018"></a> click on the <em class="italic">kind</em> column of the query span – the first one in our list – we’ll get a detailed visualization, similar to the one depicted in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/B21861_09_6.jpg" alt="Figure 9.6 – Trace details visualized on the Phoenix server UI" width="1650" height="739" data-type="figure" id="untitled_figure_74" title2="– Trace details visualized on the Phoenix server UI" no2="9.6">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6 – Trace details visualized on the Phoenix server UI</p>
			<p>As you can see, we can now get a detailed understanding of each individual step performed during this span. In this case, we see a decomposed view of the query engine operation: first, the retrieval part, and then the final response synthesis using the LLM. By clicking on each individual step, we can explore its attributes and underlying mechanics. And because Phoenix runs locally, all this data <span class="No-Break">remains private.</span></p>
			<p class="callout-heading">Practical exercise</p>
			<p class="callout">Here’s a useful exercise you could attempt now. Try to reconfigure some of the samples discussed in the previous chapters to use the Phoenix framework. You’ll get a better understanding of how different components work in LlamaIndex and also have a chance to familiarize yourself with this <span class="No-Break">great tool.</span></p>
			<p>And if you want to go deeper<a id="_idIndexMarker1019"></a> and explore more advanced tracing<a id="_idIndexMarker1020"></a> features of Phoenix, you’ll find everything you need in their official <span class="No-Break">documentation: </span><a href="https://docs.arize.com/phoenix/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.arize.com/phoenix/</span></a><span class="No-Break">.</span></p>
			<p>Next, let’s talk about how we can use Phoenix to evaluate and optimize our <span class="No-Break">RAG apps.</span></p>
			<h3 id="f_14__idParaDest-210" data-type="sect2" class="sect2" title2="Evaluating our RAG system" no2="9.3.2"><a id="_idTextAnchor209"></a>9.3.2. Evaluating our RAG system</h3>
			<p>When developing LLM-based<a id="_idIndexMarker1021"></a> systems, proper evaluation is essential for checking how well a RAG pipeline works. In general, LLM applications have to deal with very diverse inputs, and there is usually not a single, absolute answer they are supposed to return. That means evaluating them can prove to be a <span class="No-Break">challenging task.</span></p>
			<p>In general, evaluating a RAG pipeline involves assessing key aspects such as <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Retrieval quality</strong>: Evaluating the relevance and effectiveness of the retrieved Nodes in providing the necessary information to answer <span class="No-Break">the query</span></li>
				<li><strong class="bold">Generation quality</strong>: Assessing the quality of the final output, including its correctness, coherence, and adherence to the <span class="No-Break">provided context</span></li>
				<li><strong class="bold">Faithfulness</strong>: Ensuring that the generated output is faithful to the retrieved information and does not introduce hallucinations <span class="No-Break">or inconsistencies</span></li>
				<li><strong class="bold">Efficiency</strong>: Measuring the computational efficiency and scalability of the RAG pipeline, especially in real-world scenarios with <span class="No-Break">large-scale datasets</span></li>
				<li><strong class="bold">Robustness</strong>: Testing the RAG system’s ability to handle diverse queries, edge cases, and potential <span class="No-Break">adversarial inputs</span></li>
			</ul>
			<p>To address these evaluation challenges, several tools and frameworks have been developed to facilitate the evaluation process. These tools aim to provide automated metrics, reference-based comparisons, and also human-in-the-loop evaluation methodologies. By leveraging these evaluation frameworks, we can obtain insights into the strengths and weaknesses of our RAG pipelines, identify areas for improvement, and iterate on their designs to enhance <span class="No-Break">overall performance.</span></p>
			<p>Because in the previous section, we’ve seen how Phoenix can help us with its tracing functionality, I’d like to continue building on the previous example and first explore some of the evaluation features provided by <span class="No-Break">this framework.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Using the Phoenix framework evaluation features" no2="9.3.2.1">9.3.2.1. Using the Phoenix framework evaluation features</h4>
			<p>Since manual labeling<a id="_idIndexMarker1022"></a> and testing evaluation data<a id="_idIndexMarker1023"></a> can be very time-consuming, Phoenix uses GPT-4 as a reference to decide on the correctness of our RAG’s answers. This framework provides out-of-the-box support for batch processing, custom datasets, and pre-tested evaluation templates. Unlike traditional, more basic evaluation libraries that lack rigor for production environments, Phoenix ensures data science rigor, high throughput, and flexibility across different environments, making it significantly faster and more adaptable for evaluating both the model and the context in which it is used. Phoenix can be used<a id="_idIndexMarker1024"></a> for evaluating two important dimensions of a RAG workflow: <strong class="bold">retrieval</strong> and <span class="No-Break"><strong class="bold">LLM inference</strong></span><span class="No-Break">.</span></p>
			<p>For retrieval, Phoenix evaluates<a id="_idIndexMarker1025"></a> the relevancy of the retrieved context. In other words,  it verifies if the retrieved Nodes actually contain an answer to the query or not. When evaluating LLM inference, the framework checks three <span class="No-Break">main attributes:</span></p>
			<ul>
				<li><strong class="bold">Correctness</strong>: This verifies if the system has accurately answered <span class="No-Break">a question</span></li>
				<li><strong class="bold">Hallucinations</strong>: This aims to identify any unrealistic or fabricated responses by the LLM in relation to the context it <span class="No-Break">was provided</span></li>
				<li><strong class="bold">Toxicity</strong>: This checks for any harmful content in the AI’s responses, including racism, bias, or <span class="No-Break">general toxicity</span></li>
			</ul>
			<p>Because a complex RAG scenario could sometimes rely on many individual spans, being able to individually evaluate each one becomes an essential feature. This way, we can isolate the source of errors and stop them from propagating further in the flow. Since it uses an LLM for running evaluations, Phoenix returns not just the test result but also an explanation provided by the model. This can be very useful for understanding the root cause of a failed evaluation and pinpointing the misbehaving component in our <span class="No-Break">RAG application.</span></p>
			<p>Let’s have a look at a simple example to understand how Phoenix can be used for evaluation. In order to minimize costs and keep the code simple, we’re going to use the previous approach we used for the tracing example. We’re going to ingest the contents of our <strong class="source-inline">ch9/files</strong> folder, create a vector index, and run a simple query against the index. In a real scenario, you would probably run these evaluators against a much larger dataset in order to cover as many edge cases as possible and increase the probability of finding underlying issues in the pipeline. Here is <span class="No-Break">an example:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_282" title2="(no caption)" no2="">from llama_index.core import (
&nbsp;&nbsp;&nbsp;&nbsp;SimpleDirectoryReader,
&nbsp;&nbsp;&nbsp;&nbsp;VectorStoreIndex,
&nbsp;&nbsp;&nbsp;&nbsp;set_global_handler
)
import phoenix as px
px.launch_app()
set_global_handler("arize_phoenix")
documents = SimpleDirectoryReader('files').load_data()
index = VectorStoreIndex.from_documents(documents)
qe = index.as_query_engine()
response = qe.query("Tell me about ancient Rome")
print(response)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>So far, the first part is identical <a id="_idIndexMarker1026"></a>to the previous<a id="_idIndexMarker1027"></a> example. It’s time to add the part responsible for the evaluation. We’ll begin by importing the necessary Phoenix components. Two functions, <strong class="source-inline">get_retrieved_documents()</strong> and <strong class="source-inline">get_qa_with_reference()</strong>, will be responsible for fetching the documents retrieved by queries and the queries with their reference answers for evaluation. We’re also importing three of the Phoenix evaluators: <strong class="source-inline">HallucinationEvaluator</strong>, <strong class="source-inline">QAEvaluator</strong>, and <strong class="source-inline">RelevanceEvaluator</strong>. These evaluators will assess the hallucination in responses, the correctness of question-answer pairs, and the relevance of retrieved documents, respectively. We also need to import <strong class="source-inline">run_evals()</strong>, which will be responsible for performing the evaluation tasks and returning DataFrames containing the evaluation results. Finally, the <strong class="source-inline">DocumentEvaluations</strong> and <strong class="source-inline">SpanEvaluations</strong> classes will be used to encapsulate evaluation results and display these results in the Phoenix <span class="No-Break">server UI:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_283" title2="(no caption)" no2="">from phoenix.session.evaluation import (
&nbsp;&nbsp;&nbsp;&nbsp;get_qa_with_reference,
&nbsp;&nbsp;&nbsp;&nbsp;get_retrieved_documents
)
from phoenix.experimental.evals import (
&nbsp;&nbsp;&nbsp;&nbsp;HallucinationEvaluator,
&nbsp;&nbsp;&nbsp;&nbsp;RelevanceEvaluator,
&nbsp;&nbsp;&nbsp;&nbsp;QAEvaluator,
&nbsp;&nbsp;&nbsp;&nbsp;OpenAIModel,
&nbsp;&nbsp;&nbsp;&nbsp;run_evals
)
from phoenix.trace import DocumentEvaluations, SpanEvaluations</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Now that the imports <a id="_idIndexMarker1028"></a>are complete, it’s time to prepare<a id="_idIndexMarker1029"></a> our evaluations. First, we declare the LLM that will be used to perform evaluations. This should always be the best <span class="No-Break">available model:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_284" title2="(no caption)" no2="">model = OpenAIModel(model="gpt-4-turbo-preview")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Once the evaluation model has been defined, it’s time to prepare our data. We’ll fetch the retrieved documents and queries in separate data frames. These data frames will later become the input for <span class="No-Break">evaluator functions:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_285" title2="(no caption)" no2="">retrieved_documents_df = get_retrieved_documents(px.Client())
queries_df = get_qa_with_reference(px.Client())</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Now that we have the data, we need to define evaluator functions and run the <span class="No-Break">actual evaluations:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_286" title2="(no caption)" no2="">hallucination_evaluator = HallucinationEvaluator(model)
qa_correctness_evaluator = QAEvaluator(model)
relevance_evaluator = RelevanceEvaluator(model)
hallucination_eval_df, qa_correctness_eval_df = run_evals(
&nbsp;&nbsp;&nbsp;&nbsp;dataframe=queries_df,
&nbsp;&nbsp;&nbsp;&nbsp;evaluators=[hallucination_evaluator, qa_correctness_evaluator],
&nbsp;&nbsp;&nbsp;&nbsp;provide_explanation=True,
)
relevance_eval_df = run_evals(
&nbsp;&nbsp;&nbsp;&nbsp;dataframe=retrieved_documents_df,
&nbsp;&nbsp;&nbsp;&nbsp;evaluators=[relevance_evaluator],
&nbsp;&nbsp;&nbsp;&nbsp;provide_explanation=True,
)[0]</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>When running the evaluators, notice<a id="_idIndexMarker1030"></a> that I’m setting<a id="_idIndexMarker1031"></a> the <strong class="source-inline">provide_explanation</strong> argument to <strong class="source-inline">True</strong>. This ensures that explanations for the evaluation scores are included in the response from the LLM. The last part involves encapsulating the results in corresponding <strong class="source-inline">SpanEvaluations</strong> and <strong class="source-inline">DocumentEvaluations</strong> classes and sending them to the Phoenix server so that they can be properly displayed in <span class="No-Break">the UI:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_287" title2="(no caption)" no2="">px.Client().log_evaluations(
&nbsp;&nbsp;&nbsp;&nbsp;SpanEvaluations(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;eval_name="Hallucination",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dataframe=hallucination_eval_df),
&nbsp;&nbsp;&nbsp;&nbsp;SpanEvaluations(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;eval_name="QA Correctness",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dataframe=qa_correctness_eval_df),
&nbsp;&nbsp;&nbsp;&nbsp;DocumentEvaluations(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;eval_name="Relevance",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dataframe=relevance_eval_df),
)
input("Press &lt;ENTER&gt; to exit...")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Just like in the previous example, the input at the end keeps the script running until the user decides to exit by pressing the <em class="italic">Enter</em> key. This allows us to view and interact with the Phoenix app before closing it. If everything went smoothly, accessing the UI at  http://localhost:6006/ should reveal an output similar to what we can see in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/Image96429.jpg" alt="Figure 9.7 – Visualizing evaluation results in the Phoenix server UI" width="1650" height="617" data-type="figure" id="untitled_figure_75" title2="– Visualizing evaluation results in the Phoenix server UI" no2="9.7">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7 – Visualizing evaluation results in the Phoenix server UI</p>
			<p>As you can see, the <em class="italic">evaluations</em> column has been updated with the values returned by the evaluators we just executed. We can now see the results, as well as the rationale for each <span class="No-Break">individual score.</span></p>
			<p>The topic of evaluating RAG apps is huge and could probably become the subject of an entirely separate book. There are many nuances and different approaches that could be considered regarding evaluation. I’ve only shown you a tool – Phoenix – but there are many other options for this purpose, including LlamaIndex’s own instrumentation. If you’re planning to explore this topic deeper, I encourage you to start by reading the LlamaIndex<a id="_idIndexMarker1032"></a> official documentation here: <a href="https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html" target="_blank" rel="noopener noreferrer">https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html</a>. Also, get a better understanding of the complete<a id="_idIndexMarker1033"></a> capabilities of the Phoenix framework<a id="_idIndexMarker1034"></a> by reading their official<a id="_idIndexMarker1035"></a> documentation <span class="No-Break">here: </span><span class="No-Break">https://docs.arize.com/phoenix/</span><span class="No-Break">.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Other alternatives for evaluation – RAGAS" no2="9.3.2.2">9.3.2.2. Other alternatives for evaluation – RAGAS</h4>
			<p>While Phoenix provides a comprehensive evaluation<a id="_idIndexMarker1036"></a> framework for RAG pipelines, there are other alternatives available. Another notable framework is <strong class="bold">Retrieval-Augmented Generation Assessment</strong> (<strong class="bold">RAGAS</strong>), which is based on the techniques introduced by Es et al. (2023) in their paper, <em class="italic">RAGAS: Automated Evaluation of Retrieval Augmented Generation</em> (<a href="https://doi.org/10.48550/arXiv.2309.15217" target="_blank" rel="noopener noreferrer">https://doi.org/10.48550/arXiv.2309.15217</a>). The RAGAS framework provides a practical implementation of these evaluation methods, along with additional features <span class="No-Break">and integrations.</span></p>
			<p>RAGAS is specifically designed for evaluating and analyzing RAG systems. It offers a standardized approach to assess various aspects of a RAG pipeline, including retrieval quality, generation quality, and the interplay between the retrieval and <span class="No-Break">generation components.</span></p>
			<p>Key features of RAGAS<a id="_idIndexMarker1037"></a> include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Retrieval evaluation</strong>: RAGAS assesses the quality of the retrieval component by measuring the relevance of the retrieved Nodes<a id="_idIndexMarker1038"></a> to the given query using metrics such as <strong class="bold">Recall@k</strong> – the proportion of relevant Nodes retrieved within the top <em class="italic">k</em> results, where k is a user-defined parameter. Another metric that measures retrieval quality is the <strong class="bold">Mean Reciprocal Rank</strong> (<strong class="bold">MRR</strong>) – measuring how quickly<a id="_idIndexMarker1039"></a> the system finds the first <span class="No-Break">relevant Node.</span></li>
				<li><strong class="bold">Generation evaluation</strong>: RAGAS also evaluates the quality of the generated text using a combination of automatic metrics<a id="_idIndexMarker1040"></a> and human evaluation. The automatic metrics include <strong class="bold">Bilingual Evaluation Understudy</strong> (<strong class="bold">BLEU</strong>), which measures the similarity between the generated text and a reference text by comparing overlapping word sequences, and <strong class="bold">Recall-Oriented Understudy for Gisting Evaluation</strong> (<strong class="bold">ROUGE</strong>), which calculates the overlap of words<a id="_idIndexMarker1041"></a> and word sequences between the generated text and the reference text. To complement these automatic metrics, RAGAS also incorporates human evaluation to assess aspects such as fluency, coherence, and relevance of the generated output, providing a comprehensive assessment of the <span class="No-Break">generation quality.</span></li>
				<li><strong class="bold">Retrieval-generation interplay</strong>: The framework also analyzes the interplay between the retrieval and generation components by measuring how much the generated text relies on the retrieved Nodes. It introduces<a id="_idIndexMarker1042"></a> metrics such as <strong class="bold">Retrieval Dependency</strong> (<strong class="bold">RD</strong>), which quantifies how much the generated text depends<a id="_idIndexMarker1043"></a> on the retrieved Nodes, and <strong class="bold">Retrieval Relevance</strong> (<strong class="bold">RR</strong>), which measures the relevance of the retrieved Nodes to the generated text to quantify <span class="No-Break">this relationship.</span></li>
				<li><strong class="bold">Simulation</strong>: RAGAS includes a simulation<a id="_idIndexMarker1044"></a> component that allows us to simulate different retrieval scenarios and analyze their impact on the generation quality. This helps in understanding the robustness and generalization ability of RAG models under various retrieval conditions. By manipulating the retrieval results, users can test how the RAG model performs under scenarios such as retrieving irrelevant, partially relevant, or noisy data. The simulation feature provides insights into the interplay between the retrieval and generation components, enabling us to identify strengths and weaknesses and guide improvements in the <span class="No-Break">RAG model.</span></li>
				<li><strong class="bold">Fine-grained analysis</strong>: RAGAS enables fine-grained analysis of RAG pipelines by providing tools to visualize and interpret the retrieval-generation process, such as attention weights and individual <span class="No-Break">Node contributions.</span></li>
			</ul>
			<p>A key advantage of this framework is that it enables reference-free evaluation of RAG pipelines, meaning it does not rely on ground truth annotations. This allows for more efficient and scalable <span class="No-Break">evaluation cycles.</span></p>
			<p>Compared to Phoenix, RAGAS offers a more focused evaluation framework specifically tailored for RAG systems. While Phoenix provides a general-purpose evaluation platform with features such as tracing, hallucination detection, and relevance assessment, RAGAS goes deeper into the intricacies of retrieval-generation interplay and also offers simulation capabilities. The framework provides seamless integration with LlamaIndex, simplifying the evaluation of LlamaIndex-based RAG systems. To keep things simple, I have not included any code examples in this case, but you can find detailed examples and documentation on the official project’s page, at this <span class="No-Break">URL: </span><a href="https://docs.ragas.io/en/stable/howtos/integrations/llamaindex.html" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.ragas.io/en/stable/howtos/integrations/llamaindex.html</span></a><span class="No-Break">.</span></p>
			<p>It’s worth noting that RAGAS is a more recent framework compared to Phoenix, and while it shows great promise, it may take some time for it to see the same level of adoption in the <span class="No-Break">research community.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">One thing to always keep in mind in terms of evaluation is the concept of model drift, which we have already covered in <a href="#_idTextAnchor155"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Querying Our Data, Part 2 – Postprocessing and Response Synthesis</em> section. Model drift can impact our RAG pipeline when the LLM’s behavior gradually deviates from its intended purpose or when the quality of the generated output deteriorates. Regular or even continuous evaluation can help detect and mitigate this phenomenon, ensuring the RAG system remains reliable and effective in <span class="No-Break">production environments.</span></p>
			<p>By mastering tracing and evaluation techniques, you’ll be able to create a complete system for finding and fixing problems in an LLM application. Using evaluations and tracing together, you can spot where things go wrong, figure out why, and see which part of your application needs to <span class="No-Break">be improved.</span></p>
			<p>It’s now time to focus our attention<a id="_idIndexMarker1045"></a> on our side project: the PITS tutor. In this chapter, we’ll finally get to deploy its components and run it as a standalone application. But first, let’s have a short introduction to the different deployment options provided <span class="No-Break">by </span><span class="No-Break"><strong class="bold">Streamlit</strong></span><span class="No-Break">.</span></p>
			<h2 id="f_14__idParaDest-211" data-type="sect1" class="sect1" title2="Introduction to deployment with Streamlit" no2="9.4"><a id="_idTextAnchor210"></a>9.4. Introduction to deployment with Streamlit</h2>
			<p>As I explained in <span class="No-Break"><em class="italic">Chapter 2</em></span>, <em class="italic">LlamaIndex: The Hidden Jewel - An Introduction to the LlamaIndex Ecosystem</em>, I chose Streamlit<a id="_idIndexMarker1046"></a> as the backbone for our side project because of its simplicity and the many deployment options it provides. Streamlit offers an easy approach to deploying your applications, making it possible to share your work with a broader audience with minimal effort. If you successfully followed the installation steps in <a href="#_idTextAnchor023"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, your local environment should already be ready for the next steps. However, just in case, before proceeding, make sure you have completed the necessary installation mentioned in <a href="#_idTextAnchor023"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, in the <em class="italic">Discovering Streamlit – the perfect tool for quick build and </em><span class="No-Break"><em class="italic">deployment</em></span><span class="No-Break"> section.</span></p>
			<p>Now that we’re all set up, let’s explore the deployment options available for Streamlit applications. Beyond the simplest method of running apps on your local machine, Streamlit offers a variety of web deployment solutions<a id="_idIndexMarker1047"></a> to cater to different needs <span class="No-Break">and preferences:</span></p>
			<ul>
				<li><strong class="bold">Streamlit Community Cloud</strong>: This user-friendly platform is the most straightforward option for deploying Streamlit apps, enabling users to deploy directly from their GitHub repositories in just a few clicks. It requires minimal configuration, and once deployed, your app will be accessible via a unique URL on Streamlit Community Cloud, making it easy to share <span class="No-Break">with others.</span></li>
				<li><strong class="bold">Custom cloud services</strong>: For those seeking greater control over their deployment environment, Streamlit<a id="_idIndexMarker1048"></a> apps can be deployed on various cloud services, including <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>), <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>), and Azure. Deployment <a id="_idIndexMarker1049"></a>on these platforms might involve additional steps such as containerizing your app with Docker and configuring cloud-specific services such as AWS Elastic Beanstalk, Google App Engine, or Azure <span class="No-Break">App Service.</span></li>
				<li><strong class="bold">Self-hosting</strong>: If you have your own servers, opting to self-host your Streamlit applications gives you maximum control over the deployment environment and resources. This method involves setting up a server environment capable of running Python applications, installing Streamlit, and configuring your network for Streamlit app access. The self-hosting option answers to specific requirements for security, performance, or customization that cloud platforms <span class="No-Break">cannot meet.</span></li>
				<li><strong class="bold">Heroku</strong>: Heroku (https://www.heroku.com/) is another well-known platform<a id="_idIndexMarker1050"></a> for deploying Streamlit apps <a id="_idIndexMarker1051"></a>due to its simplicity and a free tier suitable for small projects <span class="No-Break">and prototypes.</span></li>
				<li><strong class="bold">Streamlit in Snowflake</strong>: For use cases prioritizing security and <strong class="bold">role-based access control</strong> (<strong class="bold">RBAC</strong>), Streamlit’s integration with Snowflake<a id="_idIndexMarker1052"></a> offers a secure coding and deployment environment within the Snowflake platform. You can easily sign up for a trial Snowflake account, create a warehouse and database for your apps, and deploy Streamlit applications directly<a id="_idIndexMarker1053"></a> <span class="No-Break">within Snowflake.</span></li>
			</ul>
			<p>Each of these deployment options offers unique benefits, with different advantages in terms of level of control, scalability, security requirements, and budget constraints. However, I have chosen to show you the simplest option and probably the most appropriate choice for our PITS application: deployment in Streamlit Community Cloud. However, for a commercial-ready solution, the other options would have been a <span class="No-Break">better choice.</span></p>
			<h2 id="f_14__idParaDest-212" data-type="sect1" class="sect1" title2="HANDS-ON – a step-by-step deployment guide" no2="9.5"><a id="_idTextAnchor211"></a>9.5. HANDS-ON – a step-by-step deployment guide</h2>
			<p>It’s time to share our PITS tutoring<a id="_idIndexMarker1054"></a> application with the world. However, as a quick side note, keep in mind that the current version is far from being ready for use in a multi-user, real-world environment. To keep the code base small and the deployment steps simple, I have designed PITS as a pure experiment in LlamaIndex. After all, the purpose of this book was not to delve into the architectural intricacies of building a full-fledged Streamlit application but rather to explain the tools and features that are available in LlamaIndex. This is the main reason why some of the PITS source files are not explained in detail in this book. Rest assured, however, that you will find plenty of comments in these modules, and if the comments available in the GitHub code aren’t enough, you can always explore the official Streamlit documentation and get a better<a id="_idIndexMarker1055"></a> understanding of the framework <span class="No-Break">here: </span><a href="https://docs.streamlit.io/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.streamlit.io/</span></a><span class="No-Break">.</span></p>
			<p>However, a brief introduction to the way Streamlit applications is built is in order. We’ll use one of the PITS UI files as an example, and I’ll walk you through the code to give you a basic understanding of the principles of Streamlit applications. Here is the code for <strong class="source-inline">app.py</strong>, our main program in the PITS structure. This code is responsible for orchestrating the execution of the various components that make up the tutoring application. It acts as the central hub, routing users through the onboarding process, handling session management, and dynamically presenting the quiz and training interfaces based on user interactions and <span class="No-Break">session data:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_288" title2="(no caption)" no2="">from user_onboarding import user_onboarding
from session_functions import load_session, delete_session, save_session
from logging_functions import reset_log
from quiz_UI import show_quiz
from training_UI import show_training_UI
import streamlit as st</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>We start by importing<a id="_idIndexMarker1056"></a> the necessary modules and components, including Streamlit. We also import several custom functions from the other modules, such as <strong class="source-inline">user_onboarding</strong>, <strong class="source-inline">load_session</strong>, <strong class="source-inline">delete_session</strong>, <strong class="source-inline">save_session</strong>, <strong class="source-inline">reset_log</strong>, <strong class="source-inline">show_quiz</strong>, and <strong class="source-inline">show_training_UI</strong>, each serving a specific role in the application’s flow. Following the imports, the <strong class="source-inline">main()</strong> function encapsulates the <span class="No-Break">application’s logic:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_289" title2="(no caption)" no2="">def main():
&nbsp;&nbsp;&nbsp;&nbsp;st.set_page_config(layout="wide")
&nbsp;&nbsp;&nbsp;&nbsp;st.sidebar.title('P.I.T.S.')
&nbsp;&nbsp;&nbsp;&nbsp;st.sidebar.markdown('### Your Personalized Intelligent Tutoring System')</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The use of <strong class="source-inline">st.set_page_config</strong> at the beginning establishes the basic layout of our web application. Streamlit provides a sidebar feature, and we’ll make use of that to streamline our UI. Next, the application’s flow is primarily controlled through conditional statements that check for the presence of certain keys in Streamlit’s <strong class="source-inline">(st.session_state)</strong> session state. This session state acts as persistent storage across reruns of the app within the same browser session, allowing the application to remember user choices, entered information, and other <span class="No-Break">stateful data:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_290" title2="(no caption)" no2="">&nbsp;&nbsp;&nbsp;&nbsp;if 'show_quiz' in st.session_state and 
&nbsp;&nbsp;&nbsp;&nbsp;st.session_state['show_quiz']:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;show_quiz(st.session_state['study_subject'])
&nbsp;&nbsp;&nbsp;&nbsp;elif 'resume_session' in st.session_state and 
&nbsp;&nbsp;&nbsp;&nbsp;st.session_state['resume_session']:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st.session_state['show_quiz'] = False
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;show_training_UI(st.session_state['user_name'], 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st.session_state['study_subject'])
&nbsp;&nbsp;&nbsp;&nbsp;elif not load_session(st.session_state):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;user_onboarding()</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p class="callout-heading">A quick note on Streamlit’s session state</p>
			<p class="callout">Web applications are inherently<a id="_idIndexMarker1057"></a> stateless, meaning each request and response between the client and server are independent. Streamlit’s session state allows us to overcome this by providing a way to maintain state across reruns of the app within the same browser session. This is essential for creating an interactive and user-friendly experience, as it allows the application to remember user choices, inputs, and actions without requiring the user to re-enter data after <span class="No-Break">every interaction.</span></p>
			<p>I’ll briefly explain what happens in the previous part of <span class="No-Break">the code:</span></p>
			<ul>
				<li><strong class="bold">Quiz display logic</strong>: If the user has opted to take a quiz (<strong class="source-inline">'show_quiz' in st.session_state</strong>), the quiz interface is displayed by <span class="No-Break">calling </span><span class="No-Break"><strong class="source-inline">show_quiz()</strong></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Resuming sessions</strong>: If the user has already chosen to resume an existing session (<strong class="source-inline">st.session_state['resume_session']=True</strong>), the app will take them directly to the <span class="No-Break">training UI.</span></li>
				<li><strong class="bold">User onboarding and session management</strong>:<strong class="source-inline"> load_session(st.session_state)</strong> checks whether session data exists. If not, the user is directed to the onboarding process <span class="No-Break">through </span><span class="No-Break"><strong class="source-inline">user_onboarding().</strong></span></li>
			</ul>
			<p>Next, let’s see what happens<a id="_idIndexMarker1058"></a> when an existing session is found but <strong class="source-inline">show quiz</strong> is <strong class="source-inline">False</strong> and the user hasn’t clicked on the <strong class="bold">Resume session</strong> <span class="No-Break">button yet:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_291" title2="(no caption)" no2=""> else:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st.write(f"Welcome back {st.session_state['user_name']}!")
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;col1, col2 = st.columns(2)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if col1.button(f"Resume your study of 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{st.session_state['study_subject']}"):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st.session_state['resume_session'] = True
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st.rerun()
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if col2.button('Start a new session'):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;delete_session(st.session_state)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;reset_log()
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for key in list(st.session_state.keys()):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;del st.session_state[key]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st.rerun()</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The first operation in this <strong class="source-inline">else</strong> block is displaying a welcome back message. The app then displays two buttons, allowing the user to decide whether they want to resume the existing training session or start a fresh one. Choosing to start a new session will basically reset everything and rerun the entire code to start the application from the beginning. Resuming the session at this point will determine<a id="_idIndexMarker1059"></a> the app to run <strong class="source-inline">show_training_UI</strong> and continue the existing <span class="No-Break">training session.</span></p>
			<h3 id="f_14__idParaDest-213" data-type="sect2" class="sect2" title2="Deploying our PITS project on Streamlit Community Cloud" no2="9.5.1"><a id="_idTextAnchor212"></a>9.5.1. Deploying our PITS project on Streamlit Community Cloud</h3>
			<p>Because of the way<a id="_idIndexMarker1060"></a> the internal folder<a id="_idIndexMarker1061"></a> structure of the Streamlit Community Cloud environment is implemented, we’ll have to make a few modifications to our PITS folder structure. The plan is to deploy the application straight from a GitHub repository. However, one of the requirements for deploying from GitHub into the Community Cloud environment is that the main <strong class="source-inline">.py</strong> file is hosted in the <strong class="source-inline">root</strong> folder of the repository. That is not the case for PITS as the folder structure is a bit different. <strong class="source-inline">app.py</strong>, which is the main file in our case, is currently found in the <strong class="source-inline">Building-Data-Driven-Applications-with-LlamaIndex\PITS_APP</strong> folder. To fix that, we’ll first make a copy of the <strong class="source-inline">PITS_APP</strong> subfolder, and then we’ll initiate a new GitHub repository from that new folder. To keep things simple and require minimum changes, I will guide you on how to create a new repository containing just the PITS app and then deploy it from your own <span class="No-Break">GitHub account:</span></p>
			<ol>
				<li>First, let’s create a copy<a id="_idIndexMarker1062"></a> of our local <strong class="source-inline">PITS_APP</strong> subfolder. Open Command Prompt<a id="_idIndexMarker1063"></a> and navigate to the <strong class="source-inline">Building-Data-Driven-Applications-with-LlamaIndex</strong> folder of your cloned repository. From that folder, type the <span class="No-Break">following command:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="terminal" id="untitled_terminal_13" title2="(no caption)" no2=""><strong class="bold">xcopy PITS_APP C:\PITS_APP /E /I</strong></pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>				<li>This will create a folder on your <strong class="source-inline">C:</strong> drive containing only the source files of the PITS application. If you navigate to the newly created folder and list its contents with the <strong class="source-inline">dir</strong> command, the output should look like <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.8</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/B21861_09_8.jpg" alt="Figure 9.8 – The contents of the C:\PITS_APP folder" width="770" height="659" data-type="figure" id="untitled_figure_76" title2="– The contents of the C:\PITS_APP folder" no2="9.8">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.8 – The contents of the C:\PITS_APP folder</p>
			<ol>
				<li value="3">The next step is to sign in to your GitHub account and create a new repository. Let’s name it <strong class="source-inline">PITS_ONLINE</strong>, as in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.9</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/B21861_09_9.jpg" alt="Figure 9.9 – Creating a new GitHub repository named PITS_ONLINE" width="1539" height="531" data-type="figure" id="untitled_figure_77" title2="– Creating a new GitHub repository named PITS_ONLINE" no2="9.9">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.9 – Creating a new GitHub repository named PITS_ONLINE</p>
			<ol>
				<li value="4">Once created, note the repository<a id="_idIndexMarker1064"></a> URL for the next steps. Next, we’ll initialize<a id="_idIndexMarker1065"></a> a new local repository in the desired folder. Open your CLI and navigate to the folder you want to turn into a separate repository – <strong class="source-inline">C:\PITS_APP</strong> – then execute the <span class="No-Break">following command:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="terminal" id="untitled_terminal_14" title2="(no caption)" no2=""><strong class="bold">git init</strong></pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>				<li>Next, add and commit the existing files by running the <span class="No-Break">following command:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_292" title2="(no caption)" no2=""><strong class="bold">Git add .</strong>
<strong class="bold">git commit -m "Initial commit for PITS_ONLINE repository"</strong></pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>				<li>It’s now time to link your local repository to the GitHub repository you created. Replace the URL with your GitHub URL and append <strong class="source-inline">.git</strong> at the end in the <span class="No-Break">following command:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="terminal" id="untitled_terminal_15" title2="(no caption)" no2=""><strong class="bold">git remote add origin &lt;your_repository_URL&gt;.git</strong></pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>				<li>And finally, we push the contents to the new online repository with the <span class="No-Break">following command:</span><div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_293" title2="(no caption)" no2=""><strong class="bold">git branch -M main</strong>
<strong class="bold">git push -u origin main</strong></pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div></li>			</ol>
			<p>If everything went smoothly you should now have a brand-new GitHub repository containing the PITS <span class="No-Break">source code.</span></p>
			<p>Let’s handle the Community Cloud <span class="No-Break">deployment next.</span></p>
			<p>Deploying Streamlit applications into their Community Cloud environment is a fairly simple and straightforward process. To begin our deployment, the first step is to sign up for a free Streamlit account here: <a href="https://share.streamlit.io/signup" target="_blank" rel="noopener noreferrer">https://share.streamlit.io/signup</a>. The best option is to use your GitHub account both for signing up and signing in to your Streamlit account. Once logged in, simply click on the <strong class="bold">New app</strong> button to begin the deployment process. You’ll be taken to a screen similar to what you can see in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.10</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/B21861_09_10.jpg" alt="Figure 9.10 – Deploying an application into Streamlit Community Cloud" width="1154" height="1015" data-type="figure" id="untitled_figure_78" title2="– Deploying an application into Streamlit Community Cloud" no2="9.10">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.10 – Deploying an application into Streamlit Community Cloud</p>
			<p>If you signed in to Streamlit<a id="_idIndexMarker1066"></a> using GitHub, you should already<a id="_idIndexMarker1067"></a> have the <strong class="source-inline">PITS_ONLINE</strong> repository listed as an option. Select it, then, under the <strong class="bold">Main file path</strong> field, change the default value to <strong class="source-inline">app.py</strong> and then click <strong class="bold">Deploy</strong>. From here, the Streamlit deployment service takes over and prepares the required environment for your application. This might take a while, but if you want to check on the progress, you can always expand the <strong class="bold">Manage app</strong> section on the bottom right of your screen. When everything is ready, the application should <span class="No-Break">start automatically.</span></p>
			<p>You can now ingest your existing training materials, have PITS generate slides and narrations about your desired study topic, and ask its chatbot any questions related to <span class="No-Break">the contents.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Don’t forget, you’re using your own API key. To keep costs under control, you should first experiment on a limited scale by uploading some small training resources and always keeping an eye on the OpenAI API usage. The good news is that the majority of the cost is incurred during slides and narration generation. However, once that is completed, the resulting material is stored and reused in <span class="No-Break">future sessions.</span></p>
			<p>Simple, isn’t it? Although offering an environment with limited resources, the Streamlit Community Cloud service makes it really easy to deploy simple apps and share quick prototypes. Your app is now online and can easily be shared with <span class="No-Break">other users.</span></p>
			<p>If anything went wrong, though, and you didn’t manage to complete the deployment, head over to the official<a id="_idIndexMarker1068"></a> documentation, and look for a solution: <a href="https://docs.streamlit.io/streamlit-community-cloud/deploy-your-app" target="_blank" rel="noopener noreferrer">https://docs.streamlit.io/streamlit-community-cloud/deploy-your-app</a>. In the Streamlit documentation, you’ll also find additional deployment options and configurations available<a id="_idIndexMarker1069"></a> that might be useful for your <span class="No-Break">future</span><span class="No-Break"><a id="_idIndexMarker1070"></a></span><span class="No-Break"> projects.</span></p>
			<h2 id="f_14__idParaDest-214" data-type="sect1" class="sect1" title2="Summary" no2="9.6"><a id="_idTextAnchor213"></a>9.6. Summary</h2>
			<p>In this chapter, we explored customizing and enhancing RAG workflows with LlamaIndex. We covered techniques to leverage open source LLMs such as Zephyr using tools such as LM Studio, offering cost-effective and privacy-focused alternatives to commercial models. The chapter discussed intelligent routing across multiple LLMs with services such as Neutrino and OpenRouter for optimized performance. Community-built Llama Packs were highlighted as powerful ways to rapidly prototype and build advanced components, and the chapter introduced the Llama CLI for streamlining RAG development and <span class="No-Break">deployment workflows.</span></p>
			<p>We talked about advanced tracing with Phoenix, allowing us to gain deep insight into application execution flows and pinpoint problems through visualization. The evaluation of RAG systems was covered using Phoenix’s relevance, hallucination, and QA correctness evaluators, ensuring the robust performance of our LlamaIndex apps. Streamlit’s deployment options, especially the Community Cloud service for easy application sharing, simplified the deployment process. A step-by-step guide demonstrated how to deploy the PITS tutoring application to <span class="No-Break">the cloud.</span></p>
			<p>With a strong grasp of customization, evaluation, and deployment techniques, developers can now build production-ready, optimized RAG applications tailored to their <span class="No-Break">unique requirements.</span></p>
			<p>Our journey continues with an exploration of the role of prompt engineering in enhancing the effectiveness of GenAI within the <span class="No-Break">LlamaIndex framework.</span></p>
		</div>
<div id="f_15__idContainer115" data-type="chapter" class="chapter" file="B21861_10_xhtml" title2="Prompt Engineering Guidelines and Best Practices" no2="10">
			<h1 id="f_15__idParaDest-215" class="chapter-number"><a id="_idTextAnchor214"></a>10</h1>
			<h1 id="f_15__idParaDest-216"><a id="_idTextAnchor215"></a>Prompt Engineering Guidelines and Best Practices</h1>
			<p>In this chapter, we embark on an exploration of how the latest advancements in technology are reshaping our interaction with digital tools and applications. As the digital landscape evolves, the traditional interfaces we’ve relied upon for decades are being reimagined, paving the way for more intuitive and efficient forms of communication between humans and machines. At the heart of this transformation is the advent of <a id="_idIndexMarker1071"></a>conversational interfaces powered by <strong class="bold">natural language</strong> (<strong class="bold">NL</strong>). As a result, understanding how to write effective prompts to customize the behavior of our LlamaIndex components becomes a critical skill in building and improving <span class="No-Break">RAG applications.</span></p>
			<p>Therefore, in this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Why prompts are your <span class="No-Break">secret weapon</span></li>
				<li>Understanding how LlamaIndex <span class="No-Break">uses prompts</span></li>
				<li>Customizing <span class="No-Break">default prompts</span></li>
				<li>The golden rules of <span class="No-Break">prompt engineering</span></li>
			</ul>
			<h2 id="f_15__idParaDest-217" data-type="sect1" class="sect1" title2="Technical requirements" no2="10.1"><a id="_idTextAnchor216"></a>10.1. Technical requirements</h2>
			<p>All code samples from this chapter can be found in the <strong class="source-inline">ch10</strong> subfolder of the book’s GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex</span></a><span class="No-Break">.</span></p>
			<h2 id="f_15__idParaDest-218" data-type="sect1" class="sect1" title2="Why prompts are your secret weapon" no2="10.2"><a id="_idTextAnchor217"></a>10.2. Why prompts are your secret weapon</h2>
			<p>I was 6 years old when I started writing my first lines of code using a ZX Spectrum computer. At the time, in the mid-1980s, computers were still a new thing in the world, and not many people <a id="_idIndexMarker1072"></a>understood the extraordinary impact they were going to have on human society. Today, we all live in a reality dominated, and in many ways, driven by technology. The way we relate to technology has also changed fundamentally in the last 40 years. Almost all human activities have come to be touched to a greater or lesser extent by <span class="No-Break">technological progress.</span></p>
			<p>What hasn’t changed much is the way we interact with technology. With a few notable exceptions – such as the introduction of touch screens and voice interfaces – our interaction with technology has remained almost unchanged. We use, as we did 40 years ago, rudimentary methods to get computers to perform the functions <span class="No-Break">we need.</span></p>
			<p class="callout-heading">Clarification</p>
			<p class="callout">When I say rudimentary, I’m not necessarily referring to the sophistication of the interface itself – although functionally, if we were to compare a modern-day keyboard or mouse, we would find that even here, the advances are not fantastic. I’m referring rather to another aspect that unfortunately continues to stagnate: the bandwidth that our current interfaces <span class="No-Break">can offer.</span></p>
			<p>The way we currently interact with our technology is long due <span class="No-Break">for replacement.</span></p>
			<p>Let’s go through a simple <span class="No-Break">rationale together:</span></p>
			<ul>
				<li>The computing power offered by IT systems continues to grow at a rapid pace. Even if Moore’s law – see <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.1</em> – is arguably no longer considered a valid benchmark, progress is far from slowing <span class="No-Break">down (</span><a href="https://en.wikipedia.org/wiki/Moore%27s_law" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://en.wikipedia.org/wiki/Moore%27s_law</span></a><span class="No-Break">).</span></li>
				<li>We live in a world dominated almost entirely by applications. At the moment, applications are the layer between the user and the machine that makes our interaction with a computer possible – apps running on local systems, apps running on mobile devices, or apps running in the cloud. Each app offers a very specific set <span class="No-Break">of functionalities.</span></li>
				<li>Many apps are designed to run only on specific platforms and cannot be easily ported to other platforms. This means a different app for each <span class="No-Break">specific platform.</span></li>
				<li>Many applications overlap in terms of functionality. For a given task there are, in most cases, dozens of different applications that can perform it. So, there is a lot <span class="No-Break">of duplication.</span></li>
				<li>Our interaction with technology has remained broadly the same bandwidth as 40 years ago. We use almost the same types of interfaces – keyboard, mouse, touchscreen, gesture- or voice-based – to control <span class="No-Break">application logic.</span></li>
				<li>Almost every application <a id="_idIndexMarker1073"></a>comes with its own UI. There is a mandatory learning curve that users have to go through to learn how to operate each application. If we multiply this time by the number of applications that a typical user uses on a regular basis, we find that we actually spend a lot of time learning to use a tool effectively, and this eats into the actual time we spend using the tool to <span class="No-Break">be productive.</span></li>
				<li>The number of software applications – including both publicly available applications and those used privately by organizations – is already huge. There are already more than 1 billion applications in the world. That’s without taking into account the fact that an application very often exists in several different versions. And the number <span class="No-Break">is growing.</span></li>
				<li>From an evolutionary point of view, the capacity of the human brain has remained unchanged throughout this time. Neuroplasticity gives us a remarkable ability to learn and adapt to new technologies, but unfortunately, evolution itself cannot keep up with <span class="No-Break">technological progress.</span></li>
			</ul>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/B21861_10_1.jpg" alt="Figure 10.1 – According to Moore’s law, the number of transistors roughly doubles every 2 years" width="1650" height="1085" data-type="figure" id="untitled_figure_79" title2="– According to Moore’s law, the number of transistors roughly doubles every 2 years" no2="10.1">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – According to Moore’s law, the number of transistors roughly doubles every 2 years</p>
			<p>See where I’m aiming? This very specific way of interacting with technology, combined with the rapid <a id="_idIndexMarker1074"></a>evolution of technology, is slowly making us victims of our own success. On the one hand, we have managed to build a huge number of specialized tools capable of solving a huge number of problems. But now, we have a bigger problem: we have so many tools that organizing and using them efficiently has become an extremely complicated process. A new paradigm <span class="No-Break">is needed.</span></p>
			<p>Conversational <a id="_idIndexMarker1075"></a>interfaces, based on <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>), present themselves as a promising alternative to the current way of interacting with technology. They represent a natural evolution in the way we communicate with our devices. Instead of relying on complex visual interfaces and input methods that require effort and time to learn, conversational interfaces allow us to use NL – the most fundamental and intuitive form of <span class="No-Break">human communication.</span></p>
			<p>This is where a new core competency in this new paradigm <span class="No-Break">comes in.</span></p>
			<p class="callout-heading">Prompt engineering</p>
			<p class="callout">As human-machine interaction becomes increasingly dependent on NL, the ability to formulate <a id="_idIndexMarker1076"></a>effective prompts that guide <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) algorithms toward <a id="_idIndexMarker1077"></a>desired responses or actions becomes essential. This skill involves not only formulating prompts clearly but also anticipating how different formulations may influence the interpretation and execution of commands by <span class="No-Break">the AI.</span></p>
			<p>Conversational interfaces transform the interaction with technology into a dialogue where linguistic precision and understanding of algorithmic subtleties become key factors in achieving desired outcomes. The ability to interact directly and effectively with computer <a id="_idIndexMarker1078"></a>systems using NL can significantly reduce the barrier between humans and technology. It offers a pathway to democratizing access to technology, making it accessible to a wider range of users, regardless of their <span class="No-Break">technical expertise.</span></p>
			<p>There are already indications that the intensive use of prompts in our everyday interactions with LLMs can improve even our interpersonal communication skills, as shown, for example, by this study: Liu et al. (2023), <em class="italic">Improving Interpersonal Communication by Simulating Audiences with Language </em><span class="No-Break"><em class="italic">Models</em></span><span class="No-Break"> (</span><a href="https://doi.org/10.48550/arXiv.2311.00687" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://doi.org/10.48550/arXiv.2311.00687</span></a><span class="No-Break">).</span></p>
			<p>Imagine computer systems that can replace the functionality of dozens or even hundreds of different applications but without the complexity of traditional interfaces. Language interaction: a form of technology where LLMs, augmented with RAG, take the place of applications and operating systems, giving us a universal and much simpler way to use computing power. Without getting too deep into the area of speculation, if I were to make a medium-to-long-term prediction, this is the direction I think we are heading in. In the short term, classical computing systems will continue to prevail. At first, conversational agent-based interfaces will gradually simplify user interaction with them, masking the complexity of the backend application layer. Then, as dedicated AI hardware becomes a commodity, a large part of the applications will be phased out of the ecosystem, and the functionality they provide will be taken over by <span class="No-Break">AI models.</span></p>
			<p>And I think this whole exposition justifies the title I have chosen for this section. Next, let’s discover together how prompts are used by LlamaIndex for <span class="No-Break">LLM interactions.</span></p>
			<h2 id="f_15__idParaDest-219" data-type="sect1" class="sect1" title2="Understanding how LlamaIndex uses prompts" no2="10.3"><a id="_idTextAnchor218"></a>10.3. Understanding how LlamaIndex uses prompts</h2>
			<p>In terms of mechanics, a RAG-based application follows exactly the same rules and principles of <a id="_idIndexMarker1079"></a>interaction that a simple user would use in a chat session with an LLM. A major difference comes from the fact that RAG is actually a <a id="_idIndexMarker1080"></a>kind of prompt engineer on steroids. Behind the scenes, for almost every indexing, retrieval, metadata extraction, or final response synthesis operation, the RAG framework programmatically produces prompts. These prompts are enriched with context and then sent to <span class="No-Break">the LLM.</span></p>
			<p>In LlamaIndex, for each type of operation that requires an LLM, there is a default prompt that is used as a template. Take <strong class="source-inline">TitleExtractor</strong> as an example. This is one of the metadata extractors that we already talked about in <a href="#_idTextAnchor068"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Ingesting Data into Our RAG Workflow</em>. The <strong class="source-inline">TitleExtractor</strong> class uses two predefined prompt templates to get titles from text nodes inside documents. It does this in <span class="No-Break">two steps:</span></p>
			<ol>
				<li>It gets potential titles from individual text Nodes using the <strong class="source-inline">node_template</strong> argument, which creates prompts to generate <span class="No-Break">appropriate titles</span></li>
				<li>Combines the individual Node titles into one overall comprehensive title for the whole Document using the <span class="No-Break"><strong class="source-inline">combine_template</strong></span><span class="No-Break"> prompt</span></li>
			</ol>
			<p>The default values for the <strong class="source-inline">TitleExtractor</strong> prompts are stored in <span class="No-Break">two constants:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_294" title2="(no caption)" no2=""><strong class="bold">DEFAULT_TITLE_NODE_TEMPLATE</strong> = """\
Context: <strong class="bold">{context_str}</strong>. Give a title that summarizes all of \ the unique entities, titles or themes found in the context. Title: """
<strong class="bold">DEFAULT_TITLE_COMBINE_TEMPLATE</strong> = """\
<strong class="bold">{context_str}</strong>. Based on the above candidate titles and content, \ what is the comprehensive title for this document? Title: """</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Looking at these two default templates used by <strong class="source-inline">TitleExtractor</strong>, we can easily understand how they work. Each template contains a <em class="italic">fixed</em> text part and a <em class="italic">dynamic</em> part, designated by <strong class="source-inline">{context_str}</strong> or other variables. That is where LlamaIndex will actually inject the text content of our Nodes during execution, as seen in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B21861_10_2.jpg" alt="Figure 10.2 – How prompts are built by injecting variables into a prompt template" width="1650" height="385" data-type="figure" id="untitled_figure_80" title2="– How prompts are built by injecting variables into a prompt template" no2="10.2">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – How prompts are built by injecting variables into a prompt template</p>
			<p>The prompt templates used by metadata extractors such as <strong class="source-inline">TitleExtractor</strong> are defined <a id="_idIndexMarker1081"></a>directly within the <strong class="source-inline">metadata_extractors.py</strong> module. The relative path of this module in the LlamaIndex <a id="_idIndexMarker1082"></a>GitHub repository is <strong class="source-inline">llama-index-core/llama_index/core/extractors/metadata_extractors.py</strong>. However, this is just an exception as the vast majority of the default templates are defined in two other key modules: <strong class="source-inline">llama-index-core/llama_index/core/prompts/default_prompts.py</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">llama-index-core/llama_index/core/prompts/chat_prompts.py</strong></span><span class="No-Break">.</span></p>
			<p>Because a RAG workflow built with LlamaIndex can have so many different components that rely on LLM interactions and not all prompt templates can be easily located within the code base, the framework provides a simple method to identify the templates used by a specific component. That method is called <strong class="source-inline">get_prompts()</strong> and can be used with agents, retrievers, query engines, response synthesizers, and many other RAG components. Here is a simple example of how we can use it to obtain a list of prompt templates used by a query engine built on top <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">SummaryIndex</strong></span><span class="No-Break">:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_295" title2="(no caption)" no2="">from llama_index.core import SummaryIndex, SimpleDirectoryReader
documents = SimpleDirectoryReader("files").load_data()
summary_index = SummaryIndex.from_documents(documents)
qe = summary_index.as_query_engine()</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The first part of the code should be very straightforward at this point. We import <strong class="source-inline">SummaryIndex</strong> and <strong class="source-inline">SimpleDirectoryReader</strong> and then ingest the two sample files that should have been cloned from our GitHub repository. Once the files have been ingested as Documents, we build an index and a query engine from that index. In this example, we won’t run any queries because we don’t need to. We just want to see the prompts. Therefore, the next step retrieves a dictionary containing the default prompts used within the <span class="No-Break">query engine:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_296" title2="(no caption)" no2="">prompts = qe.get_prompts()</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The dictionary returned by the <strong class="source-inline">get_prompts()</strong> method maps keys, which identify the different <a id="_idIndexMarker1083"></a>prompt types used by the query engine, to values <a id="_idIndexMarker1084"></a>that are the actual prompt templates. The last part of the code is responsible for iterating and displaying the keys and their <span class="No-Break">corresponding templates:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_297" title2="(no caption)" no2="">for k, p in prompts.items():
&nbsp;&nbsp;&nbsp;&nbsp;print(f"Prompt Key: {k}")
&nbsp;&nbsp;&nbsp;&nbsp;print("Text:")
&nbsp;&nbsp;&nbsp;&nbsp;print(p.get_template())
&nbsp;&nbsp;&nbsp;&nbsp;print("\n")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p><span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.3</em> shows the results after running <span class="No-Break">this sample:</span></p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B21861_10_3.jpg" alt="Figure 10.3 – The two prompt templates used by the SummaryIndex query engine" width="1492" height="511" data-type="figure" id="untitled_figure_81" title2="– The two prompt templates used by the SummaryIndex query engine" no2="10.3">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – The two prompt templates used by the SummaryIndex query engine</p>
			<p>Examining the output, we’ll see the two templates used by the query engine: <strong class="source-inline">text_qa_template</strong> and <strong class="source-inline">refine_template</strong>. You’ll notice that both keys begin with the text <strong class="source-inline">response_synthesizer:</strong>. This indicates the exact component of the query engine that actually uses the prompts – in our case, the response synthesizer. Following the same logic, we can use the <strong class="source-inline">get_prompts()</strong> method on many other types of RAG components in order to understand prompts used under <span class="No-Break">the hood.</span></p>
			<p class="callout-heading">Pro tip</p>
			<p class="callout">An alternative option to inspect the underlying prompts would be to use an advanced tracing method – such as the one using the Arize AI Phoenix framework, presented in <a href="#_idTextAnchor197"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Customizing and Deploying Our LlamaIndex Project</em>. Phoenix provides a visual representation of the execution flow, making it easier to understand how and when different prompts are used, in addition to displaying the final prompts with the inserted context. One caveat of using that method, though, is that instead of getting the original prompt templates, we’ll see the final prompts – also including any context already inserted in <span class="No-Break">the prompt.</span></p>
			<p>Now that we <a id="_idIndexMarker1085"></a>have a reliable technique for inspecting prompts, the next <a id="_idIndexMarker1086"></a>step explores ways in which can customize them. Building on the title extractor and query engine examples, in the next section, we’ll explore how to customize prompts used by various <span class="No-Break">RAG components.</span></p>
			<h2 id="f_15__idParaDest-220" data-type="sect1" class="sect1" title2="Customizing default prompts" no2="10.4"><a id="_idTextAnchor219"></a>10.4. Customizing default prompts</h2>
			<p>While the default prompts provided by LlamaIndex are designed to work well in most scenarios, there may <a id="_idIndexMarker1087"></a>be instances where customization is necessary or desirable. For example, you might want to adjust prompts to do <span class="No-Break">the following:</span></p>
			<ul>
				<li>Incorporate domain-specific knowledge <span class="No-Break">or terminology</span></li>
				<li>Adapt prompts to a particular writing style <span class="No-Break">or tone</span></li>
				<li>Modify prompts to prioritize certain types of information <span class="No-Break">or outputs</span></li>
				<li>Experiment with different prompt structures to optimize performance <span class="No-Break">or quality</span></li>
			</ul>
			<p>By customizing prompts, we can fine-tune the interaction between the RAG components and the language model, potentially leading to improved accuracy, relevance, and overall effectiveness of <span class="No-Break">our application.</span></p>
			<p>The good news is that we can modify the behavior of various LlamaIndex components by supplying our own custom prompt templates. The not-so-good news is that contrary to common expectations, writing a good prompt template is not a trivial task. One would have to consider many intricacies such as accuracy, relevance, query formulation, prompt size, output formatting, and others. Because of the involved complexity, the recommended approach for customization is to start with the default prompts and use them as a foundation for making any desired modifications. Changes should be incremental and ideally followed by rigorous evaluation against a diverse set of edge cases. We will have <a id="_idIndexMarker1088"></a>a more detailed discussion about general principles and best practices for writing prompts in the next section. For now, let us focus on the methods used for <span class="No-Break">prompt customization.</span></p>
			<p>In LlamaIndex, every RAG component that exposes the <strong class="source-inline">get_prompts()</strong> method also provides an equivalent for modifying these prompt templates – the <strong class="source-inline">update_prompts()</strong> method. So, this is the easiest way to change a particular prompt template. Let’s take our example from the previous section and experiment with a different prompt. This time, we will adapt the <strong class="source-inline">text_qa_template</strong> template to also rely on the LLM’s own knowledge when answering the query. The default <strong class="source-inline">text_qa_template</strong> template would normally look <span class="No-Break">like this:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_298" title2="(no caption)" no2="">Context information is below.
---------------------
{context_str}
---------------------
Given the context information <strong class="bold">and not prior knowledge</strong>, answer the query.
Query: {query_str}
Answer:</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>In the following example, we’ll make a very subtle change to this template and see how that will affect the behavior of our query engine. Let’s have a look at <span class="No-Break">the code:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_299" title2="(no caption)" no2="">from llama_index.core import SummaryIndex, SimpleDirectoryReader
from llama_index.core import PromptTemplate
documents = SimpleDirectoryReader("files").load_data()
summary_index = SummaryIndex.from_documents(documents)
qe = summary_index.as_query_engine()</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>So far, the code is identical to the previous example, with only one additional import that I will explain in a few moments. This time, though, we’ll first run a query using the default template. We’ll use this response as a <span class="No-Break">reference later:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_300" title2="(no caption)" no2="">print(qe.query("Who burned Rome?"))
print("------------------------")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>It’s now time <a id="_idIndexMarker1089"></a>to change the <strong class="source-inline">prompt_template</strong> template. We first define a string containing the <span class="No-Break">new version:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_301" title2="(no caption)" no2="">new_qa_template = (
"Context information is below."
"---------------------"
"{context_str}"
"---------------------"
"Given the context information "
"<strong class="bold">and any of your prior knowledge</strong>, "
"answer the query."
"Query: {query_str}"
"Answer:")</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>If you carefully compare the new version with the original template, you’ll notice a subtle but very important change. In this new version, I’m instructing the model to apply not just the knowledge provided in the retrieved context but also use its own knowledge on the matter. It’s time to make use of that new import we added at the beginning of our code. Because the <strong class="source-inline">update_prompts()</strong> method requires the prompts to be in the <strong class="source-inline">BasePromptTemplate</strong> format, we must first make sure that our new prompt is structured <span class="No-Break">like this:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_302" title2="(no caption)" no2="">template = PromptTemplate(new_qa_template)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>We’re now ready to rerun <span class="No-Break">the query:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_303" title2="(no caption)" no2="">qe. Update_prompts(
&nbsp;&nbsp;&nbsp;&nbsp;{"response_synthesizer: text_qa_template": template}
)
print(qe.query("Who burned Rome?"))</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Let’s have a look at the final output shown in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B21861_10_4.jpg" alt="Figure 10.4 – The query output before and after updating the prompt templates" width="997" height="152" data-type="figure" id="untitled_figure_82" title2="– The query output before and after updating the prompt templates" no2="10.4">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – The query output before and after updating the prompt templates</p>
			<p>As you can see <a id="_idIndexMarker1090"></a>in the output, that slight modification in the <strong class="source-inline">text_qa_template</strong> template of the query engine completely changed its behavior. In a similar fashion, instead of changing the answering approach, we could have instructed the LLM to answer in a certain linguistic style, speak in rhymes, or anything else we might need. I think the value this feature provides for a RAG application is pretty clear <span class="No-Break">by now.</span></p>
			<p>Unfortunately, not all LlamaIndex components support the <strong class="source-inline">update_prompts()</strong> method. Take, for example, the <strong class="source-inline">TitleExtractor</strong> metadata extractor that I mentioned in the previous section. Although metadata extractors do not support the <strong class="source-inline">update_prompts()</strong> method, the good news is that we can still change their underlying prompt templates by using arguments. In particular, the two templates used by <strong class="source-inline">TitleExtractor</strong> can be customized with the <strong class="source-inline">node_template</strong> and <strong class="source-inline">combine_template</strong> arguments. Let’s have a look at <span class="No-Break">an example:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_304" title2="(no caption)" no2="">from llama_index.core import SimpleDirectoryReader
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.extractors import TitleExtractor
reader = SimpleDirectoryReader('files')
documents = reader.load_data()
parser = SentenceSplitter()
nodes = parser.get_nodes_from_documents(documents)</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The first part of the example is responsible for ingesting our sample files as Documents and then chunking them into individual Nodes. Let’s extract the titles, first by using the default prompt templates that we saw in the <span class="No-Break">previous section:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_305" title2="(no caption)" no2="">title_extractor = TitleExtractor(summaries=["self"])
meta = title_extractor.extract(nodes)
print("\nFirst title: " +meta[0]['document_title'])
print("Second title: " +meta[1]['document_title'])</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The output <a id="_idIndexMarker1091"></a>so far should be something similar <span class="No-Break">to this:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_306" title2="(no caption)" no2="">First title: "The Enduring Influence of Ancient Rome: Architecture, Engineering, Conquest, and Legacy"
Second title: "The Enduring Bond: Dogs as Loyal Companions - Exploring the Unbreakable Connection Between Humans and Man's Best Friend"</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Next, let’s define a custom prompt template and pass it as an argument to <strong class="source-inline">TitleExtractor</strong> for the <span class="No-Break">second run:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_307" title2="(no caption)" no2="">combine_template = (
&nbsp;&nbsp;&nbsp;&nbsp;"{context_str}. Based on the above candidate titles "
&nbsp;&nbsp;&nbsp;&nbsp;"and content, what is the comprehensive title for "
&nbsp;&nbsp;&nbsp;&nbsp;"this document? Keep it under 6 words. Title: "
)
title_extractor = TitleExtractor(
&nbsp;&nbsp;&nbsp;&nbsp;summaries=["self"],
&nbsp;&nbsp;&nbsp;&nbsp;combine_template=combine_template
)
meta = title_extractor.extract(nodes)
print("\nFirst title: "+meta[0]['document_title'])
print("Second title: "+meta[1]['document_title']</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Because we’ve added an extra instruction in this custom prompt, the extractor should now generate <a id="_idIndexMarker1092"></a>shorter titles. The output for the second run should be something along the lines of <span class="No-Break">the following:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_308" title2="(no caption)" no2="">First title: "Roman Legacy: Architecture, Engineering, Conquest"
Second title: "Man's Best Friend: The Enduring Bond"</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>After seeing the basic mechanics of prompt customization, it’s time to move on to more <span class="No-Break">advanced methods.</span></p>
			<h3 id="f_15__idParaDest-221" data-type="sect2" class="sect2" title2="Using advanced prompting techniques in LlamaIndex" no2="10.4.1"><a id="_idTextAnchor220"></a>10.4.1. Using advanced prompting techniques in LlamaIndex</h3>
			<p>LlamaIndex offers several advanced prompting techniques that enable you to create more <a id="_idIndexMarker1093"></a>customized and expressive prompts, reuse existing prompts, and express certain operations <a id="_idIndexMarker1094"></a>more concisely. These <a id="_idIndexMarker1095"></a>techniques include partial formatting, prompt template variable mappings, and prompt function mappings. <em class="italic">Table 10.1</em> breaks down the purpose and potential use cases for <span class="No-Break">each method:</span></p>
			<table id="table001-4" class="No-Table-Style _idGenTablePara-1" data-type="table" title2="– An overview of the more advanced prompting techniques provided by LlamaIndex" no2="10.1"><colgroup><col><col></colgroup><tbody><tr class="No-Table-Style"><th class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Method</strong></span></p>
						</th><th class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Purpose</strong></span></p>
						</th></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p><span class="No-Break">Partial formatting</span></p>
						</td><td class="No-Table-Style">
							<p>Allows you to partially format a prompt by filling in some variables but leaving others to be filled in later. This is useful because it allows you to format variables as they become available, rather than maintaining all the required prompt variables until the end. The method is particularly useful in a multi-step RAG scenario that gradually builds the prompt by gathering different <span class="No-Break">user inputs.</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p>Prompt template <span class="No-Break">variable mappings</span></p>
						</td><td class="No-Table-Style">
							<p>They let you specify a mapping between some <em class="italic">expected</em> prompt keys and the keys actually used in your template, enabling you to reuse existing string templates without modifying the template variables. It is similar to creating an <em class="italic">alias</em> for <span class="No-Break">template keys.</span></p>
						</td></tr><tr class="No-Table-Style"><td class="No-Table-Style">
							<p>Prompt <span class="No-Break">function mappings</span></p>
						</td><td class="No-Table-Style">
							<p>This feature allows you to dynamically inject certain values, depending on other values or conditions, during query time by passing functions as template variables instead of <span class="No-Break">fixed values.</span></p>
						</td></tr></tbody></table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 10.1 – An overview of the more advanced prompting techniques provided by LlamaIndex</p>
			<p>You’ll <a id="_idIndexMarker1096"></a>find detailed <a id="_idIndexMarker1097"></a>code examples for all three <a id="_idIndexMarker1098"></a>methods in the official LlamaIndex <a id="_idIndexMarker1099"></a>documentation <span class="No-Break">here: </span><a href="https://docs.llamaindex.ai/en/stable/examples/prompts/advanced_prompts.html" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/prompts/advanced_prompts.html</span></a><span class="No-Break">.</span></p>
			<p>Having all these new cool gadgets in our knowledge inventory, we can now refine and tailor the dialogue between our application and the LLM, allowing us to customize the behavior of almost any RAG component <span class="No-Break">of LlamaIndex.</span></p>
			<p>For the final section of this chapter, we move our focus to an important aspect of maximizing our RAG setup’s potential: the art and science of <span class="No-Break">prompt engineering.</span></p>
			<h2 id="f_15__idParaDest-222" data-type="sect1" class="sect1" title2="The golden rules of prompt engineering" no2="10.5"><a id="_idTextAnchor221"></a>10.5. The golden rules of prompt engineering</h2>
			<p>This section is not intended to serve as a definitive guide to prompt engineering. In fact, the field is an <a id="_idIndexMarker1100"></a>ever-expanding one. Since many LLMs are demonstrating emerging capabilities that were not initially anticipated, it is only natural that our methods of interacting with these linguistic experts will also be refined over time. In other words, as LLMs evolve to better model and understand human nature, we in turn learn new ways of interacting with them. In this section, I aim to present some of the most commonly used techniques in prompt engineering, as well as the basic principles that govern the field. As stated in the previous section, writing a good prompt requires a fine balance between several parameters. Here are some of the most important aspects to consider when building prompts for a <span class="No-Break">RAG application.</span></p>
			<h3 id="f_15__idParaDest-223" data-type="sect2" class="sect2" title2="Accuracy and clarity in expression" no2="10.5.1"><a id="_idTextAnchor222"></a>10.5.1. Accuracy and clarity in expression</h3>
			<p>The prompt should be clear and precise, avoiding ambiguity. The more clearly you state what you <a id="_idIndexMarker1101"></a>need, the more likely you are to get a relevant response. It’s important to articulate the question or task in a way that leaves little room for misinterpretation. Make no assumptions about the model’s ability to understand your message. These assumptions are usually biased and tend to produce hallucinations <span class="No-Break">in return.</span></p>
			<h3 id="f_15__idParaDest-224" data-type="sect2" class="sect2" title2="Directiveness" no2="10.5.2"><a id="_idTextAnchor223"></a>10.5.2. Directiveness</h3>
			<p>How directive the prompt is can significantly impact the response. A prompt can range from open-ended – encouraging creative or broad responses – to highly specific – requesting <a id="_idIndexMarker1102"></a>a very particular type of answer. The level of directiveness should match the intended outcome. Given that we’re actually building prompt templates that mix a static part with dynamically retrieved content, consider exceptional scenarios and edge cases in which the model might misunderstand the prompt. Use clear instructions or commands (for example, <strong class="source-inline">Summarize</strong>, <strong class="source-inline">Analyze</strong>, and <strong class="source-inline">Explain</strong>) to guide the model on the desired task. Our prompts must be broad enough to accommodate varied inputs yet detailed enough to direct the <span class="No-Break">model effectively.</span></p>
			<h3 id="f_15__idParaDest-225" data-type="sect2" class="sect2" title2="Context quality" no2="10.5.3"><a id="_idTextAnchor224"></a>10.5.3. Context quality</h3>
			<p>This is a major pain point for building an effective RAG system. Both the quality and structure <a id="_idIndexMarker1103"></a>of our proprietary knowledge base as well as the ability to retrieve the most relevant context from it are very important aspects. <em class="italic">Garbage in, garbage out</em> may be regarded as a general rule applicable to this subject. Try to remove any inconsistencies in the data, special characters that might derail the LLM, duplicate data, and even grammatical errors in the text. These types of quality issues will unfortunately affect both the retrieval and the final response synthesis. Experiment with different retrieval strategies, such as the ones discussed in <a href="#_idTextAnchor131"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Querying Our Data, Part 1 – Context Retrieval</em>. Try different values for <strong class="source-inline">similarity_top_k</strong>, <strong class="source-inline">chunk_size</strong>, and <strong class="source-inline">chunk_overlap</strong>, as discussed in <a href="#_idTextAnchor068"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Ingesting Data into Our RAG Workflow</em>. Employ re-rankers and Node postprocessors to increase the context quality, as we did in <a href="#_idTextAnchor155"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Querying Our Data, Part 2 – Postprocessing and </em><span class="No-Break"><em class="italic">Response Synthesis</em></span><span class="No-Break">.</span></p>
			<h3 id="f_15__idParaDest-226" data-type="sect2" class="sect2" title2="Context quantity" no2="10.5.4"><a id="_idTextAnchor225"></a>10.5.4. Context quantity</h3>
			<p>There’s a balance between being concise and offering sufficient detail. A prompt should be brief enough to maintain focus but detailed enough to convey the specific requirements <a id="_idIndexMarker1104"></a>of the task or question. Too little context may result in answers that lack depth or relevance, while too much may confuse <a id="_idIndexMarker1105"></a>the model or lead <span class="No-Break">it off-topic.</span></p>
			<p>In RAG scenarios, as the amount of context provided in a prompt increase, it’s important to consider the potential impact on the alignment and accuracy of generated responses. While providing more context can be beneficial in many cases, as it gives the language model a broader understanding of the task at hand, there are also risks associated with excessively <span class="No-Break">long prompts.</span></p>
			<p>For example, when a prompt becomes too long, there is a higher chance of introducing irrelevant or contradictory information. This can lead to misalignment between the intended task and the model’s understanding of it. The model may give too much attention to tangential details or lose focus on the core objective. Maintaining a clear and concise prompt helps ensure that the model stays aligned with the <span class="No-Break">desired output.</span></p>
			<p>Also, as the context grows, the model has to process and consider a larger amount of information. This <a id="_idIndexMarker1106"></a>increased <strong class="bold">cognitive load</strong> can lead to a decrease in accuracy. The model may struggle to identify the most relevant pieces of information or may give undue importance to less significant details. Additionally, longer prompts are more likely to contain ambiguities or inconsistencies, which can further degrade the accuracy of <span class="No-Break">the responses.</span></p>
			<p class="callout-heading">Cognitive load in the context of LLMs</p>
			<p class="callout">Cognitive load refers to the amount of processing effort and resources required by the language model to process, understand, and generate a response based on the provided context. In the case of RAG systems, the cognitive load is directly related to the quantity and complexity of the information present in <span class="No-Break">the prompt.</span></p>
			<p>Implementing Node postprocessors such as <strong class="source-inline">SimilarityPostprocessor</strong> or <strong class="source-inline">SentenceEmbeddingOptimizer</strong> can partially mitigate this issue by filtering less relevant Nodes or shortening their content, and therefore reducing the final prompt submitted to the LLM. We covered these methods in <a href="#_idTextAnchor155"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Querying Our Data, Part 2 – Postprocessing and Response Synthesis</em>. Moreover, if the retrieved context is inherently long, consider breaking it down into smaller, more <span class="No-Break">manageable chunks.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Context ordering" no2="10.5.4.1">10.5.4.1. Context ordering</h4>
			<p>The overall effectiveness of our RAG pipeline does not rely just on the quantity and quality of context. Especially when dealing with longer context, most LLMs may perform differently <a id="_idIndexMarker1107"></a>when trying to extract the key information from that context, depending on where exactly that key information is placed. A good approach is to structure the prompt hierarchically, with the most critical information at the beginning or at the end. This ensures that the model prioritizes the core instructions and context. That’s where tools such as Node re-rankers or the <strong class="source-inline">LongContextReorder</strong> postprocessor may <span class="No-Break">become useful.</span></p>
			<p class="callout-heading">Side note</p>
			<p class="callout">There’s an increasingly popular RAG evaluation technique called the <em class="italic">needle in a haystack test</em>, in which researchers gauge the model’s ability to notice and recall a very specific piece of information from a larger context provided to the LLM. This specific information looks unsuspecting and is usually seamlessly blended into the overall context. In many ways, this method is similar to testing a human’s ability to pay attention to a certain text and then recall key information in <span class="No-Break">that text.</span></p>
			<h3 id="f_15__idParaDest-227" data-type="sect2" class="sect2" title2="Required output format" no2="10.5.5"><a id="_idTextAnchor226"></a>10.5.5. Required output format</h3>
			<p>In most cases, when building RAG workflows, we need LLMs to generate structured or semi-structured outputs. In almost all scenarios, we need the output to be predictable in terms of <a id="_idIndexMarker1108"></a>format, size, or language. Sometimes, providing a few <a id="_idIndexMarker1109"></a>examples in our prompt may lead to better responses, but that’s not a silver bullet for all scenarios. That’s were using output parsers and Pydantic programs becomes really important. We talked about these topics in <a href="#_idTextAnchor155"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Querying Our Data, Part 2 – Postprocessing and </em><span class="No-Break"><em class="italic">Response Synthesis</em></span><span class="No-Break">.</span></p>
			<h3 id="f_15__idParaDest-228" data-type="sect2" class="sect2" title2="Inference cost" no2="10.5.6"><a id="_idTextAnchor227"></a>10.5.6. Inference cost</h3>
			<p>In most cases, we’ll be running our applications within very specific cost constraints. Ignoring <a id="_idIndexMarker1110"></a>token usage would be a clear mistake. So, make <a id="_idIndexMarker1111"></a>sure you’re doing cost estimations, and always keep track of token usage. In addition, you could use tools such as <strong class="source-inline">LongLLMLinguaPostprocessor</strong> for prompt compression. We talked about this Node postprocessor in <a href="#_idTextAnchor155"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Querying Our Data, Part 2 – Postprocessing and Response Synthesis</em>. Prompt compression techniques have the potential to improve not only cost efficiency but also the quality of the final response by eliminating redundant information from our context and keeping just <span class="No-Break">key information.</span></p>
			<h3 id="f_15__idParaDest-229" data-type="sect2" class="sect2" title2="Overall system latency" no2="10.5.7"><a id="_idTextAnchor228"></a>10.5.7. Overall system latency</h3>
			<p>While this parameter depends on many factors, bloated, inefficient, or ambiguous prompts can <a id="_idIndexMarker1112"></a>also negatively affect system latency. It’s just like talking to a real person. The longer and less efficient the query, the more <a id="_idIndexMarker1113"></a>processing will be required from the model in order to best understand the actual intent behind the query. Longer processing times will negatively impact the overall <span class="No-Break">user experience.</span></p>
			<p>Prompt engineering is a continuous process of experimentation and iteration. Regularly evaluate the performance of your prompts and refine them based on the results. Remember – this is a long game, and the rules are being constantly re-written. Try to keep your knowledge up to date with the latest advancements and techniques in prompt engineering, as the field is <span class="No-Break">rapidly evolving.</span></p>
			<h3 id="f_15__idParaDest-230" data-type="sect2" class="sect2" title2="Choosing the right LLM for the task" no2="10.5.8"><a id="_idTextAnchor229"></a>10.5.8. Choosing the right LLM for the task</h3>
			<p>In the world of AI, not all LLMs are equal. In <a href="#_idTextAnchor197"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Customizing and Deploying Our LlamaIndex Project</em>, we already saw how easy is to customize different components <a id="_idIndexMarker1114"></a>of our RAG pipeline, including the underlying LLM. But there are actually many options available, so which one should we <a id="_idIndexMarker1115"></a>select for the job? Choosing the <em class="italic">wrong</em> LLM for a particular task will likely cancel many of the efforts we invested in crafting the actual prompts. It’s pretty much like trying to get an answer from the wrong person. If you’re persuasive enough, chances are you’ll get an answer at some point. However, that may not be the answer you were <span class="No-Break">looking for.</span></p>
			<p>That’s why understanding the different flavors of LLMs and knowing which one qualifies for a given task is essential. Several key characteristics should be useful for our model selection. Let’s look at <span class="No-Break">these next.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Model architecture" no2="10.5.8.1">10.5.8.1. Model architecture</h4>
			<p>Models can have different underlying architectures, and these may determine their inherent <a id="_idIndexMarker1116"></a>capabilities. For example, encoder-only models are specialized in encoding and classifying input text, useful for categorizing text into <a id="_idIndexMarker1117"></a>defined categories, such as with <strong class="bold">Bidirectional Encoder Representations from Transformers</strong> (<strong class="bold">BERT</strong>), which <a id="_idIndexMarker1118"></a>excels in <strong class="bold">next sentence prediction</strong> (<strong class="bold">NSP</strong>) <span class="No-Break">tasks (</span><a href="https://en.wikipedia.org/wiki/BERT_(language_model)" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://en.wikipedia.org/wiki/BERT_(language_model)</span></a><span class="No-Break">).</span></p>
			<p>Encoder-decoder models are capable of both understanding input text and generating responses, making them ideal for text generation and comprehension tasks, such as translation <a id="_idIndexMarker1119"></a>and summarizing articles. One example that fits in this category is <strong class="bold">Bidirectional and Auto-Regressive Transformer</strong> (<span class="No-Break"><strong class="bold">BART</strong></span><span class="No-Break">) (</span><a href="https://huggingface.co/docs/transformers/en/model_doc/bart" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://huggingface.co/docs/transformers/en/model_doc/bart</span></a><span class="No-Break">).</span></p>
			<p>Decoder-only models can decode or generate subsequent words or tokens from a given prompt and <a id="_idIndexMarker1120"></a>are primarily used for text generation. Models such as <strong class="bold">Generative Pre-trained Transformer</strong> (<strong class="bold">GPT</strong>), Mistral, Claude, and LLaMa are superstars in <span class="No-Break">this domain.</span></p>
			<p>There are <a id="_idIndexMarker1121"></a>also more exotic architectures such as <strong class="bold">Mixture-of-Experts</strong> (<strong class="bold">MoE</strong>), which essentially leverage a <em class="italic">sparse MoE</em> framework to offer dynamic, token-specific processing – see Shazeer et al. (2017), <em class="italic">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</em> (<a href="https://doi.org/10.48550/arXiv.1701.06538" target="_blank" rel="noopener noreferrer">https://doi.org/10.48550/arXiv.1701.06538</a>). This approach can significantly enhance performance across a range of domains, including mathematics, code generation, and multilingual <a id="_idIndexMarker1122"></a>tasks, as demonstrated by <span class="No-Break"><strong class="bold">Mixtral 8x7B</strong></span><span class="No-Break">.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Model size" no2="10.5.8.2">10.5.8.2. Model size</h4>
			<p>Model size is <a id="_idIndexMarker1123"></a>another critical factor to consider when selecting an LLM, as it directly impacts both the potential computational cost and the model’s capabilities. The number of parameters within an LLM, ranging from weights to biases adjusted during training, serves as a proxy for understanding the model’s complexity and, by extension, its operational expense. Larger models, such as GPT-4 with its estimated 1.76 trillion parameters, offer profound capabilities but come with higher costs <a id="_idIndexMarker1124"></a>and requirements for computational resources. On the other hand, medium-sized models, typically under 10 billion parameters, strike a balance between affordability and performance, making them suitable for a wide array of applications without breaking <span class="No-Break">the bank.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Inference speed" no2="10.5.8.3">10.5.8.3. Inference speed</h4>
			<p>That’s also a key parameter as it determines how quickly a model can process input and generate output. While larger models may offer enhanced performance in terms of output quality <a id="_idIndexMarker1125"></a>and depth, their inference speed tends to be slower due to the sheer volume of computations required. It’s important to note that inference speed is influenced by various factors beyond just the number of parameters, including the efficiency of the model architecture and the computational infrastructure used. Techniques to reduce inference time, such as model pruning, quantization, and leveraging specialized hardware, can significantly improve the usability of LLMs in <span class="No-Break">real-world applications.</span></p>
			<p>To make things even more complex, apart from these characteristics, LLMs can be specialized for various tasks or domains, enhancing their performance in specific scenarios. This specialization arises from the type of data and the training objectives used to fine-tune the model. Let’s look at some common <span class="No-Break">specializations next.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Chat models" no2="10.5.8.4">10.5.8.4. Chat models</h4>
			<p>Chat models are optimized for conversational interactions. They are designed to engage users in dialogue, providing responses that mimic human-like conversation. These models are <a id="_idIndexMarker1126"></a>adept at back-and-forth exchanges and can maintain context over a series <span class="No-Break">of interactions.</span></p>
			<p>They are the ideal choice for building chatbots or virtual assistants where the interaction is more casual or conversational. These models are used in applications requiring natural, engaging dialogue with users, such as customer service bots, entertainment applications, or virtual companions. As a particular characteristic, they tend to be more open-ended in their responses, aiming to generate replies that are engaging, contextually relevant, and sometimes <span class="No-Break">even entertaining.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Instruct models" no2="10.5.8.5">10.5.8.5. Instruct models</h4>
			<p>Instruct models are fine-tuned to understand and execute specific instructions or queries. They prioritize executing the given task based on the instruction over engaging in a dialogue. That makes them suitable for scenarios where the user needs the model to perform a <a id="_idIndexMarker1127"></a>particular task, such as summarizing a document, generating code based on a prompt, or providing detailed explanations. These models are preferred in educational tools, productivity applications, and anywhere a direct, clear response to a query is needed, such as in the intricate workflow of a <span class="No-Break">RAG application.</span></p>
			<p>They are more focused on accuracy and relevance to the task at hand rather than maintaining a conversational tone. Their responses are tailored toward fulfilling the user’s request as efficiently and effectively <span class="No-Break">as possible.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Codex models" no2="10.5.8.6">10.5.8.6. Codex models</h4>
			<p>These models are optimized for understanding and generating code. They have been trained in a <a id="_idIndexMarker1128"></a>vast corpus of programming languages and can assist with coding tasks, debug code, explain code snippets, and even generate entire programs based on a description. This makes them the perfect candidates for integrating into development environments, coding education tools, and anywhere automated coding assistance <span class="No-Break">is beneficial.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Summarization models" no2="10.5.8.7">10.5.8.7. Summarization models</h4>
			<p>Specialized in condensing long texts into shorter summaries while retaining key information <a id="_idIndexMarker1129"></a>and context. These models focus on capturing the essence of the content and presenting it concisely. They are useful for news aggregation services, research, content creation, and any scenario where quick insights from long documents <span class="No-Break">are needed.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Translation models" no2="10.5.8.8">10.5.8.8. Translation models</h4>
			<p>As the name implies these models are designed to translate text from one language to another. They have <a id="_idIndexMarker1130"></a>been trained on large multilingual datasets to understand and translate between languages with high accuracy, and they are best suited for global communication platforms, content localization, and educational tools aimed at <span class="No-Break">language learners.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Question-answering models" no2="10.5.8.9">10.5.8.9. Question-answering models</h4>
			<p>Fine-tuned to understand questions posed in NL and provide accurate answers by referencing <a id="_idIndexMarker1131"></a>provided texts or their vast training data, these models are key in building intelligent search engines, educational aids, and interactive <span class="No-Break">knowledge bases.</span></p>
			<p>And the list could probably go on with other types of models, fine-tuned for specific domains or applications. Also, keep in mind that because these different specializations tend to enhance or diminish certain capabilities of the model, our carefully crafted prompts may yield inconsistent results. For one model, a prompt may lead to near-perfect responses, while for another it could barely hit an <span class="No-Break">average mark.</span></p>
			<p>When choosing your LLM, it’s essential to weigh the trade-offs between all these characteristics and the specific requirements of your RAG application. Understanding these aspects helps in selecting a model that not only fits within your budget but also meets your performance and speed expectations. Whether you’re deploying an LLM for real-time applications requiring quick responses or complex tasks demanding deep understanding and generation capabilities, the chosen model will have a profound impact on the outcomes of your LlamaIndex application. But keep in mind that you’re never limited to using a single model for your entire RAG logic. As LlamaIndex gives you endless possibilities for customization, working with a suite of different models can also be an option. You just have to experiment and evaluate until you find the ideal mix and purpose for <span class="No-Break">each one.</span></p>
			<h3 id="f_15__idParaDest-231" data-type="sect2" class="sect2" title2="Common methods used for creating effective prompts" no2="10.5.9"><a id="_idTextAnchor230"></a>10.5.9. Common methods used for creating effective prompts</h3>
			<p>While simple prompts can be useful for many tasks, more advanced techniques are often required <a id="_idIndexMarker1132"></a>for complex <a id="_idIndexMarker1133"></a>reasoning or multi-step processes. While definitely not exhaustive, this section covers several powerful prompting techniques that can significantly enhance the performance of language models in our RAG applications. Since there’s already an abundance of study materials, free courses, and plenty of examples available on the web, in case you’re not yet familiar with these methods, take this list as a mere starting point for your future <span class="No-Break">learning path.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Few-shot prompting, also known as k-shot prompting" no2="10.5.9.1">10.5.9.1. Few-shot prompting, also known as k-shot prompting</h4>
			<p>As described in the paper by Brown et al. (2020), <em class="italic">Language Models are Few-Shot Learners</em> (<a href="https://doi.org/10.48550/arXiv.2005.14165" target="_blank" rel="noopener noreferrer">https://doi.org/10.48550/arXiv.2005.14165</a>), for complex tasks involving LLMs, few-shot <a id="_idIndexMarker1134"></a>prompting with demonstrations <a id="_idIndexMarker1135"></a>can enable in-context learning and improve performance. This <a id="_idIndexMarker1136"></a>method relies on providing a few examples of the task, along with the expected output, to condition the model. You can experiment with different numbers of examples (for example, one-shot, three-shot, and five-shot) to find the optimal balance, hence the <em class="italic">k-shot</em> <span class="No-Break">alternative name.</span></p>
			<p class="callout-heading">What about zero-shot prompting?</p>
			<p class="callout">For reference, <em class="italic">zero-shot prompting</em> involves presenting a model with a question without any <a id="_idIndexMarker1137"></a>preceding contextual question/answer pairs. This approach is more challenging for the model compared to one-shot or few-shot prompting, due to the absence <span class="No-Break">of context.</span></p>
			<p>When using few-shot prompting, keep in mind that the format you use for the examples and the distribution of the input text are important factors that can affect performance. While the few-shot prompting method increases the probability of a correct answer for simpler tasks, it may still struggle with more complex reasoning scenarios. Here’s a practical prompt example using <span class="No-Break">this technique:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_309" title2="(no caption)" no2="">Classify the following reviews as positive or negative sentiment:
&lt;The food was delicious and the service was excellent!&gt; // Positive
&lt;I waited over an hour and my meal arrived cold.&gt; // Negative
&lt;The ambiance was nice but the dishes were overpriced.&gt; //
Output:</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>Providing the model with a few examples in this style enables in-context learning and improves performance on the task without <span class="No-Break">requiring fine-tuning.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Chain-of-Thought (CoT) prompting" no2="10.5.9.2">10.5.9.2. Chain-of-Thought (CoT) prompting</h4>
			<p>First introduced in the paper by Wei et al. (2023), <em class="italic">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</em> (<a href="https://doi.org/10.48550/arXiv.2201.11903" target="_blank" rel="noopener noreferrer">https://doi.org/10.48550/arXiv.2201.11903</a>), this method provides impressive results for LLM tasks requiring reasoning or multi-step processes. We can use CoT prompting to encourage the model to break down the problem and show its thought process. We can include <a id="_idIndexMarker1138"></a>examples in our prompts, demonstrating <a id="_idIndexMarker1139"></a>the step-by-step reasoning process in the prompt. Here is a practical <span class="No-Break">prompt example:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_310" title2="(no caption)" no2="">There are 15 students in a class. 8 students have dogs as pets.
If 3 more students get a dog, how many of them would have a dog as a pet then?
Step 1) Initially there are 15 students and 8 have dogs
Step 2) 3 more students will get dogs soon
Step 3) So the final number is the initial 8 students with dogs plus the 3 new students = 8 + 3 = 11
Therefore, the number of students that would have a dog as a pet is 11.
A factory makes 100 items daily. On Tuesday, they boost production by 40% for a special order. However, to adjust inventory, they cut Thursday's output by 20% from Tuesday's high. Then, expecting a sales increase, Friday's output rises by 10% over the day before. Calculate the production numbers for Tuesday, Thursday, and Friday.</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<p>The first part of the prompt demonstrates the reasoning process, guiding the LLM to better answer the second part – which represents the <span class="No-Break">actual task.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Self-consistency" no2="10.5.9.3">10.5.9.3. Self-consistency</h4>
			<p>Self-consistency <a id="_idIndexMarker1140"></a>aims to improve the performance <a id="_idIndexMarker1141"></a>of CoT prompting by sampling multiple, diverse reasoning paths and using the generations to select the most consistent answer. First introduced in the paper by Wang et al. (2023), <em class="italic">Self-Consistency Improves Chain of Thought Reasoning in Language Models</em>  (<a href="https://doi.org/10.48550/arXiv.2203.11171" target="_blank" rel="noopener noreferrer">https://doi.org/10.48550/arXiv.2203.11171</a>), the self-consistency method helps boost performance on tasks involving arithmetic and commonsense reasoning by replacing the more traditional CoT prompting. Self-consistency involves providing few-shot CoT examples, generating multiple reasoning paths, and then selecting the most consistent answer based on <span class="No-Break">these paths.</span></p>
			<p>This approach <a id="_idIndexMarker1142"></a>acknowledges that language models, like humans, may <a id="_idIndexMarker1143"></a>sometimes make mistakes or take incorrect reasoning steps. However, by leveraging the diversity of reasoning paths and selecting the most consistent answer, self-consistency can potentially provide better answers than <span class="No-Break">CoT prompting.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Tree of Thoughts (ToT) prompting" no2="10.5.9.4">10.5.9.4. Tree of Thoughts (ToT) prompting</h4>
			<p>ToT is a framework that generalizes over CoT prompting and encourages the exploration of thoughts <a id="_idIndexMarker1144"></a>that serve as <a id="_idIndexMarker1145"></a>intermediate steps for general problem-solving with language models. Under the hood, it maintains a <em class="italic">tree of thoughts</em>, where thoughts represent coherent language sequences that serve as intermediate steps toward solving a problem. The language model’s ability to generate and evaluate thoughts is combined with specialized search algorithms to enable systematic exploration of thoughts. ToT prompting involves prompting the language model to evaluate intermediate thoughts as <em class="italic">sure</em>/<em class="italic">maybe</em>/<em class="italic">impossible</em> with regard to reaching the desired solution and then using search algorithms to explore the most promising paths. The method was presented for the first time in the following papers: Yao et al. (2023), <em class="italic">Tree of Thoughts: Deliberate Problem Solving with Large Language Models</em> (<a href="https://doi.org/10.48550/arXiv.2305.10601" target="_blank" rel="noopener noreferrer">https://doi.org/10.48550/arXiv.2305.10601</a>), and Long et al. (2023), <em class="italic">Large Language Model Guided </em><span class="No-Break"><em class="italic">Tree-of-Thought</em></span><span class="No-Break"> (</span><a href="https://doi.org/10.48550/arXiv.2305.08291" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://doi.org/10.48550/arXiv.2305.08291</span></a><span class="No-Break">).</span></p>
			<p>Here’s a <span class="No-Break">sample prompt:</span></p>
			<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre class="source-code" data-type="example" id="untitled_example_311" title2="(no caption)" no2="">Let's simulate a verbal conversation between three experts who tackle a complex puzzle.
Each expert outlines one step in their thought process before exchanging insights with the others, without adding any unnecessary remarks. As they progress, any expert who identifies a flaw in their reasoning exits the discussion. The process continues until a solution is found or all available options have been exhausted. The problem they need to solve is:
"Using only numbers 3, 3, 7, 7 and basic arithmetic operations, is it possible to obtain the value 25?"</pre><div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div></div>			<h4 data-type="sect3" class="sect3" title2="Prompt chaining" no2="10.5.9.5">10.5.9.5. Prompt chaining</h4>
			<p>This method relies on breaking down complex tasks into subtasks and using a chain of prompts, where <a id="_idIndexMarker1146"></a>each prompt’s output serves <a id="_idIndexMarker1147"></a>as an input for the next. Similar to the approach I used for the PITS application in the <strong class="source-inline">training_material_builder.py</strong> module, prompt chaining can improve the reliability, transparency, and controllability of the application. By default, in RAG applications, we use separate prompts for retrieving relevant information and generating a final output based on the <span class="No-Break">retrieved context.</span></p>
			<p>By following these golden rules and methods, you can develop more effective and reliable RAG applications using LlamaIndex and leverage the full potential <span class="No-Break">of LLMs.</span></p>
			<h2 id="f_15__idParaDest-232" data-type="sect1" class="sect1" title2="Summary" no2="10.6"><a id="_idTextAnchor231"></a>10.6. Summary</h2>
			<p>This chapter explored the importance of prompt engineering in building effective RAG applications with LlamaIndex. We learned how to inspect and customize the default prompts used by <span class="No-Break">various components.</span></p>
			<p>The chapter provided an overview of key principles and best practices for crafting high-quality prompts, as well as advanced prompting techniques. Additionally, it emphasized the significance of choosing the right language model for the task at hand and understanding their different architectures, capabilities, <span class="No-Break">and trade-offs.</span></p>
			<p>Finally, we talked about some simple yet powerful prompting methods, such as few-shot prompting, CoT prompting, self-consistency, ToT, and prompt chaining to enhance the reasoning and problem-solving abilities of language models. Mastering prompt engineering is crucial for unlocking the full potential of LLMs in <span class="No-Break">RAG applications.</span></p>
			<p>As we prepare to wrap up our journey, I invite you to join me in the final chapter of this book, where I will do my best to equip you with some additional learning tools and provide you with a bit of guidance on your future <span class="No-Break">learning path.</span></p>
		</div>
<div id="f_16__idContainer116" data-type="chapter" class="chapter" file="B21861_11_xhtml" title2="Conclusion and Additional Resources" no2="11">
			<h1 id="f_16__idParaDest-233" class="chapter-number"><a id="_idTextAnchor232"></a>11</h1>
			<h1 id="f_16__idParaDest-234"><a id="_idTextAnchor233"></a>Conclusion and Additional Resources</h1>
			<p>In this final chapter, we’ll reflect on the key takeaways from our exploration of RAG and its potential to revolutionize the field of AI. We’ll discuss the importance of staying updated with the latest developments, highlight valuable resources such as Replit bounties and the LlamaIndex community, and emphasize the need for responsible <span class="No-Break">AI development.</span></p>
			<p>As we look to the future, we’ll consider the impact of specialized AI hardware and the ethical considerations that must guide our progress. This chapter serves as a call to action for you to continue learning, contributing, and shaping the exciting world of RAG and AI, while always keeping the well-being of humanity at the forefront of <span class="No-Break">our endeavors.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Other projects and <span class="No-Break">further learning</span></li>
				<li>Key takeaways and final words <span class="No-Break">and encouragement</span></li>
			</ul>
			<h2 id="f_16__idParaDest-235" data-type="sect1" class="sect1" title2="Other projects and further learning" no2="11.1"><a id="_idTextAnchor234"></a>11.1. Other projects and further learning</h2>
			<p>As we approach the end of this book, it becomes clear that our journey toward mastering the LlamaIndex framework is only just beginning. I believe that theoretical knowledge can <a id="_idIndexMarker1148"></a>only take us so far. Practical applications are the key to having a real understanding of the information and its application to real-world problems. For this reason, I strongly encourage you to practice and experiment with the tools described in this book. The best way to practice is by studying and building actual <span class="No-Break">RAG applications.</span></p>
			<h3 id="f_16__idParaDest-236" data-type="sect2" class="sect2" title2="The LlamaIndex examples collection" no2="11.1.1"><a id="_idTextAnchor235"></a>11.1.1. The LlamaIndex examples collection</h3>
			<p>A great starting point for solidifying your knowledge is the plethora of examples and cookbooks <a id="_idIndexMarker1149"></a>available on the official LlamaIndex documentation page: <a href="https://docs.llamaindex.ai/en/stable/examples/" target="_blank" rel="noopener noreferrer">https://docs.llamaindex.ai/en/stable/examples/</a>. By examining and experimenting with the examples and cookbooks available there, you will gain practical insights into how to use nearly every component of the framework. Additionally, you will learn how to construct more complex RAG workflows by combining these components. This resource provides valuable code snippets, best practices, and real-world use cases that can help you understand the intricacies of building <span class="No-Break">RAG applications.</span></p>
			<p>Although some examples were also covered in this book, I had to be concise and therefore took some shortcuts. As a result, I have simplified the code in many cases. So, even if you’re already familiar with the topic, it’s worth having a look at some of the most interesting ones in there. Hundreds of examples are included, but to help you get started, I’ve noted a few very useful ones that you could <span class="No-Break">begin with.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Slack chat data connector" no2="11.1.1.1">11.1.1.1. Slack chat data connector</h4>
			<p>This simple example demonstrates how to use the LlamaIndex Slack data connector to perform <a id="_idIndexMarker1150"></a>question-answering over Slack chat <span class="No-Break">data: </span><a href="https://docs.llamaindex.ai/en/stable/examples/data_connectors/SlackDemo/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/data_connectors/SlackDemo/</span></a><span class="No-Break">.</span></p>
			<p>It showcases <a id="_idIndexMarker1151"></a>how to integrate the Slack API to retrieve chat history and build an index for efficient information retrieval. This basic example is the perfect starting point for organizations that heavily rely on Slack for communication and want to extract valuable insights from their chat data, build a chatbot, or implement a ChatOps model. Together with many other examples provided, the data connectors section provides a very useful learning resource. You can expand your knowledge about ingesting data from different sources into your <span class="No-Break">RAG workflow.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Discord thread management" no2="11.1.1.2">11.1.1.2. Discord thread management</h4>
			<p>Similar <a id="_idIndexMarker1152"></a>to the Slack data connector <a id="_idIndexMarker1153"></a>example, this Discord thread management example showcases the use of LlamaIndex to ingest, manage, and query Discord chat <span class="No-Break">data: </span><a href="https://docs.llamaindex.ai/en/stable/examples/discover_llamaindex/document_management/Discord_Thread_Management/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/discover_llamaindex/document_management/Discord_Thread_Management/</span></a><span class="No-Break">.</span></p>
			<p>It demonstrates the process of indexing Discord threads and refreshing the index with new data as it comes in. Following the approach demonstrated in this example, you can build applications that efficiently search and retrieve information from your Discord chat history. This opens up possibilities for building chatbots and virtual assistants or simply providing <a id="_idIndexMarker1154"></a>a way to quickly access important discussions <a id="_idIndexMarker1155"></a>and decisions made within Discord. For communities and organizations that use Discord as their primary communication platform, this example could provide a simple boilerplate for building a more complex <span class="No-Break">RAG solution.</span></p>
			<h4 data-type="sect3" class="sect3" title2="A multi-modal retrieval application that uses GPT4-V" no2="11.1.1.3">11.1.1.3. A multi-modal retrieval application that uses GPT4-V</h4>
			<p>This more <a id="_idIndexMarker1156"></a>advanced example showcases the use of LlamaIndex with GPT4-V to build a multi-modal retrieval system that uses both text and image <span class="No-Break">data: </span><a href="https://docs.llamaindex.ai/en/stable/examples/multi_modal/gpt4v_multi_modal_retrieval/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/multi_modal/gpt4v_multi_modal_retrieval/</span></a><span class="No-Break">.</span></p>
			<p class="callout-heading">Side note about multi-modal RAG</p>
			<p class="callout">Multi-modal RAG <a id="_idIndexMarker1157"></a>combines information retrieval across multiple modalities – such as text and images – with the reasoning and generation capabilities of LLMs. Potential use cases for multi-modal RAG are vast, ranging from building knowledge bases and question-answering systems that can handle both text and visual queries, to powering engaging multi-modal conversational agents, to enabling new types of creative and analytical applications that blend language <span class="No-Break">and vision.</span></p>
			<p>Because we didn’t cover multi-modal RAG in this book, I strongly encourage you to study this demonstration. Armed with the knowledge gained from this book and the explanations provided in this example, you’ll soon realize that extending your apps with multi-modal features does not represent such a big challenge at <span class="No-Break">this point.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Multi-tenancy RAG example" no2="11.1.1.4">11.1.1.4. Multi-tenancy RAG example</h4>
			<p>This example walks through the process of setting up a multi-user RAG system, including configuring the vector databases, indexing tenant-specific data, and handling user <span class="No-Break">queries: </span><a href="https://docs.llamaindex.ai/en/stable/examples/multi_tenancy/multi_tenancy_rag/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/multi_tenancy/multi_tenancy_rag/</span></a><span class="No-Break">.</span></p>
			<p>It explains <a id="_idIndexMarker1158"></a>a similar but more detailed approach than the one I used in the <em class="italic">Implementing metadata filters</em> section in <a href="#_idTextAnchor131"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Querying Our Data, Part 1 – Context Retrieval</em>. By utilizing separate vector databases <a id="_idIndexMarker1159"></a>for each tenant, group, or user, the example demonstrates how to ensure data isolation and privacy while providing basic RAG functions such as question-answering and <span class="No-Break">content generation.</span></p>
			<p>It shows a viable method for managing multiple tenants within a single application, making it a great starting point for production-ready RAG systems that must accommodate various clients or <span class="No-Break">user groups.</span></p>
			<p class="callout-heading">Wondering where this may be useful?</p>
			<p class="callout">Imagine a company that provides a chatbot service to multiple clients. Each client wants their own customized chatbot trained on their specific knowledge base and FAQs. With a multi-tenancy RAG system, the company can maintain separate indexes for each client, ensuring that queries to one client’s chatbot only retrieve information from that client’s knowledge base. This ensures data privacy and provides a personalized experience for <span class="No-Break">each client.</span></p>
			<p>By exploring this multi-tenancy RAG implementation, you can better understand how to design secure and efficient RAG systems that accommodate the needs of multiple tenants without compromising performance or <span class="No-Break">user experience.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Prompt engineering techniques for RAG" no2="11.1.1.5">11.1.1.5. Prompt engineering techniques for RAG</h4>
			<p>This example <a id="_idIndexMarker1160"></a>builds on the topic of <a id="_idIndexMarker1161"></a>customizing the prompts that are used in the RAG pipeline – a topic we covered in <a href="#_idTextAnchor214"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Prompt Engineering Guidelines and Best </em><span class="No-Break"><em class="italic">Practices</em></span><span class="No-Break">: </span><a href="https://docs.llamaindex.ai/en/stable/examples/prompts/prompts_rag/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/prompts/prompts_rag/</span></a><span class="No-Break">.</span></p>
			<p>The sample code illustrates how to use prompt engineering techniques to enhance the performance of different LlamaIndex RAG components. It explains strategies such as adding few-shot examples to the prompts to improve performance on various tasks. It also demonstrates techniques such as variable mapping and functions and gives an example of using prompt customization to handle context transformations, such as filtering personal data. This example, combined with the other examples available in the prompts <a id="_idIndexMarker1162"></a>section, represents a big step toward understanding <a id="_idIndexMarker1163"></a>how effective prompts can improve the quality and performance of RAG in specific <span class="No-Break">use cases.</span></p>
			<h4 data-type="sect3" class="sect3" title2="CitationQueryEngine implementation" no2="11.1.1.6">11.1.1.6. CitationQueryEngine implementation</h4>
			<p>This example is similar to the example discussed in <a href="#_idTextAnchor155"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Querying Our Data, Part 2 – Postprocessing and Response Synthesis</em> in the <em class="italic">Extracting structured outputs using output parsers</em> section. There, I showcased a simple method that not only answers a user question using their proprietary data but also points to the exact chunk of <a id="_idIndexMarker1164"></a>data that was used to generate the answer. Providing the source is an essential feature for a RAG system <a id="_idIndexMarker1165"></a>where transparency and traceability are important requirements. Here is a more advanced <span class="No-Break">example: </span><a href="https://docs.llamaindex.ai/en/stable/examples/query_engine/citation_query_engine/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/query_engine/citation_query_engine/</span></a><span class="No-Break">.</span></p>
			<p>This sample demonstrates a more advanced querying technique that enhances the context and traceability of retrieved information. By leveraging the power of citations, users can easily track the sources of the retrieved text, providing a clear and transparent way to verify the authenticity and reliability of the information. This example demonstrates how to set up <strong class="source-inline">CitationQueryEngine</strong> with customizable settings, allowing us to fine-tune the behavior of the engine according to our specific needs. It also provides guidance on inspecting the actual source of the retrieved information, enabling a detailed examination of the original context <span class="No-Break">when necessary.</span></p>
			<p><strong class="source-inline">CitationQueryEngine</strong> is particularly useful for researchers, journalists, auditors, compliance clerks, or anyone who requires a high level of transparency and accountability in their information retrieval process. By integrating this powerful tool into our RAG workflow, we can ensure that the information we rely on is well-documented and easily traceable to <span class="No-Break">its sources.</span></p>
			<p>Another very useful section in the LlamaIndex official documentation website is the <strong class="bold">Open-Source </strong><span class="No-Break"><strong class="bold">Community</strong></span><span class="No-Break"> tab.</span></p>
			<p>Available at <a href="https://docs.llamaindex.ai/en/stable/community/full_stack_projects/" target="_blank" rel="noopener noreferrer">https://docs.llamaindex.ai/en/stable/community/full_stack_projects/</a>, this section contains a collection of full-stack applications created by the LlamaIndex team. The main benefit here is that all the sample applications included have been open sourced under an MIT license, which means that you can freely use them out of the box to kickstart <span class="No-Break">your projects.</span></p>
			<p>Exploring <a id="_idIndexMarker1166"></a>these examples will strengthen the theoretical knowledge gained from this book and empower you to build robust, efficient, and innovative RAG applications. So, dive in, experiment, and let your creativity guide you in solving real-life problems using intelligent <span class="No-Break">retrieval systems.</span></p>
			<h3 id="f_16__idParaDest-237" data-type="sect2" class="sect2" title2="Moving forward – Replit bounties" no2="11.1.2"><a id="_idTextAnchor236"></a>11.1.2. Moving forward – Replit bounties</h3>
			<p>Applying theoretical concepts in solving real problems is probably one of the best ways to further develop your skillset. As a potential next step, once you gain confidence in your RAG and LlamaIndex skills, you might be interested in taking on coding challenges or working on small, potentially profitable projects. Replit, an online coding platform, can be an excellent resource for this purpose. Replit offers a browser-based development environment that allows you to write, run, and share code in various programming languages. It provides a collaborative and interactive space for developers to work on projects, learn from one another, and even earn money through <strong class="bold">Replit </strong><span class="No-Break"><strong class="bold">bounties</strong></span><span class="No-Break">: </span><a href="https://docs.replit.com/bounties/faq" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://docs.replit.com/bounties/faq</span></a><span class="No-Break">.</span></p>
			<p class="callout-heading">How bounties work</p>
			<p class="callout">One of the unique features of Replit is its bounties system, which encourages users to participate in coding challenges and contribute to open source projects while being rewarded for their efforts. Project maintainers or individuals who require assistance in solving specific problems or implementing new features create these bounties. Developers can explore the available bounties, select those that align with their skills and interests, and start working <span class="No-Break">on them.</span></p>
			<p>By participating in Replit bounties, you can gain practical experience in developing RAG solutions and applying the concepts covered in this book. These bounties often present real-world scenarios and requirements, providing you with the opportunity to tackle hands-on problems and enhance your <span class="No-Break">problem-solving abilities.</span></p>
			<p>Furthermore, the Replit platform nurtures a supportive and collaborative community. You can engage with other developers, learn from their approaches, and receive constructive feedback on your code. This interaction with the community can help your growth as a developer, broaden your knowledge, and keep you informed about the latest trends and best practices in <span class="No-Break">the field.</span></p>
			<p>To explore LlamaIndex-related content on Replit, you can go to <a href="https://replit.com/search?query=llamaindex" target="_blank" rel="noopener noreferrer">https://replit.com/search?query=llamaindex</a>. This search will help you discover relevant projects, code snippets, and discussions related to LlamaIndex, enabling you to apply your RAG skills in practical contexts and potentially uncover <span class="No-Break">lucrative opportunities.</span></p>
			<h3 id="f_16__idParaDest-238" data-type="sect2" class="sect2" title2="The power of many – the LlamaIndex community" no2="11.1.3"><a id="_idTextAnchor237"></a>11.1.3. The power of many – the LlamaIndex community</h3>
			<p>One of the most valuable resources available to any developer working with LlamaIndex is the vibrant and supportive community that has grown around the framework. With tens of thousands of developers actively participating, the LlamaIndex community offers a wealth of knowledge, experience, and inspiration. Joining this thriving community provides numerous benefits for developers at all skill levels. Whether you’re a beginner just starting with LlamaIndex or an experienced developer looking to take your projects to the next level, engaging with the community can help you achieve <span class="No-Break">your goals.</span></p>
			<p>The LlamaIndex community is full of developers who have worked on a wide range of projects, from simple proof-of-concepts to complex, real-world applications. By engaging with the community, you can learn from their experiences, discover best practices, and gain valuable insights that can help you improve your projects. You can ask questions, share your projects, and learn from the experiences of others who are also building on <span class="No-Break">the framework.</span></p>
			<p>The community is also a great place to showcase your LlamaIndex projects and get feedback from other developers. Sharing your work can help you refine your skills, gather new ideas, and even inspire others who are working on similar projects. Also, being a part of the LlamaIndex community allows you to contribute to the ongoing development and improvement of the framework itself. Whether by providing feedback, reporting bugs, or even contributing code, you can help shape the future of LlamaIndex and make it an even more powerful tool for developers around <span class="No-Break">the world.</span></p>
			<p>To get started, you can sign up for the project’s newsletter, join the official LlamaIndex Discord server, participate in discussions on the GitHub repository, or attend community events and webinars. The <strong class="bold">LlamaIndex Blog</strong>, which is available at <a href="https://www.llamaindex.ai/blog" target="_blank" rel="noopener noreferrer">https://www.llamaindex.ai/blog</a>, is another great resource that can help you stay up-to-date with the latest developments in the LlamaIndex ecosystem. The blog features a wide range of articles, tutorials, and case studies that showcase how developers are using LlamaIndex to build innovative applications across <span class="No-Break">various domains.</span></p>
			<h2 id="f_16__idParaDest-239" data-type="sect1" class="sect1" title2="Key takeaways, final words, and encouragement" no2="11.2"><a id="_idTextAnchor238"></a>11.2. Key takeaways, final words, and encouragement</h2>
			<p>The future of generative AI is a complex and rapidly evolving landscape with immense potential for transforming industries, augmenting human capabilities, and driving economic growth. In other words, the future looks bright. However, this future also brings significant technical, ethical, and societal challenges that must be carefully managed to ensure the responsible use of these powerful technologies. As it already happened numerous times in our history, innovation can foster progress and improvement but it can also lead to unintended consequences and disruptions that ripple through society. The rise of generative AI is no exception to <span class="No-Break">this pattern.</span></p>
			<p>While not being a direct contributor to the evolution of generative AI, RAG is definitely a catalyst for accelerating the progress of LLMs. It amplifies the capabilities of even the simplest models, creating new possibilities but also bigger challenges and risks. The software we develop has an increasingly significant impact on our society, and as our everyday lives become more influenced by software, we must exercise <span class="No-Break">greater caution.</span></p>
			<p>In many use cases for implementing RAG in combination with generative AI, what a single, proficient developer can produce today used to be the work of an entire company just a few years ago. And this is not entirely good news for us. While most companies are driven by profits and market success, they also have more checks and bounds in place and governance that guides them in their operations. This governance often includes ethical considerations, compliance with regulations, and a level of accountability that might not be as stringent or easily enforceable for individual developers or smaller teams. As computational costs decline and AI expertise becomes more widespread, smaller entities such as startups, local governments, and community groups may increasingly develop their own customized, RAG-infused LLMs to address niche requirements. This shift could erode the centralized dominance of big tech firms and foster a more diverse and dynamic ecosystem of AI innovation. The agility and innovation that smaller entities can bring to the table with tools such as RAG combined with generative AI are indeed remarkable, but this also opens up Pandora’s box of potential misuse and <span class="No-Break">ethical dilemmas.</span></p>
			<p class="callout-heading">Just to clarify my message</p>
			<p class="callout">I’m not suggesting that all hope is lost. I’m simply aiming to highlight and raise awareness of this risk. As these technologies evolve, the importance of integrating ethical considerations into the development process cannot be overstated. The democratization of AI technology means that the responsibility for its impact spreads across a wider array <span class="No-Break">of stakeholders.</span></p>
			<p>It’s not just about <em class="italic">what we can create</em>, but also about <em class="italic">what we should create</em>. This includes considering the long-term implications of our work and ensuring that we’re not inadvertently creating tools that can be used for harmful purposes. That being said, for starters, the Stanford Encyclopedia of Philosophy <em class="italic">Guideline on the Ethics of Artificial Intelligence and Robotics</em> should be considered a mandatory starting point for any aspiring AI <span class="No-Break">developer: </span><a href="https://plato.stanford.edu/entries/ethics-ai" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://plato.stanford.edu/entries/ethics-ai</span></a><span class="No-Break">.</span></p>
			<p>Because developers are not the only ones who should bear responsibility for the ethical use of AI technologies, several guidelines for organizations have also been published. A notable example is the <em class="italic">AI and the Role of the Board of Directors</em> article published at the Harvard Law School Forum on Corporate Governance by Holly J. Gregory and Sidley Austin LLP. This particular article provides a comprehensive governance guideline for corporate boards that want to improve internal controls and their oversight over the company’s AI-related <span class="No-Break">activities: </span><a href="https://corpgov.law.harvard.edu/2023/10/07/ai-and-the-role-of-the-board-of-directors/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://corpgov.law.harvard.edu/2023/10/07/ai-and-the-role-of-the-board-of-directors/</span></a><span class="No-Break">.</span></p>
			<p>Other useful resources providing ethical guidance for developing AI systems include the <em class="italic">Ethically Aligned Design</em>, written by the Institute of Electrical and Electronics Engineers (<a href="https://standards.ieee.org/industry-connections/ec/ead-v1/" target="_blank" rel="noopener noreferrer">https://standards.ieee.org/industry-connections/ec/ead-v1/</a>), and the <em class="italic">OECD AI Principles</em>, available <span class="No-Break">at </span><span class="No-Break">https://oecd.ai/en/ai-principles</span><span class="No-Break">.</span></p>
			<h3 id="f_16__idParaDest-240" data-type="sect2" class="sect2" title2="On the future of RAG in the larger context of generative AI" no2="11.2.1"><a id="_idTextAnchor239"></a>11.2.1. On the future of RAG in the larger context of generative AI</h3>
			<p>In many ways, writing this book felt like a race against the clock. The field is progressing so fast that keeping up with the latest developments and ensuring the content remains relevant is a constant challenge. Each chapter seemed to beckon for updates, even before the <em class="italic">ink was dry</em> on the previous one. As I navigated the latest research, breakthroughs, and debates, I was acutely aware of the need to present information that was not only accurate but also anticipated future trends. The aim was not only to depict the present situation but also to offer ideas that would be relevant and valuable in the long run. In particular, I’d like to highlight a few significant updates in the field that have led me to consider how RAG will be impacted in the <span class="No-Break">long run.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Long-context LLMs are becoming something common" no2="11.2.1.1">11.2.1.1. Long-context LLMs are becoming something common</h4>
			<p>The advent of LLMs such as <strong class="bold">Google’s Gemini 1.5</strong>, which can process up to 1 million tokens, has sparked a debate about the future of RAG: <a href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/" target="_blank" rel="noopener noreferrer">https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/</a>. With such a huge capacity for context ingestion, a legitimate question arises: <em class="italic">Do we still need RAG with </em><span class="No-Break"><em class="italic">these models?</em></span></p>
			<p>Despite the impressive capabilities of these models, they still have limitations, such as high cost, latency, and potential accuracy issues with large context windows. In contrast, RAG offers advantages in terms of cost, better control of information flow, and easier troubleshooting, making it a strong contender in the LLM space. The expanding capacity of models to ingest more data is exciting, but it does not guarantee proper understanding since accuracy can decline for content in the middle sections of lengthy text. RAG’s complementary strengths, such as filtration of irrelevant information, handling rapidly evolving knowledge, modular architectures, and specialized functionality, make it relevant even in the face of massively <span class="No-Break">scaled models.</span></p>
			<p>Therefore, in my opinion, even as the LLM context windows continue to increase in size, RAG will continue to play a crucial role in harnessing their potential while mitigating <span class="No-Break">their limitations.</span></p>
			<h4 data-type="sect3" class="sect3" title2="The emergence of specialized and highly efficient hardware for AI" no2="11.2.1.2">11.2.1.2. The emergence of specialized and highly efficient hardware for AI</h4>
			<p>Hardware <a id="_idIndexMarker1167"></a>innovations such as Groq’s <strong class="bold">GroqChip™</strong>, specifically <a id="_idIndexMarker1168"></a>designed for running AI models with extremely low latency, could significantly impact the landscape of AI and the role of RAG. Built from the ground up to accelerate AI, ML, and HPC workloads, the GroqChip™ reduces data movement for predictable low-latency performance, bottleneck-free. This could make cloud-based AI more accessible and powerful, allowing for the development of more sophisticated applications. By focusing on inference speed and efficient data processing and having a fully deterministic architecture, this technology can enable real-time generation of text, images, audio, and even video, potentially reducing the need for local AI hardware. This could make cloud-based AI more accessible and powerful, allowing for the development of more <span class="No-Break">sophisticated applications.</span></p>
			<p>Combined <a id="_idIndexMarker1169"></a>with RAG, Groq’s chips could help mitigate some of the limitations of LLMs by providing faster access to relevant information and even reducing the need for extensive context windows. The ability to process data rapidly and efficiently could also enhance RAG’s strengths, such as handling rapidly evolving knowledge and enabling modular architectures. A mix of such advanced hardware and RAG techniques could lead to more powerful, efficient, and adaptable AI systems that can better serve users’ needs while maintaining the benefits of information filtration and augmentation. Less latency means better user experience. A better user experience usually leads to <span class="No-Break">faster adoption.</span></p>
			<p>If this technology proves viable, traditional players in the hardware field such as NVIDIA, Intel and AMD will most probably follow through with similar products in the <span class="No-Break">near future.</span></p>
			<h4 data-type="sect3" class="sect3" title2="Multimodal is becoming the new norm" no2="11.2.1.3">11.2.1.3. Multimodal is becoming the new norm</h4>
			<p>Lately, all major players in the LLM field seem to converge on the adoption of multi-modal features. The mixture of RAG and multimodal AI represents a leap forward in creating <a id="_idIndexMarker1170"></a>systems that can comprehend and interact with the world in ways more similar to humans. This synergy could revolutionize how we access information, make decisions, and communicate, making AI more intuitive and aligned with our natural ways of processing information. Going beyond text and NLP capabilities, the fusion of RAG with multimodal AI promises to enhance the relevance and precision of generated content. For instance, in educational applications, it could provide tailored learning materials that combine textual explanations with illustrative diagrams, audio explanations, and interactive simulations. In healthcare, it might analyze medical reports, patient history, and imaging together to support diagnostic processes. The potential for creating more immersive and interactive entertainment experiences is also vast, from video games to <span class="No-Break">virtual reality.</span></p>
			<h4 data-type="sect3" class="sect3" title2="The AI regulation landscape is gradually taking shape" no2="11.2.1.4">11.2.1.4. The AI regulation landscape is gradually taking shape</h4>
			<p>As so often in recent history, the rapid advance of technology has left governments and institutions <a id="_idIndexMarker1171"></a>off-side. It’s a new field, one that abounds with opportunities but also risks. It is almost certain that in the near future, laws and regulations will be updated to cover this area and to ensure the safe and harmonious use of AI. The European Union has already set the tone by recently passing <a id="_idIndexMarker1172"></a>the so-called <strong class="bold">EU Artificial Intelligence Act</strong><em class="italic"> </em>(<strong class="bold">EU AI </strong><span class="No-Break"><strong class="bold">Act</strong></span><span class="No-Break">): </span><a href="https://artificialintelligenceact.eu/" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://artificialintelligenceact.eu/</span></a><span class="No-Break">.</span></p>
			<p>This landmark legislation classifies AI applications based on risk and strictly regulates or outright bans those deemed harmful, such as non-consensual biometric surveillance and social scoring systems. It emphasizes the need for transparency, accountability, and human oversight of high-risk applications, and strengthens the rights of individuals to understand and challenge AI-driven decisions. The EU AI Act marks the EU as a leader in AI governance <a id="_idIndexMarker1173"></a>and could set a precedent for other countries to follow, similar to the impact of the EU’s <strong class="bold">General Data Protection Regulation</strong> (<strong class="bold">GDPR</strong>) on data privacy <span class="No-Break">laws worldwide.</span></p>
			<p>Our future RAG solutions should be built considering these trends in regulations. They’ll need flexibility in terms of underlying models being used – as new rules could potentially restrict or outright ban the usage of a certain LLM, our apps should be redundant and portable in such scenarios. Also, to maximize compliance and stakeholder value, we should aim for <span class="No-Break">several objectives:</span></p>
			<ul>
				<li><strong class="bold">Transparency</strong>: RAG systems should be designed with transparency in mind, allowing users to understand how the AI model generates its outputs. This includes providing clear information about the data sources used, the logic of the retrieval process, and any potential limitations that could reduce the overall trust that users can place in <span class="No-Break">the output.</span></li>
				<li><strong class="bold">Human oversight</strong>: On top of comprehensive evaluation, high-risk RAG applications should incorporate human oversight and control mechanisms. This allows for human intervention when necessary and ensures that the AI system’s decisions can be challenged or overridden <span class="No-Break">if needed.</span></li>
				<li><strong class="bold">Data privacy and security</strong>: RAG workflows should be developed with strong data privacy and security measures in place. This includes adhering to data protection regulations, ensuring secure storage and processing of user data, and implementing measures to prevent unauthorized access or abuse. Implementing guardrails and misuse case testing (<a href="https://en.wikipedia.org/wiki/Misuse_case" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Misuse_case</a>) should be mandatory in case of applications that handle <span class="No-Break">high-value data.</span></li>
				<li><strong class="bold">Fairness and non-discrimination</strong>: RAG systems should be designed to avoid unfair bias and discrimination. This involves carefully curating our data sources, testing for biases, and implementing measures to mitigate any identified biases in the <span class="No-Break">RAG outputs.</span></li>
				<li><strong class="bold">Accountability</strong>: From a governance perspective, RAG applications should have clear accountability mechanisms in place. This includes designating responsible parties for <a id="_idIndexMarker1174"></a>the AI system’s actions, establishing processes for auditing and monitoring the system’s performance, and providing channels for users to report issues <span class="No-Break">or concerns.</span></li>
				<li><strong class="bold">Continuous monitoring and improvement</strong>: RAG pipelines should be subject to ongoing monitoring and evaluation to ensure they continue to operate as intended and comply with relevant regulations. This involves regularly assessing the system’s performance, addressing any identified issues, and updating any components as needed to improve its accuracy <span class="No-Break">and reliability.</span></li>
				<li><strong class="bold">Stakeholder engagement</strong>: Ideally, developers of RAG applications should engage with relevant stakeholders, including users, regulators, and civil society groups, to understand their needs and concerns. This feedback should be incorporated into the design and development process to ensure the system provides maximum value while adhering to ethical and <span class="No-Break">legal standards.</span></li>
			</ul>
			<p>By keeping these ideas in mind when creating and using RAG applications, developers can make sure their solutions remain compliant and at the same time, they provide solutions that are reliable, effective, and <span class="No-Break">deliver value.</span></p>
			<h3 id="f_16__idParaDest-241" data-type="sect2" class="sect2" title2="A small philosophical nugget for you to consider" no2="11.2.2"><a id="_idTextAnchor240"></a>11.2.2. A small philosophical nugget for you to consider</h3>
			<p>Lastly, I’d like to share with you a beautiful analogy extracted from an article written by John Nosta – founder of NostaLab. A visionary innovator, observing the future at the intersection <a id="_idIndexMarker1175"></a>of technology, science, and humanity, Mr. Nosta speaks about a less obvious effect that LLMs have on human society. Here’s a quick summary of <span class="No-Break">his concept:</span></p>
			<p>“Large language models are changing the way we think. They contain vast amounts of knowledge and are increasingly evolving toward human-like intelligence and probably beyond. As they grow in size and complexity, LLMs resemble a cognitive black hole, blurring the line between human and machine intelligence, potentially leading to their convergence. In the article, the idea of human escape velocity is a wonderful metaphor describing the difficulty of preserving human independence in the era of AI. The goal is to use AI to improve our cognitive abilities, creativity, and ethical reasoning. As LLMs become more integrated into human thinking and behavior, it is important to approach this new territory with care. To foster a symbiotic relationship that promotes a shared cognitive evolution, it is important to actively engage with AI’s capabilities rather than passively benefiting from them. The use of LLMs represents a transformative moment in AI, challenging our understanding of intelligence, consciousness, and what it means to be human in a <span class="No-Break">digital universe.”</span></p>
			<p>If you find these ideas intriguing, you can read the full article <span class="No-Break">here: </span><a href="https://www.psychologytoday.com/us/blog/the-digital-self/202403/llms-and-the-specter-of-the-cognitive-black-hole" target="_blank" rel="noopener noreferrer"><span class="No-Break">https://www.psychologytoday.com/us/blog/the-digital-self/202403/llms-and-the-specter-of-the-cognitive-black-hole</span></a><span class="No-Break">.</span></p>
			<h2 id="f_16__idParaDest-242" data-type="sect1" class="sect1" title2="Summary" no2="11.3"><a id="_idTextAnchor241"></a>11.3. Summary</h2>
			<p>This is a final encouragement for the road ahead. Alas, our time together has come to an end, but this is not a conclusion; rather, it is the beginning of a new journey. As you embark on this exciting path, it may initially appear that the road ahead is full of obstacles. However, remember that where there is a will, there is always a way. The knowledge and insights you have gained from this book will serve as essential items in your toolbox, empowering you to navigate the complexities that lie ahead. These concepts and techniques will provide a solid foundation upon which you can build, adapt, and innovate as you encounter new problems and opportunities in the ever-evolving landscape of AI. As you progress on this journey, I urge you to cultivate and maintain a <span class="No-Break">curious mindset.</span></p>
			<p>Curiosity is the fuel that propels us forward, driving us to ask questions, seek answers, and explore uncharted territories. It is through curiosity that we discover new possibilities, uncover hidden insights, and push the boundaries of what <span class="No-Break">is achievable.</span></p>
			<p>Above all, <em class="italic">never stop learning</em>, for knowledge is a <span class="No-Break">lifelong pursuit.</span></p>
		</div>
<div id="f_17__idContainer117" class="appendix" data-type="appendix" file="B21861_Index_xhtml" title2="Index" no2="">
			<h1 class="H1---Chapter" id="f_17__idParaDest-243"><a id="_idTextAnchor242"></a>Index</h1>
<p style="font-style: italic;">As this ebook edition doesn't have fixed pagination, the page numbers below are hyperlinked for reference only, based on the printed edition of this book.</p>
			<p class="index-head">Symbols</p>
			<p>8-bit integers (INT8)  <a href="#_idIndexMarker956" aria-label="Footnote 269">269</a></p>
			<p>16-bit floating-point (FP16)  <a href="#_idIndexMarker955" aria-label="Footnote 269">269</a></p>
			<p class="index-head">A</p>
			<p>abstract syntax tree (AST)  <a href="#_idIndexMarker239" aria-label="Footnote 72">72</a></p>
			<p>advanced node parsers</p>
			<p class="index-2">HTMLNodeParser  <a href="#_idIndexMarker253" aria-label="Footnote 75">75</a></p>
			<p class="index-2">JSONNodeParser  <a href="#_idIndexMarker259" aria-label="Footnote 75">75</a></p>
			<p class="index-2">LangchainNodeParser  <a href="#_idIndexMarker249" aria-label="Footnote 74">74</a></p>
			<p class="index-2">MarkdownNodeParser  <a href="#_idIndexMarker258" aria-label="Footnote 75">75</a></p>
			<p class="index-2">SentenceWindowNodeParser  <a href="#_idIndexMarker244" aria-label="Footnote 73">73</a></p>
			<p class="index-2">SimpleFileNodeParser  <a href="#_idIndexMarker252" aria-label="Footnote 74">74</a></p>
			<p class="index-2">using  <a href="#_idIndexMarker240" aria-label="Footnote 73">73</a></p>
			<p>advanced prompting techniques</p>
			<p class="index-2">using, in LlamaIndex  <a href="#_idIndexMarker1094" aria-label="Footnote 309">309</a></p>
			<p>advanced retrieval mechanisms</p>
			<p class="index-2">building, with naive retrieval method  <a href="#_idIndexMarker585" aria-label="Footnote 164">164</a></p>
			<p class="index-2">metadata filters, implementing  <a href="#_idIndexMarker589" aria-label="Footnote 165">165</a>-<a href="#_idIndexMarker594" aria-label="Footnote 167">167</a></p>
			<p class="index-2">queries, transforming  <a href="#_idIndexMarker604" aria-label="Footnote 171">171</a>-<a href="#_idIndexMarker608" aria-label="Footnote 173">173</a></p>
			<p class="index-2">queries, rewriting  <a href="#_idIndexMarker604" aria-label="Footnote 171">171</a>-<a href="#_idIndexMarker608" aria-label="Footnote 173">173</a></p>
			<p class="index-2">selectors, using for advanced decision logic  <a href="#_idIndexMarker595" aria-label="Footnote 167">167</a>, <a href="#_idIndexMarker597" aria-label="Footnote 168">168</a></p>
			<p class="index-2">specific sub-queries, creating  <a href="#_idIndexMarker609" aria-label="Footnote 173">173</a>, <a href="#_idIndexMarker611" aria-label="Footnote 174">174</a></p>
			<p class="index-2">tools  <a href="#_idIndexMarker600" aria-label="Footnote 169">169</a>-<a href="#_idIndexMarker603" aria-label="Footnote 171">171</a></p>
			<p>advanced routing</p>
			<p class="index-2">implementing , with RouterQueryEngine  <a href="#_idIndexMarker752" aria-label="Footnote 214">214</a>-<a href="#_idIndexMarker758" aria-label="Footnote 216">216</a></p>
			<p>advanced tracing</p>
			<p class="index-2">using  <a href="#_idIndexMarker1007" aria-label="Footnote 280">280</a>, <a href="#_idIndexMarker1009" aria-label="Footnote 281">281</a></p>
			<p>agentic functionality  <a href="#_idIndexMarker599" aria-label="Footnote 169">169</a></p>
			<p>agentic strategies implementation  <a href="#_idIndexMarker829" aria-label="Footnote 237">237</a></p>
			<p class="index-2">agents, enhancing with utility tools  <a href="#_idIndexMarker876" aria-label="Footnote 247">247</a></p>
			<p class="index-2">agents, interacting with  <a href="#_idIndexMarker874" aria-label="Footnote 247">247</a></p>
			<p class="index-2">LLMCompiler agent, using for advanced scenarios  <a href="#_idIndexMarker891" aria-label="Footnote 251">251</a>-<a href="#_idIndexMarker903" aria-label="Footnote 254">254</a></p>
			<p class="index-2">low-level Agent Protocol API, using  <a href="#_idIndexMarker904" aria-label="Footnote 254">254</a>, <a href="#_idIndexMarker909" aria-label="Footnote 255">255</a></p>
			<p class="index-2">OpenAIAgent  <a href="#_idIndexMarker847" aria-label="Footnote 241">241</a>-<a href="#_idIndexMarker860" aria-label="Footnote 245">245</a></p>
			<p class="index-2">ReActAgent  <a href="#_idIndexMarker866" aria-label="Footnote 246">246</a>, <a href="#_idIndexMarker873" aria-label="Footnote 247">247</a></p>
			<p class="index-2">reasoning loops  <a href="#_idIndexMarker843" aria-label="Footnote 240">240</a>, <a href="#_idIndexMarker845" aria-label="Footnote 241">241</a></p>
			<p class="index-2">tools and ToolSpec, building for agents  <a href="#_idIndexMarker832" aria-label="Footnote 237">237</a>-<a href="#_idIndexMarker841" aria-label="Footnote 240">240</a></p>
			<p>Agent Protocol</p>
			<p class="index-2">reference link  <a href="#_idIndexMarker905" aria-label="Footnote 254">254</a></p>
			<p>agent runners  <a href="#_idIndexMarker907" aria-label="Footnote 255">255</a></p>
			<p>agents  <a href="#_idIndexMarker830" aria-label="Footnote 237">237</a></p>
			<p>agent workers  <a href="#_idIndexMarker908" aria-label="Footnote 255">255</a></p>
			<p>Amazon Web Services (AWS)  <a href="#_idIndexMarker1048" aria-label="Footnote 291">291</a></p>
			<p>Arize Phoenix</p>
			<p class="index-2">reference link  <a href="#_idIndexMarker1035" aria-label="Footnote 288">288</a></p>
			<p>artificial intelligence (AI)  <a href="#_idIndexMarker002" aria-label="Footnote 3">3</a>, <a href="#_idIndexMarker1077" aria-label="Footnote 302">302</a></p>
			<p>asynchronous execution  <a href="#_idIndexMarker581" aria-label="Footnote 163">163</a></p>
			<p>asynchronous operation  <a href="#_idIndexMarker583" aria-label="Footnote 163">163</a></p>
			<p>attention mechanism  <a href="#_idIndexMarker014" aria-label="Footnote 5">5</a></p>
			<p class="index-head">B</p>
			<p>BaseChatStore class  <a href="#_idIndexMarker793" aria-label="Footnote 228">228</a></p>
			<p>Bidirectional and Auto-Regressive Transformer (BART)  <a href="#_idIndexMarker1119" aria-label="Footnote 313">313</a></p>
			<p>Bidirectional Encoder Representations from Transformers (BERT)  <a href="#_idIndexMarker1117" aria-label="Footnote 313">313</a></p>
			<p>Bilingual Evaluation Understudy (BLEU)  <a href="#_idIndexMarker1040" aria-label="Footnote 289">289</a></p>
			<p class="index-head">C</p>
			<p>callback function  <a href="#_idIndexMarker242" aria-label="Footnote 73">73</a></p>
			<p>CallbackManager  <a href="#_idIndexMarker329" aria-label="Footnote 90">90</a></p>
			<p>Chain-of-Thought (CoT) prompting  <a href="#_idIndexMarker1138" aria-label="Footnote 317">317</a></p>
			<p>chatbot-based support systems</p>
			<p class="index-2">advantages  <a href="#_idIndexMarker775" aria-label="Footnote 224">224</a></p>
			<p>chatbots  <a href="#_idIndexMarker773" aria-label="Footnote 224">224</a></p>
			<p>ChatEngine  <a href="#_idIndexMarker782" aria-label="Footnote 226">226</a>, <a href="#_idIndexMarker785" aria-label="Footnote 227">227</a></p>
			<p>chat_interface function  <a href="#_idIndexMarker920" aria-label="Footnote 257">257</a>, <a href="#_idIndexMarker927" aria-label="Footnote 261">261</a></p>
			<p>chat memory</p>
			<p class="index-2">working  <a href="#_idIndexMarker788" aria-label="Footnote 227">227</a>-<a href="#_idIndexMarker797" aria-label="Footnote 229">229</a></p>
			<p>chat modes  <a href="#_idIndexMarker787" aria-label="Footnote 227">227</a></p>
			<p class="index-2">condense and context mode  <a href="#_idIndexMarker824" aria-label="Footnote 235">235</a>, <a href="#_idIndexMarker826" aria-label="Footnote 236">236</a></p>
			<p class="index-2">condense question mode  <a href="#_idIndexMarker811" aria-label="Footnote 233">233</a>-<a href="#_idIndexMarker820" aria-label="Footnote 235">235</a></p>
			<p class="index-2">context mode  <a href="#_idIndexMarker801" aria-label="Footnote 230">230</a>-<a href="#_idIndexMarker806" aria-label="Footnote 232">232</a></p>
			<p class="index-2">simple mode  <a href="#_idIndexMarker798" aria-label="Footnote 229">229</a>, <a href="#_idIndexMarker800" aria-label="Footnote 230">230</a></p>
			<p>ChatOps  <a href="#_idIndexMarker776" aria-label="Footnote 225">225</a></p>
			<p class="index-2">reference link  <a href="#_idIndexMarker777" aria-label="Footnote 225">225</a></p>
			<p class="index-2">visual representation  <a href="#_idIndexMarker780" aria-label="Footnote 225">225</a></p>
			<p>chat_store attribute  <a href="#_idIndexMarker792" aria-label="Footnote 228">228</a></p>
			<p>chat_store_key parameter  <a href="#_idIndexMarker794" aria-label="Footnote 228">228</a></p>
			<p>chunk_overlap  <a href="#_idIndexMarker274" aria-label="Footnote 77">77</a>-<a href="#_idIndexMarker278" aria-label="Footnote 79">79</a></p>
			<p>chunk_size  <a href="#_idIndexMarker273" aria-label="Footnote 77">77</a>-<a href="#_idIndexMarker277" aria-label="Footnote 79">79</a></p>
			<p>CitationQueryEngine</p>
			<p class="index-2">implementation  <a href="#_idIndexMarker1165" aria-label="Footnote 322">322</a></p>
			<p>Claude-2  <a href="#_idIndexMarker181" aria-label="Footnote 51">51</a></p>
			<p>clinical decision support system (CDSS)  <a href="#_idIndexMarker558" aria-label="Footnote 156">156</a></p>
			<p>CodeSplitter  <a href="#_idIndexMarker235" aria-label="Footnote 72">72</a></p>
			<p>coding environment</p>
			<p class="index-2">Git, installing  <a href="#_idIndexMarker060" aria-label="Footnote 22">22</a>, <a href="#_idIndexMarker062" aria-label="Footnote 23">23</a></p>
			<p class="index-2">LlamaIndex, installing  <a href="#_idIndexMarker065" aria-label="Footnote 23">23</a></p>
			<p class="index-2">OpenAI API key, signing up  <a href="#_idIndexMarker068" aria-label="Footnote 23">23</a></p>
			<p class="index-2">preparing  <a href="#_idIndexMarker054" aria-label="Footnote 21">21</a></p>
			<p class="index-2">project execution  <a href="#_idIndexMarker082" aria-label="Footnote 27">27</a></p>
			<p class="index-2">Python, installing  <a href="#_idIndexMarker055" aria-label="Footnote 22">22</a></p>
			<p class="index-2">Streamlit  <a href="#_idIndexMarker076" aria-label="Footnote 26">26</a></p>
			<p class="index-2">Streamlit, installing  <a href="#_idIndexMarker081" aria-label="Footnote 27">27</a></p>
			<p class="index-2">verification  <a href="#_idIndexMarker084" aria-label="Footnote 27">27</a>, <a href="#_idIndexMarker085" aria-label="Footnote 28">28</a></p>
			<p>cognitive load  <a href="#_idIndexMarker1106" aria-label="Footnote 311">311</a></p>
			<p>CohereRerank  <a href="#_idIndexMarker687" aria-label="Footnote 198">198</a></p>
			<p class="index-2">reference link  <a href="#_idIndexMarker688" aria-label="Footnote 198">198</a></p>
			<p>collections  <a href="#_idIndexMarker430" aria-label="Footnote 116">116</a></p>
			<p>command line</p>
			<p class="index-2">Retrieval-Augmented Generation (RAG), working  <a href="#_idIndexMarker1000" aria-label="Footnote 279">279</a>, <a href="#_idIndexMarker1004" aria-label="Footnote 280">280</a></p>
			<p>command-line interface (CLI)  <a href="#_idIndexMarker995" aria-label="Footnote 277">277</a></p>
			<p>ComposableGraph  <a href="#_idIndexMarker154" aria-label="Footnote 44">44</a>, <a href="#_idIndexMarker495" aria-label="Footnote 131">131</a>-<a href="#_idIndexMarker503" aria-label="Footnote 134">134</a></p>
			<p class="index-2">tips  <a href="#_idIndexMarker500" aria-label="Footnote 133">133</a></p>
			<p class="index-2">usage, demonstrating  <a href="#_idIndexMarker498" aria-label="Footnote 132">132</a>, <a href="#_idIndexMarker499" aria-label="Footnote 133">133</a></p>
			<p>concept drift  <a href="#_idIndexMarker707" aria-label="Footnote 200">200</a></p>
			<p>CondensePlusContextChatEngine  <a href="#_idIndexMarker823" aria-label="Footnote 235">235</a>, <a href="#_idIndexMarker825" aria-label="Footnote 236">236</a></p>
			<p class="index-2">parameters  <a href="#_idIndexMarker827" aria-label="Footnote 236">236</a></p>
			<p>CondenseQuestionChatEngine  <a href="#_idIndexMarker812" aria-label="Footnote 233">233</a></p>
			<p class="index-2">implementing  <a href="#_idIndexMarker816" aria-label="Footnote 234">234</a>, <a href="#_idIndexMarker822" aria-label="Footnote 235">235</a></p>
			<p class="index-2">parameters  <a href="#_idIndexMarker815" aria-label="Footnote 233">233</a>, <a href="#_idIndexMarker817" aria-label="Footnote 234">234</a></p>
			<p>connectors  <a href="#_idIndexMarker099" aria-label="Footnote 36">36</a></p>
			<p>ContextChatEngine  <a href="#_idIndexMarker802" aria-label="Footnote 230">230</a>, <a href="#_idIndexMarker803" aria-label="Footnote 231">231</a></p>
			<p class="index-2">implementing  <a href="#_idIndexMarker808" aria-label="Footnote 232">232</a></p>
			<p class="index-2">parameters  <a href="#_idIndexMarker805" aria-label="Footnote 231">231</a>, <a href="#_idIndexMarker807" aria-label="Footnote 232">232</a></p>
			<p>context ordering  <a href="#_idIndexMarker1107" aria-label="Footnote 311">311</a></p>
			<p>context quantity  <a href="#_idIndexMarker1105" aria-label="Footnote 311">311</a></p>
			<p>continuous evaluation  <a href="#_idIndexMarker700" aria-label="Footnote 200">200</a></p>
			<p>conversation-driven collaboration  <a href="#_idIndexMarker778" aria-label="Footnote 225">225</a></p>
			<p>conversation history persistence example  <a href="#_idIndexMarker795" aria-label="Footnote 228">228</a></p>
			<p>conversation tracking for PITS</p>
			<p class="index-2">implementing  <a href="#_idIndexMarker915" aria-label="Footnote 257">257</a>-<a href="#_idIndexMarker928" aria-label="Footnote 261">261</a></p>
			<p>cosine similarity  <a href="#_idIndexMarker377" aria-label="Footnote 105">105</a>, <a href="#_idIndexMarker389" aria-label="Footnote 108">108</a>, <a href="#_idIndexMarker390" aria-label="Footnote 109">109</a></p>
			<p>coversation_engine.py  <a href="#_idIndexMarker916" aria-label="Footnote 257">257</a></p>
			<p>create-llama, example</p>
			<p class="index-2">reference link  <a href="#_idIndexMarker1002" aria-label="Footnote 280">280</a></p>
			<p>create, read, update, delete (CRUD)  <a href="#_idIndexMarker436" aria-label="Footnote 117">117</a></p>
			<p>Custom Embeddings</p>
			<p class="index-2">reference link  <a href="#_idIndexMarker411" aria-label="Footnote 112">112</a></p>
			<p>custom extractor</p>
			<p class="index-2">defining  <a href="#_idIndexMarker317" aria-label="Footnote 87">87</a></p>
			<p>custom postprocessors  <a href="#_idIndexMarker712" aria-label="Footnote 201">201</a></p>
			<p>CustomTransformation  <a href="#_idIndexMarker347" aria-label="Footnote 95">95</a></p>
			<p class="index-head">D</p>
			<p>data</p>
			<p class="index-2">indexing  <a href="#_idIndexMarker362" aria-label="Footnote 102">102</a></p>
			<p>DatabaseToolSpec class  <a href="#_idIndexMarker838" aria-label="Footnote 239">239</a></p>
			<p>data drift  <a href="#_idIndexMarker705" aria-label="Footnote 200">200</a></p>
			<p>data ingestion</p>
			<p class="index-2">via LlamaHub  <a href="#_idIndexMarker200" aria-label="Footnote 62">62</a></p>
			<p>data loaders  <a href="#_idIndexMarker098" aria-label="Footnote 36">36</a>, <a href="#_idIndexMarker205" aria-label="Footnote 63">63</a></p>
			<p>data privacy</p>
			<p class="index-2">scrubbing  <a href="#_idIndexMarker337" aria-label="Footnote 92">92</a>-<a href="#_idIndexMarker342" aria-label="Footnote 94">94</a></p>
			<p class="index-2">sensitive information  <a href="#_idIndexMarker338" aria-label="Footnote 92">92</a></p>
			<p class="index-2">with metadata extractors  <a href="#_idIndexMarker333" aria-label="Footnote 91">91</a>, <a href="#_idIndexMarker336" aria-label="Footnote 92">92</a></p>
			<p>data readers  <a href="#_idIndexMarker204" aria-label="Footnote 63">63</a></p>
			<p>deep learning (DL)  <a href="#_idIndexMarker017" aria-label="Footnote 5">5</a></p>
			<p>default prompts</p>
			<p class="index-2">advanced prompting techniques, using in LlamaIndex  <a href="#_idIndexMarker1093" aria-label="Footnote 309">309</a></p>
			<p class="index-2">customizing  <a href="#_idIndexMarker1087" aria-label="Footnote 305">305</a>-<a href="#_idIndexMarker1092" aria-label="Footnote 309">309</a></p>
			<p>dense retrieval  <a href="#_idIndexMarker614" aria-label="Footnote 175">175</a>, <a href="#_idIndexMarker618" aria-label="Footnote 176">176</a></p>
			<p class="index-2">benefits  <a href="#_idIndexMarker636" aria-label="Footnote 180">180</a></p>
			<p>dense search  <a href="#_idIndexMarker532" aria-label="Footnote 146">146</a></p>
			<p class="index-2">drawbacks  <a href="#_idIndexMarker616" aria-label="Footnote 175">175</a>, <a href="#_idIndexMarker617" aria-label="Footnote 176">176</a></p>
			<p>deployment guide</p>
			<p class="index-2">step-by-step  <a href="#_idIndexMarker1054" aria-label="Footnote 291">291</a>-<a href="#_idIndexMarker1058" aria-label="Footnote 293">293</a></p>
			<p>DevOps principles</p>
			<p class="index-2">reference link  <a href="#_idIndexMarker779" aria-label="Footnote 225">225</a></p>
			<p>directed acyclic graph (DAG)  <a href="#_idIndexMarker899" aria-label="Footnote 253">253</a></p>
			<p class="index-2">reference link  <a href="#_idIndexMarker900" aria-label="Footnote 253">253</a></p>
			<p>discord thread management  <a href="#_idIndexMarker1153" aria-label="Footnote 320">320</a></p>
			<p>distance search  <a href="#_idIndexMarker383" aria-label="Footnote 107">107</a></p>
			<p>docstrings  <a href="#_idIndexMarker836" aria-label="Footnote 238">238</a></p>
			<p>Document chunking  <a href="#_idIndexMarker115" aria-label="Footnote 39">39</a></p>
			<p>documents  <a href="#_idIndexMarker092" aria-label="Footnote 34">34</a>-<a href="#_idIndexMarker103" aria-label="Footnote 37">37</a>, <a href="#_idIndexMarker168" aria-label="Footnote 48">48</a></p>
			<p class="index-2">Nodes, extracting automatically from  <a href="#_idIndexMarker118" aria-label="Footnote 39">39</a>-<a href="#_idIndexMarker125" aria-label="Footnote 41">41</a></p>
			<p class="index-2">parsing, into nodes  <a href="#_idIndexMarker226" aria-label="Footnote 70">70</a></p>
			<p>Document store  <a href="#_idIndexMarker425" aria-label="Footnote 115">115</a></p>
			<p>DocumentSummaryIndex  <a href="#_idIndexMarker144" aria-label="Footnote 44">44</a>, <a href="#_idIndexMarker451" aria-label="Footnote 120">120</a>, <a href="#_idIndexMarker454" aria-label="Footnote 121">121</a></p>
			<p class="index-2">parameters  <a href="#_idIndexMarker455" aria-label="Footnote 121">121</a></p>
			<p class="index-2">usage model  <a href="#_idIndexMarker457" aria-label="Footnote 122">122</a></p>
			<p>DocumentSummaryIndex retrievers  <a href="#_idIndexMarker544" aria-label="Footnote 150">150</a></p>
			<p class="index-2">DocumentSummary IndexEmbeddingRetriever  <a href="#_idIndexMarker547" aria-label="Footnote 152">152</a></p>
			<p class="index-2">DocumentSummary IndexLLMRetriever  <a href="#_idIndexMarker545" aria-label="Footnote 150">150</a>, <a href="#_idIndexMarker546" aria-label="Footnote 151">151</a></p>
			<p>dot product  <a href="#_idIndexMarker394" aria-label="Footnote 109">109</a>, <a href="#_idIndexMarker396" aria-label="Footnote 110">110</a></p>
			<p class="index-head">E</p>
			<p>effective prompts methods</p>
			<p class="index-2">Chain-of-Thought (CoT) prompting  <a href="#_idIndexMarker1139" aria-label="Footnote 317">317</a></p>
			<p class="index-2">creating  <a href="#_idIndexMarker1133" aria-label="Footnote 316">316</a></p>
			<p class="index-2">few-shot prompting  <a href="#_idIndexMarker1134" aria-label="Footnote 316">316</a></p>
			<p class="index-2">prompt chaining  <a href="#_idIndexMarker1146" aria-label="Footnote 318">318</a></p>
			<p class="index-2">self-consistency  <a href="#_idIndexMarker1140" aria-label="Footnote 317">317</a></p>
			<p class="index-2">Tree of Thoughts (ToT) prompting  <a href="#_idIndexMarker1144" aria-label="Footnote 317">317</a></p>
			<p>ELIZA chatbot interface  <a href="#_idIndexMarker774" aria-label="Footnote 224">224</a></p>
			<p>embedding  <a href="#_idIndexMarker188" aria-label="Footnote 54">54</a>, <a href="#_idIndexMarker368" aria-label="Footnote 103">103</a>, <a href="#_idIndexMarker385" aria-label="Footnote 107">107</a></p>
			<p class="index-2">reference link  <a href="#_idIndexMarker986" aria-label="Footnote 276">276</a></p>
			<p>embedding model  <a href="#_idIndexMarker379" aria-label="Footnote 106">106</a></p>
			<p class="index-2">considerations  <a href="#_idIndexMarker414" aria-label="Footnote 112">112</a>, <a href="#_idIndexMarker416" aria-label="Footnote 113">113</a></p>
			<p class="index-2">customizing  <a href="#_idIndexMarker985" aria-label="Footnote 276">276</a></p>
			<p>EmbeddingRecencyPostprocessor  <a href="#_idIndexMarker675" aria-label="Footnote 196">196</a></p>
			<p class="index-2">parameters  <a href="#_idIndexMarker676" aria-label="Footnote 196">196</a></p>
			<p>EntityExtractor  <a href="#_idIndexMarker300" aria-label="Footnote 84">84</a></p>
			<p class="index-2">parameter  <a href="#_idIndexMarker306" aria-label="Footnote 85">85</a></p>
			<p>EU Artificial Intelligence Act (EU AI Act)  <a href="#_idIndexMarker1172" aria-label="Footnote 327">327</a></p>
			<p>Euclidean distance  <a href="#_idIndexMarker400" aria-label="Footnote 110">110</a>, <a href="#_idIndexMarker401" aria-label="Footnote 111">111</a></p>
			<p>Evaluation, LlamaIndex</p>
			<p class="index-2">reference link  <a href="#_idIndexMarker1032" aria-label="Footnote 288">288</a></p>
			<p>evaluation techniques</p>
			<p class="index-2">using  <a href="#_idIndexMarker1006" aria-label="Footnote 280">280</a>, <a href="#_idIndexMarker1010" aria-label="Footnote 281">281</a></p>
			<p class="index-head">F</p>
			<p>few-shot prompting  <a href="#_idIndexMarker1135" aria-label="Footnote 316">316</a></p>
			<p>FixedRecencyPostprocessor  <a href="#_idIndexMarker673" aria-label="Footnote 196">196</a></p>
			<p>fixed-size chunking  <a href="#_idIndexMarker374" aria-label="Footnote 105">105</a></p>
			<p>full fine-tuning  <a href="#_idIndexMarker042" aria-label="Footnote 15">15</a></p>
			<p>fully homomorphic encryption (FHE)  <a href="#_idIndexMarker335" aria-label="Footnote 92">92</a></p>
			<p>FunctionTool  <a href="#_idIndexMarker834" aria-label="Footnote 237">237</a></p>
			<p class="index-head">G</p>
			<p>General Data Protection Regulation (GDPR)  <a href="#_idIndexMarker1173" aria-label="Footnote 327">327</a></p>
			<p>generative AI future</p>
			<p class="index-2">AI regulation landscape  <a href="#_idIndexMarker1171" aria-label="Footnote 327">327</a>, <a href="#_idIndexMarker1174" aria-label="Footnote 328">328</a></p>
			<p class="index-2">hardware innovation  <a href="#_idIndexMarker1167" aria-label="Footnote 326">326</a></p>
			<p class="index-2">multimodal  <a href="#_idIndexMarker1170" aria-label="Footnote 327">327</a></p>
			<p>generative AI (GenAI)  <a href="#_idIndexMarker003" aria-label="Footnote 3">3</a>, <a href="#_idIndexMarker004" aria-label="Footnote 4">4</a>, <a href="#_idIndexMarker039" aria-label="Footnote 14">14</a>, <a href="#_idIndexMarker936" aria-label="Footnote 267">267</a></p>
			<p>Generative Pre-trained Transformer-Generated Unified Format (GGUF)  <a href="#_idIndexMarker949" aria-label="Footnote 268">268</a>, <a href="#_idIndexMarker950" aria-label="Footnote 269">269</a></p>
			<p>Generative Pre-trained Transformer (GPT)  <a href="#_idIndexMarker016" aria-label="Footnote 5">5</a>, <a href="#_idIndexMarker1120" aria-label="Footnote 313">313</a></p>
			<p>Git</p>
			<p class="index-2">installing  <a href="#_idIndexMarker059" aria-label="Footnote 22">22</a>, <a href="#_idIndexMarker063" aria-label="Footnote 23">23</a></p>
			<p class="index-2">reference link  <a href="#_idIndexMarker061" aria-label="Footnote 22">22</a></p>
			<p>GmailToolSpec  <a href="#_idIndexMarker862" aria-label="Footnote 245">245</a></p>
			<p>Google Cloud Platform (GCP)  <a href="#_idIndexMarker1049" aria-label="Footnote 291">291</a></p>
			<p>GPT-3.5-Turbo model  <a href="#_idIndexMarker177" aria-label="Footnote 50">50</a></p>
			<p class="index-2">features  <a href="#_idIndexMarker178" aria-label="Footnote 50">50</a></p>
			<p>GPT-4  <a href="#_idIndexMarker179" aria-label="Footnote 50">50</a></p>
			<p>GPU offloading  <a href="#_idIndexMarker957" aria-label="Footnote 270">270</a></p>
			<p>Graph Store  <a href="#_idIndexMarker428" aria-label="Footnote 115">115</a></p>
			<p>GroqChip™  <a href="#_idIndexMarker1168" aria-label="Footnote 326">326</a></p>
			<p>Guardrails  <a href="#_idIndexMarker726" aria-label="Footnote 206">206</a></p>
			<p>GuardrailsOutputParser  <a href="#_idIndexMarker727" aria-label="Footnote 206">206</a></p>
			<p class="index-2">parameter  <a href="#_idIndexMarker730" aria-label="Footnote 206">206</a></p>
			<p class="index-head">H</p>
			<p>Heroku  <a href="#_idIndexMarker1051" aria-label="Footnote 291">291</a></p>
			<p class="index-2">reference link  <a href="#_idIndexMarker1050" aria-label="Footnote 291">291</a></p>
			<p>HierarchicalNodeParser  <a href="#_idIndexMarker264" aria-label="Footnote 76">76</a></p>
			<p>high-level API  <a href="#_idIndexMarker744" aria-label="Footnote 210">210</a></p>
			<p>HTMLNodeParser  <a href="#_idIndexMarker254" aria-label="Footnote 75">75</a></p>
			<p>Hugging Face  <a href="#_idIndexMarker407" aria-label="Footnote 111">111</a>, <a href="#_idIndexMarker413" aria-label="Footnote 112">112</a></p>
			<p class="index-2">reference link  <a href="#_idIndexMarker405" aria-label="Footnote 111">111</a></p>
			<p>Hypothetical Document Embeddings (HyDE)  <a href="#_idIndexMarker606" aria-label="Footnote 172">172</a></p>
			<p class="index-head">I</p>
			<p>include_prev_next_rel option  <a href="#_idIndexMarker279" aria-label="Footnote 79">79</a></p>
			<p>index  <a href="#_idIndexMarker137" aria-label="Footnote 43">43</a>-<a href="#_idIndexMarker170" aria-label="Footnote 48">48</a></p>
			<p>indexes  <a href="#_idIndexMarker363" aria-label="Footnote 102">102</a></p>
			<p class="index-2">persisting  <a href="#_idIndexMarker419" aria-label="Footnote 114">114</a>, <a href="#_idIndexMarker421" aria-label="Footnote 115">115</a></p>
			<p class="index-2">potential cost, building  <a href="#_idIndexMarker504" aria-label="Footnote 134">134</a>-<a href="#_idIndexMarker510" aria-label="Footnote 138">138</a></p>
			<p class="index-2">potential cost, querying  <a href="#_idIndexMarker504" aria-label="Footnote 134">134</a>-<a href="#_idIndexMarker510" aria-label="Footnote 138">138</a></p>
			<p class="index-2">reusing  <a href="#_idIndexMarker419" aria-label="Footnote 114">114</a>, <a href="#_idIndexMarker421" aria-label="Footnote 115">115</a></p>
			<p>index_id</p>
			<p class="index-2">usage  <a href="#_idIndexMarker513" aria-label="Footnote 138">138</a></p>
			<p>Index Store  <a href="#_idIndexMarker426" aria-label="Footnote 115">115</a></p>
			<p>index types</p>
			<p class="index-2">ComposableGraph  <a href="#_idIndexMarker153" aria-label="Footnote 44">44</a></p>
			<p class="index-2">DocumentSummaryIndex  <a href="#_idIndexMarker143" aria-label="Footnote 44">44</a></p>
			<p class="index-2">features  <a href="#_idIndexMarker156" aria-label="Footnote 45">45</a>, <a href="#_idIndexMarker365" aria-label="Footnote 102">102</a>, <a href="#_idIndexMarker366" aria-label="Footnote 103">103</a></p>
			<p class="index-2">KeywordTableIndex  <a href="#_idIndexMarker150" aria-label="Footnote 44">44</a></p>
			<p class="index-2">KnowledgeGraphIndex  <a href="#_idIndexMarker151" aria-label="Footnote 44">44</a></p>
			<p class="index-2">SummaryIndex  <a href="#_idIndexMarker141" aria-label="Footnote 44">44</a></p>
			<p class="index-2">TreeIndex  <a href="#_idIndexMarker147" aria-label="Footnote 44">44</a></p>
			<p class="index-2">VectorStoreIndex  <a href="#_idIndexMarker145" aria-label="Footnote 44">44</a></p>
			<p>index types, in LlamaIndex</p>
			<p class="index-2">DocumentSummaryIndex  <a href="#_idIndexMarker452" aria-label="Footnote 120">120</a>, <a href="#_idIndexMarker453" aria-label="Footnote 121">121</a></p>
			<p class="index-2">exploring  <a href="#_idIndexMarker440" aria-label="Footnote 118">118</a></p>
			<p class="index-2">KeywordTableIndex  <a href="#_idIndexMarker459" aria-label="Footnote 122">122</a></p>
			<p class="index-2">KnowledgeGraphIndex  <a href="#_idIndexMarker484" aria-label="Footnote 128">128</a></p>
			<p class="index-2">SummaryIndex  <a href="#_idIndexMarker442" aria-label="Footnote 118">118</a>, <a href="#_idIndexMarker444" aria-label="Footnote 119">119</a></p>
			<p class="index-2">TreeIndex  <a href="#_idIndexMarker470" aria-label="Footnote 124">124</a>, <a href="#_idIndexMarker471" aria-label="Footnote 125">125</a></p>
			<p>inference cost  <a href="#_idIndexMarker1111" aria-label="Footnote 312">312</a></p>
			<p>information retrieval (IR)  <a href="#_idIndexMarker699" aria-label="Footnote 199">199</a></p>
			<p>ingestion pipeline</p>
			<p class="index-2">using  <a href="#_idIndexMarker343" aria-label="Footnote 94">94</a>-<a href="#_idIndexMarker350" aria-label="Footnote 97">97</a></p>
			<p>initialize_chatbot function  <a href="#_idIndexMarker919" aria-label="Footnote 257">257</a></p>
			<p>Inverse Document Frequency (IDF)  <a href="#_idIndexMarker623" aria-label="Footnote 177">177</a></p>
			<p class="index-head">J</p>
			<p>JSONNodeParser  <a href="#_idIndexMarker260" aria-label="Footnote 75">75</a></p>
			<p class="index-head">K</p>
			<p>KeywordExtractor  <a href="#_idIndexMarker307" aria-label="Footnote 86">86</a></p>
			<p>KeywordNodePostprocessor  <a href="#_idIndexMarker648" aria-label="Footnote 188">188</a>-<a href="#_idIndexMarker652" aria-label="Footnote 190">190</a></p>
			<p class="index-2">exclude_keywords  <a href="#_idIndexMarker654" aria-label="Footnote 190">190</a></p>
			<p class="index-2">lang  <a href="#_idIndexMarker655" aria-label="Footnote 190">190</a></p>
			<p class="index-2">required_keywords  <a href="#_idIndexMarker653" aria-label="Footnote 190">190</a></p>
			<p>KeywordTableIndex  <a href="#_idIndexMarker149" aria-label="Footnote 44">44</a>, <a href="#_idIndexMarker460" aria-label="Footnote 122">122</a>, <a href="#_idIndexMarker461" aria-label="Footnote 123">123</a></p>
			<p class="index-2">parameters  <a href="#_idIndexMarker462" aria-label="Footnote 123">123</a></p>
			<p class="index-2">usage model  <a href="#_idIndexMarker464" aria-label="Footnote 123">123</a></p>
			<p class="index-2">working  <a href="#_idIndexMarker465" aria-label="Footnote 124">124</a></p>
			<p>KnowledgeGraphIndex  <a href="#_idIndexMarker152" aria-label="Footnote 44">44</a>, <a href="#_idIndexMarker485" aria-label="Footnote 128">128</a></p>
			<p class="index-2">customizable parameters  <a href="#_idIndexMarker489" aria-label="Footnote 129">129</a>, <a href="#_idIndexMarker490" aria-label="Footnote 130">130</a></p>
			<p class="index-2">practical use case  <a href="#_idIndexMarker488" aria-label="Footnote 129">129</a></p>
			<p class="index-2">structure, building  <a href="#_idIndexMarker493" aria-label="Footnote 131">131</a></p>
			<p class="index-2">usage model  <a href="#_idIndexMarker491" aria-label="Footnote 130">130</a></p>
			<p>KnowledgeGraphIndex retrievers  <a href="#_idIndexMarker567" aria-label="Footnote 158">158</a></p>
			<p class="index-2">embedding mode  <a href="#_idIndexMarker569" aria-label="Footnote 159">159</a></p>
			<p class="index-2">hybrid mode  <a href="#_idIndexMarker570" aria-label="Footnote 160">160</a>, <a href="#_idIndexMarker572" aria-label="Footnote 161">161</a></p>
			<p class="index-2">keyword mode  <a href="#_idIndexMarker568" aria-label="Footnote 159">159</a></p>
			<p class="index-2">KnowledgeGraphRAGRetriever  <a href="#_idIndexMarker573" aria-label="Footnote 161">161</a>, <a href="#_idIndexMarker577" aria-label="Footnote 162">162</a></p>
			<p>knowledge graph (KG)  <a href="#_idIndexMarker486" aria-label="Footnote 128">128</a></p>
			<p>k-shot prompting  <a href="#_idIndexMarker1136" aria-label="Footnote 316">316</a></p>
			<p class="index-head">L</p>
			<p>LangChain framework  <a href="#_idIndexMarker250" aria-label="Footnote 74">74</a></p>
			<p>LangchainNodeParser  <a href="#_idIndexMarker248" aria-label="Footnote 74">74</a></p>
			<p>LangchainOutputParser  <a href="#_idIndexMarker732" aria-label="Footnote 207">207</a>-<a href="#_idIndexMarker737" aria-label="Footnote 209">209</a></p>
			<p class="index-2">configurable parameters  <a href="#_idIndexMarker739" aria-label="Footnote 209">209</a></p>
			<p>language models</p>
			<p class="index-2">optimizing  <a href="#_idIndexMarker037" aria-label="Footnote 14">14</a></p>
			<p>Large Language Model Meta AI (LLaMA)  <a href="#_idIndexMarker932" aria-label="Footnote 266">266</a></p>
			<p>Large Language Models (LLMs)  <a href="#_idIndexMarker000" aria-label="Footnote 3">3</a>-<a href="#_idIndexMarker011" aria-label="Footnote 5">5</a>, <a href="#_idIndexMarker033" aria-label="Footnote 13">13</a>, <a href="#_idIndexMarker930" aria-label="Footnote 265">265</a>, <a href="#_idIndexMarker1175" aria-label="Footnote 328">328</a></p>
			<p class="index-2">augmenting, with RAG  <a href="#_idIndexMarker027" aria-label="Footnote 11">11</a>, <a href="#_idIndexMarker031" aria-label="Footnote 12">12</a></p>
			<p class="index-2">challenges, exploring  <a href="#_idIndexMarker021" aria-label="Footnote 8">8</a>-<a href="#_idIndexMarker025" aria-label="Footnote 11">11</a></p>
			<p class="index-2">reference link  <a href="#_idIndexMarker186" aria-label="Footnote 53">53</a></p>
			<p class="index-2">role  <a href="#_idIndexMarker018" aria-label="Footnote 6">6</a>, <a href="#_idIndexMarker020" aria-label="Footnote 7">7</a></p>
			<p class="index-2">with Neutrino  <a href="#_idIndexMarker968" aria-label="Footnote 273">273</a>-<a href="#_idIndexMarker983" aria-label="Footnote 276">276</a></p>
			<p class="index-2">with OpenRouter  <a href="#_idIndexMarker969" aria-label="Footnote 273">273</a>-<a href="#_idIndexMarker984" aria-label="Footnote 276">276</a></p>
			<p>LLaMA 2  <a href="#_idIndexMarker934" aria-label="Footnote 266">266</a>, <a href="#_idIndexMarker938" aria-label="Footnote 267">267</a></p>
			<p>LLAMA2-CHAT-70B  <a href="#_idIndexMarker947" aria-label="Footnote 268">268</a></p>
			<p>Llama CLI</p>
			<p class="index-2">using  <a href="#_idIndexMarker998" aria-label="Footnote 278">278</a></p>
			<p>llama.cpp  <a href="#_idIndexMarker935" aria-label="Footnote 267">267</a></p>
			<p>LlamaHub</p>
			<p class="index-2">overview  <a href="#_idIndexMarker206" aria-label="Footnote 63">63</a></p>
			<p class="index-2">reference link  <a href="#_idIndexMarker101" aria-label="Footnote 36">36</a></p>
			<p class="index-2">used, for data ingestion  <a href="#_idIndexMarker201" aria-label="Footnote 62">62</a></p>
			<p>LlamaHub data loaders</p>
			<p class="index-2">data, ingesting from database  <a href="#_idIndexMarker212" aria-label="Footnote 65">65</a>, <a href="#_idIndexMarker214" aria-label="Footnote 66">66</a></p>
			<p class="index-2">data, ingesting from sources with multiple file formats  <a href="#_idIndexMarker215" aria-label="Footnote 66">66</a></p>
			<p class="index-2">data, ingesting from web page  <a href="#_idIndexMarker209" aria-label="Footnote 64">64</a>, <a href="#_idIndexMarker210" aria-label="Footnote 65">65</a></p>
			<p class="index-2">using, to ingest data  <a href="#_idIndexMarker208" aria-label="Footnote 64">64</a></p>
			<p>LlamaIndex  <a href="#_idIndexMarker034" aria-label="Footnote 13">13</a>, <a href="#_idIndexMarker043" aria-label="Footnote 16">16</a>, <a href="#_idIndexMarker045" aria-label="Footnote 17">17</a>, <a href="#_idIndexMarker089" aria-label="Footnote 34">34</a>, <a href="#_idIndexMarker781" aria-label="Footnote 226">226</a></p>
			<p class="index-2">advanced prompting techniques, using  <a href="#_idIndexMarker1095" aria-label="Footnote 309">309</a></p>
			<p class="index-2">built-in chat modes  <a href="#_idIndexMarker786" aria-label="Footnote 227">227</a></p>
			<p class="index-2">code repository structure  <a href="#_idIndexMarker086" aria-label="Footnote 28">28</a>, <a href="#_idIndexMarker087" aria-label="Footnote 29">29</a></p>
			<p class="index-2">generating, embeddings  <a href="#_idIndexMarker404" aria-label="Footnote 111">111</a></p>
			<p class="index-2">Hugging Face  <a href="#_idIndexMarker406" aria-label="Footnote 111">111</a>, <a href="#_idIndexMarker412" aria-label="Footnote 112">112</a></p>
			<p class="index-2">installing  <a href="#_idIndexMarker064" aria-label="Footnote 23">23</a></p>
			<p class="index-2">prompts, using  <a href="#_idIndexMarker1079" aria-label="Footnote 303">303</a>-<a href="#_idIndexMarker1085" aria-label="Footnote 305">305</a></p>
			<p class="index-2">reference link  <a href="#_idIndexMarker357" aria-label="Footnote 98">98</a>, <a href="#_idIndexMarker1099" aria-label="Footnote 309">309</a></p>
			<p class="index-2">sparse retrieval, implementing  <a href="#_idIndexMarker628" aria-label="Footnote 178">178</a>-<a href="#_idIndexMarker639" aria-label="Footnote 182">182</a></p>
			<p>LlamaIndex application</p>
			<p class="index-2">building  <a href="#_idIndexMarker172" aria-label="Footnote 48">48</a>, <a href="#_idIndexMarker173" aria-label="Footnote 49">49</a></p>
			<p class="index-2">code, adding  <a href="#_idIndexMarker182" aria-label="Footnote 51">51</a></p>
			<p class="index-2">LLM, customizing  <a href="#_idIndexMarker176" aria-label="Footnote 50">50</a></p>
			<p class="index-2">logging feature, using  <a href="#_idIndexMarker174" aria-label="Footnote 49">49</a></p>
			<p class="index-2">settings, customizing  <a href="#_idIndexMarker187" aria-label="Footnote 53">53</a>, <a href="#_idIndexMarker189" aria-label="Footnote 54">54</a></p>
			<p class="index-2">temperature parameter  <a href="#_idIndexMarker184" aria-label="Footnote 52">52</a>, <a href="#_idIndexMarker185" aria-label="Footnote 53">53</a></p>
			<p>LlamaIndex, core elements</p>
			<p class="index-2">Documents  <a href="#_idIndexMarker091" aria-label="Footnote 34">34</a>-<a href="#_idIndexMarker102" aria-label="Footnote 37">37</a></p>
			<p class="index-2">index  <a href="#_idIndexMarker138" aria-label="Footnote 43">43</a>-<a href="#_idIndexMarker157" aria-label="Footnote 45">45</a></p>
			<p class="index-2">Nodes  <a href="#_idIndexMarker107" aria-label="Footnote 37">37</a>, <a href="#_idIndexMarker109" aria-label="Footnote 38">38</a></p>
			<p>LlamaIndex examples  <a href="#_idIndexMarker1149" aria-label="Footnote 319">319</a></p>
			<p class="index-2">CitationQueryEngine, implementation  <a href="#_idIndexMarker1164" aria-label="Footnote 322">322</a></p>
			<p class="index-2">discord thread management  <a href="#_idIndexMarker1152" aria-label="Footnote 320">320</a></p>
			<p class="index-2">multi-modal retrieval application  <a href="#_idIndexMarker1156" aria-label="Footnote 320">320</a></p>
			<p class="index-2">multi-tenancy RAG example  <a href="#_idIndexMarker1158" aria-label="Footnote 321">321</a></p>
			<p class="index-2">prompt engineering techniques  <a href="#_idIndexMarker1160" aria-label="Footnote 321">321</a>, <a href="#_idIndexMarker1163" aria-label="Footnote 322">322</a></p>
			<p class="index-2">slack chat data connector  <a href="#_idIndexMarker1150" aria-label="Footnote 320">320</a></p>
			<p>LlamaIndex framework</p>
			<p class="index-2">learning  <a href="#_idIndexMarker1148" aria-label="Footnote 319">319</a></p>
			<p>Llama Packs</p>
			<p class="index-2">Plug and Play convenience, using  <a href="#_idIndexMarker988" aria-label="Footnote 276">276</a>-<a href="#_idIndexMarker996" aria-label="Footnote 278">278</a></p>
			<p>LlamaParse  <a href="#_idIndexMarker203" aria-label="Footnote 62">62</a>, <a href="#_idIndexMarker220" aria-label="Footnote 67">67</a>-<a href="#_idIndexMarker224" aria-label="Footnote 70">70</a></p>
			<p class="index-2">reference link  <a href="#_idIndexMarker221" aria-label="Footnote 68">68</a></p>
			<p>LLaVa</p>
			<p class="index-2">reference link  <a href="#_idIndexMarker966" aria-label="Footnote 273">273</a></p>
			<p>LLM-based re-ranking</p>
			<p class="index-2">effectiveness, measuring  <a href="#_idIndexMarker698" aria-label="Footnote 199">199</a>, <a href="#_idIndexMarker702" aria-label="Footnote 200">200</a></p>
			<p>LLMCompiler agent</p>
			<p class="index-2">executor  <a href="#_idIndexMarker896" aria-label="Footnote 252">252</a></p>
			<p class="index-2">implementing  <a href="#_idIndexMarker901" aria-label="Footnote 253">253</a></p>
			<p class="index-2">LLM planner  <a href="#_idIndexMarker894" aria-label="Footnote 252">252</a></p>
			<p class="index-2">structure  <a href="#_idIndexMarker897" aria-label="Footnote 252">252</a></p>
			<p class="index-2">task-fetching unit  <a href="#_idIndexMarker895" aria-label="Footnote 252">252</a></p>
			<p class="index-2">using, for advanced scenarios  <a href="#_idIndexMarker892" aria-label="Footnote 251">251</a>, <a href="#_idIndexMarker893" aria-label="Footnote 252">252</a></p>
			<p>LLM integrations</p>
			<p class="index-2">reference link  <a href="#_idIndexMarker939" aria-label="Footnote 267">267</a></p>
			<p>LLM quantization  <a href="#_idIndexMarker954" aria-label="Footnote 269">269</a>, <a href="#_idIndexMarker958" aria-label="Footnote 270">270</a></p>
			<p>LLMRerank  <a href="#_idIndexMarker686" aria-label="Footnote 198">198</a></p>
			<p>LLM task</p>
			<p class="index-2">chat models  <a href="#_idIndexMarker1126" aria-label="Footnote 314">314</a></p>
			<p class="index-2">codex models  <a href="#_idIndexMarker1128" aria-label="Footnote 315">315</a></p>
			<p class="index-2">inference speed  <a href="#_idIndexMarker1125" aria-label="Footnote 314">314</a></p>
			<p class="index-2">instruct models  <a href="#_idIndexMarker1127" aria-label="Footnote 314">314</a></p>
			<p class="index-2">model architecture  <a href="#_idIndexMarker1116" aria-label="Footnote 313">313</a></p>
			<p class="index-2">model size  <a href="#_idIndexMarker1123" aria-label="Footnote 313">313</a></p>
			<p class="index-2">question-answering models  <a href="#_idIndexMarker1131" aria-label="Footnote 315">315</a></p>
			<p class="index-2">selecting  <a href="#_idIndexMarker1115" aria-label="Footnote 313">313</a></p>
			<p class="index-2">summarization models  <a href="#_idIndexMarker1129" aria-label="Footnote 315">315</a></p>
			<p class="index-2">translation models  <a href="#_idIndexMarker1130" aria-label="Footnote 315">315</a></p>
			<p>LM Studio</p>
			<p class="index-2">local Inference Server interface  <a href="#_idIndexMarker963" aria-label="Footnote 272">272</a>, <a href="#_idIndexMarker965" aria-label="Footnote 273">273</a></p>
			<p class="index-2">model, selecting  <a href="#_idIndexMarker960" aria-label="Footnote 270">270</a>, <a href="#_idIndexMarker962" aria-label="Footnote 272">272</a></p>
			<p class="index-2">reference link  <a href="#_idIndexMarker944" aria-label="Footnote 267">267</a></p>
			<p class="index-2">used, for running local LLM  <a href="#_idIndexMarker943" aria-label="Footnote 267">267</a>, <a href="#_idIndexMarker952" aria-label="Footnote 269">269</a></p>
			<p>LMSYS Chatbot Arena Leaderboard</p>
			<p class="index-2">reference link  <a href="#_idIndexMarker948" aria-label="Footnote 268">268</a></p>
			<p>LoadAndSearchToolSpec utility  <a href="#_idIndexMarker880" aria-label="Footnote 248">248</a>-<a href="#_idIndexMarker884" aria-label="Footnote 250">250</a></p>
			<p>load_chat_store function  <a href="#_idIndexMarker918" aria-label="Footnote 257">257</a></p>
			<p>local AI models  <a href="#_idIndexMarker506" aria-label="Footnote 135">135</a></p>
			<p>local LLM</p>
			<p class="index-2">running, with LM Studio  <a href="#_idIndexMarker942" aria-label="Footnote 267">267</a>, <a href="#_idIndexMarker951" aria-label="Footnote 269">269</a></p>
			<p>logging  <a href="#_idIndexMarker175" aria-label="Footnote 49">49</a></p>
			<p>logging_functions.py module  <a href="#_idIndexMarker197" aria-label="Footnote 58">58</a></p>
			<p>LongContextReorder  <a href="#_idIndexMarker660" aria-label="Footnote 191">191</a></p>
			<p>LongLLMLinguaPostprocessor  <a href="#_idIndexMarker697" aria-label="Footnote 199">199</a></p>
			<p>low-level Agent Protocol API</p>
			<p class="index-2">end-to-end interaction, with chat() method  <a href="#_idIndexMarker912" aria-label="Footnote 256">256</a></p>
			<p class="index-2">step-by-step interaction, with create_ task() method  <a href="#_idIndexMarker913" aria-label="Footnote 256">256</a></p>
			<p class="index-2">using  <a href="#_idIndexMarker906" aria-label="Footnote 254">254</a>, <a href="#_idIndexMarker910" aria-label="Footnote 255">255</a></p>
			<p>low-level API  <a href="#_idIndexMarker745" aria-label="Footnote 210">210</a></p>
			<p>Low-Rank Adaptation (LoRA)  <a href="#_idIndexMarker041" aria-label="Footnote 15">15</a></p>
			<p class="index-head">M</p>
			<p>machine learning (ML)  <a href="#_idIndexMarker007" aria-label="Footnote 4">4</a></p>
			<p>MarkdownNodeParser  <a href="#_idIndexMarker257" aria-label="Footnote 75">75</a></p>
			<p>Marvin AI engineering framework  <a href="#_idIndexMarker314" aria-label="Footnote 87">87</a></p>
			<p>MarvinMetadataExtractor  <a href="#_idIndexMarker315" aria-label="Footnote 87">87</a></p>
			<p>Massive Text Embedding Benchmark (MTEB)  <a href="#_idIndexMarker415" aria-label="Footnote 113">113</a></p>
			<p>max_function_calls parameter  <a href="#_idIndexMarker858" aria-label="Footnote 244">244</a></p>
			<p>max_token parameter</p>
			<p class="index-2">working  <a href="#_idIndexMarker328" aria-label="Footnote 90">90</a></p>
			<p>Mean Reciprocal Rank (MRR)  <a href="#_idIndexMarker1039" aria-label="Footnote 289">289</a></p>
			<p>metadata  <a href="#_idIndexMarker095" aria-label="Footnote 35">35</a>, <a href="#_idIndexMarker285" aria-label="Footnote 81">81</a>, <a href="#_idIndexMarker321" aria-label="Footnote 87">87</a>, <a href="#_idIndexMarker322" aria-label="Footnote 88">88</a></p>
			<p class="index-2">custom extractor, defining  <a href="#_idIndexMarker318" aria-label="Footnote 87">87</a></p>
			<p class="index-2">EntityExtractor  <a href="#_idIndexMarker302" aria-label="Footnote 84">84</a>, <a href="#_idIndexMarker305" aria-label="Footnote 85">85</a></p>
			<p class="index-2">KeywordExtractor  <a href="#_idIndexMarker308" aria-label="Footnote 86">86</a></p>
			<p class="index-2">MarvinMetadataExtractor  <a href="#_idIndexMarker316" aria-label="Footnote 87">87</a></p>
			<p class="index-2">PydanticProgramExtractor  <a href="#_idIndexMarker309" aria-label="Footnote 86">86</a></p>
			<p class="index-2">QuestionsAnsweredExtractor  <a href="#_idIndexMarker293" aria-label="Footnote 83">83</a>, <a href="#_idIndexMarker294" aria-label="Footnote 84">84</a></p>
			<p class="index-2">SummaryExtractor  <a href="#_idIndexMarker290" aria-label="Footnote 83">83</a></p>
			<p class="index-2">TitleExtractor  <a href="#_idIndexMarker297" aria-label="Footnote 84">84</a></p>
			<p class="index-2">working with  <a href="#_idIndexMarker286" aria-label="Footnote 81">81</a>, <a href="#_idIndexMarker289" aria-label="Footnote 82">82</a></p>
			<p>metadata  <a href="#_idIndexMarker590" aria-label="Footnote 165">165</a></p>
			<p>metadata extractors  <a href="#_idIndexMarker288" aria-label="Footnote 82">82</a></p>
			<p>metadata extractors, cost estimation  <a href="#_idIndexMarker323" aria-label="Footnote 88">88</a></p>
			<p class="index-2">best practices  <a href="#_idIndexMarker324" aria-label="Footnote 88">88</a></p>
			<p class="index-2">example  <a href="#_idIndexMarker325" aria-label="Footnote 89">89</a>-<a href="#_idIndexMarker332" aria-label="Footnote 91">91</a></p>
			<p>MetadataReplacementPostProcessor  <a href="#_idIndexMarker665" aria-label="Footnote 193">193</a>, <a href="#_idIndexMarker666" aria-label="Footnote 194">194</a></p>
			<p>Mistral-7B  <a href="#_idIndexMarker946" aria-label="Footnote 268">268</a></p>
			<p>Mixtral 8x7B  <a href="#_idIndexMarker1122" aria-label="Footnote 313">313</a></p>
			<p>Mixture-of-Experts (MoE)  <a href="#_idIndexMarker1121" aria-label="Footnote 313">313</a></p>
			<p>MockLLM  <a href="#_idIndexMarker326" aria-label="Footnote 89">89</a></p>
			<p>model drift  <a href="#_idIndexMarker701" aria-label="Footnote 200">200</a></p>
			<p>model drift forms</p>
			<p class="index-2">concept drift  <a href="#_idIndexMarker706" aria-label="Footnote 200">200</a></p>
			<p class="index-2">data drift  <a href="#_idIndexMarker704" aria-label="Footnote 200">200</a></p>
			<p class="index-2">domain shift  <a href="#_idIndexMarker710" aria-label="Footnote 201">201</a></p>
			<p class="index-2">feedback loops  <a href="#_idIndexMarker709" aria-label="Footnote 201">201</a></p>
			<p class="index-2">temporal drift  <a href="#_idIndexMarker711" aria-label="Footnote 201">201</a></p>
			<p class="index-2">upstream data change  <a href="#_idIndexMarker708" aria-label="Footnote 200">200</a></p>
			<p>Model Memory Calculator</p>
			<p class="index-2">reference link  <a href="#_idIndexMarker959" aria-label="Footnote 270">270</a></p>
			<p>multi-modal RAG  <a href="#_idIndexMarker1157" aria-label="Footnote 321">321</a></p>
			<p>multiple documents</p>
			<p class="index-2">querying, with SubQuestionQueryEngine  <a href="#_idIndexMarker761" aria-label="Footnote 216">216</a>-<a href="#_idIndexMarker765" aria-label="Footnote 218">218</a></p>
			<p>multi-tenancy RAG example  <a href="#_idIndexMarker1159" aria-label="Footnote 321">321</a></p>
			<p class="index-head">N</p>
			<p>naive methods  <a href="#_idIndexMarker518" aria-label="Footnote 144">144</a>, <a href="#_idIndexMarker586" aria-label="Footnote 164">164</a></p>
			<p>named entity recognition (NER)  <a href="#_idIndexMarker303" aria-label="Footnote 84">84</a></p>
			<p>natural language generation (NLG)  <a href="#_idIndexMarker010" aria-label="Footnote 4">4</a></p>
			<p>natural language (NL)  <a href="#_idIndexMarker012" aria-label="Footnote 5">5</a>, <a href="#_idIndexMarker1071" aria-label="Footnote 299">299</a></p>
			<p>natural language processing (NLP)  <a href="#_idIndexMarker180" aria-label="Footnote 50">50</a>, <a href="#_idIndexMarker408" aria-label="Footnote 111">111</a>, <a href="#_idIndexMarker1075" aria-label="Footnote 302">302</a></p>
			<p>Natural Language to Graph Query (NL2GraphQuery)  <a href="#_idIndexMarker575" aria-label="Footnote 161">161</a></p>
			<p>Natural Language Toolkit (NLTK)  <a href="#_idIndexMarker304" aria-label="Footnote 84">84</a></p>
			<p>NERPIINodePostprocessor  <a href="#_idIndexMarker664" aria-label="Footnote 192">192</a></p>
			<p>neural networks (NNs)  <a href="#_idIndexMarker008" aria-label="Footnote 4">4</a></p>
			<p>Neutrino  <a href="#_idIndexMarker972" aria-label="Footnote 273">273</a>-<a href="#_idIndexMarker982" aria-label="Footnote 276">276</a></p>
			<p class="index-2">reference link  <a href="#_idIndexMarker970" aria-label="Footnote 273">273</a></p>
			<p>next sentence prediction (NSP)  <a href="#_idIndexMarker1118" aria-label="Footnote 313">313</a></p>
			<p>NL processing (NLP)  <a href="#_idIndexMarker026" aria-label="Footnote 11">11</a></p>
			<p>node creation models  <a href="#_idIndexMarker281" aria-label="Footnote 80">80</a>, <a href="#_idIndexMarker283" aria-label="Footnote 81">81</a></p>
			<p>node filtering postprocessors</p>
			<p class="index-2">exploring  <a href="#_idIndexMarker641" aria-label="Footnote 185">185</a></p>
			<p>Node objects</p>
			<p class="index-2">creating, manually  <a href="#_idIndexMarker114" aria-label="Footnote 39">39</a></p>
			<p>node parsers  <a href="#_idIndexMarker229" aria-label="Footnote 70">70</a>, <a href="#_idIndexMarker269" aria-label="Footnote 77">77</a></p>
			<p>node postprocessor  <a href="#_idIndexMarker165" aria-label="Footnote 46">46</a>, <a href="#_idIndexMarker339" aria-label="Footnote 92">92</a>, <a href="#_idIndexMarker713" aria-label="Footnote 201">201</a></p>
			<p>node re-ranking postprocessors</p>
			<p class="index-2">exploring  <a href="#_idIndexMarker643" aria-label="Footnote 186">186</a></p>
			<p>nodes  <a href="#_idIndexMarker108" aria-label="Footnote 37">37</a>, <a href="#_idIndexMarker110" aria-label="Footnote 38">38</a>, <a href="#_idIndexMarker169" aria-label="Footnote 48">48</a>, <a href="#_idIndexMarker225" aria-label="Footnote 70">70</a></p>
			<p class="index-2">child relationship  <a href="#_idIndexMarker132" aria-label="Footnote 42">42</a></p>
			<p class="index-2">documents, parsing  <a href="#_idIndexMarker227" aria-label="Footnote 70">70</a></p>
			<p class="index-2">extracting automatically, from Documents  <a href="#_idIndexMarker116" aria-label="Footnote 39">39</a>-<a href="#_idIndexMarker123" aria-label="Footnote 41">41</a></p>
			<p class="index-2">parent relationship  <a href="#_idIndexMarker131" aria-label="Footnote 42">42</a></p>
			<p class="index-2">relationship, creating  <a href="#_idIndexMarker126" aria-label="Footnote 41">41</a>, <a href="#_idIndexMarker134" aria-label="Footnote 42">42</a></p>
			<p class="index-2">source relationship  <a href="#_idIndexMarker130" aria-label="Footnote 42">42</a></p>
			<p>nodes types</p>
			<p class="index-2">table nodes  <a href="#_idIndexMarker355" aria-label="Footnote 98">98</a></p>
			<p class="index-2">text nodes  <a href="#_idIndexMarker353" aria-label="Footnote 98">98</a></p>
			<p>node transforming postprocessors</p>
			<p class="index-2">exploring  <a href="#_idIndexMarker642" aria-label="Footnote 186">186</a></p>
			<p>NotePad++</p>
			<p class="index-2">reference link  <a href="#_idIndexMarker056" aria-label="Footnote 22">22</a></p>
			<p class="index-head">O</p>
			<p>OnDemandLoaderTool  <a href="#_idIndexMarker886" aria-label="Footnote 250">250</a>, <a href="#_idIndexMarker890" aria-label="Footnote 251">251</a></p>
			<p>OpenAIAgent  <a href="#_idIndexMarker848" aria-label="Footnote 241">241</a>, <a href="#_idIndexMarker850" aria-label="Footnote 242">242</a></p>
			<p class="index-2">implementing  <a href="#_idIndexMarker851" aria-label="Footnote 242">242</a>-<a href="#_idIndexMarker861" aria-label="Footnote 245">245</a></p>
			<p class="index-2">parameters  <a href="#_idIndexMarker863" aria-label="Footnote 245">245</a></p>
			<p>OpenAI API key</p>
			<p class="index-2">for Linux users  <a href="#_idIndexMarker074" aria-label="Footnote 25">25</a></p>
			<p class="index-2">for Mac users  <a href="#_idIndexMarker075" aria-label="Footnote 26">26</a></p>
			<p class="index-2">for Windows users  <a href="#_idIndexMarker071" aria-label="Footnote 24">24</a>, <a href="#_idIndexMarker073" aria-label="Footnote 25">25</a></p>
			<p class="index-2">reference link  <a href="#_idIndexMarker070" aria-label="Footnote 23">23</a></p>
			<p class="index-2">signing up  <a href="#_idIndexMarker069" aria-label="Footnote 23">23</a></p>
			<p>OpenRouter  <a href="#_idIndexMarker974" aria-label="Footnote 274">274</a>-<a href="#_idIndexMarker981" aria-label="Footnote 276">276</a></p>
			<p class="index-2">reference link  <a href="#_idIndexMarker971" aria-label="Footnote 273">273</a></p>
			<p>output parsers  <a href="#_idIndexMarker723" aria-label="Footnote 206">206</a></p>
			<p class="index-2">GuardrailsOutputParser  <a href="#_idIndexMarker728" aria-label="Footnote 206">206</a>, <a href="#_idIndexMarker731" aria-label="Footnote 207">207</a></p>
			<p class="index-2">LangchainOutputParser  <a href="#_idIndexMarker734" aria-label="Footnote 207">207</a>-<a href="#_idIndexMarker738" aria-label="Footnote 209">209</a></p>
			<p class="index-2">used, for extracting structured outputs  <a href="#_idIndexMarker724" aria-label="Footnote 206">206</a></p>
			<p>output parsing techniques</p>
			<p class="index-2">implementing  <a href="#_idIndexMarker721" aria-label="Footnote 205">205</a></p>
			<p class="index-head">P</p>
			<p>personalized intelligent tutoring system (PITS)  <a href="#_idIndexMarker036" aria-label="Footnote 13">13</a>, <a href="#_idIndexMarker050" aria-label="Footnote 19">19</a></p>
			<p class="index-2">working  <a href="#_idIndexMarker051" aria-label="Footnote 19">19</a>-<a href="#_idIndexMarker053" aria-label="Footnote 21">21</a></p>
			<p>personally identifiable information (PII)  <a href="#_idIndexMarker340" aria-label="Footnote 93">93</a>, <a href="#_idIndexMarker661" aria-label="Footnote 192">192</a></p>
			<p>Phoenix</p>
			<p class="index-2">used, for tracing RAG workflows  <a href="#_idIndexMarker1012" aria-label="Footnote 281">281</a>-<a href="#_idIndexMarker1020" aria-label="Footnote 284">284</a></p>
			<p>Phoenix framework</p>
			<p class="index-2">reference link  <a href="#_idIndexMarker1008" aria-label="Footnote 281">281</a></p>
			<p>Phoenix framework evaluation features</p>
			<p class="index-2">LLM inference  <a href="#_idIndexMarker1024" aria-label="Footnote 285">285</a></p>
			<p class="index-2">retrieval  <a href="#_idIndexMarker1025" aria-label="Footnote 285">285</a></p>
			<p class="index-2">using  <a href="#_idIndexMarker1023" aria-label="Footnote 285">285</a>-<a href="#_idIndexMarker1034" aria-label="Footnote 288">288</a></p>
			<p>PIINodePostprocessor  <a href="#_idIndexMarker662" aria-label="Footnote 192">192</a></p>
			<p class="index-2">arguments  <a href="#_idIndexMarker663" aria-label="Footnote 192">192</a></p>
			<p>PITS project</p>
			<p class="index-2">building  <a href="#_idIndexMarker358" aria-label="Footnote 98">98</a>-<a href="#_idIndexMarker361" aria-label="Footnote 100">100</a></p>
			<p class="index-2">code structure  <a href="#_idIndexMarker191" aria-label="Footnote 55">55</a></p>
			<p class="index-2">deploying, on Streamlit Community Cloud  <a href="#_idIndexMarker1061" aria-label="Footnote 294">294</a>-<a href="#_idIndexMarker1069" aria-label="Footnote 297">297</a></p>
			<p class="index-2">quizzes, building  <a href="#_idIndexMarker769" aria-label="Footnote 219">219</a>-<a href="#_idIndexMarker772" aria-label="Footnote 221">221</a></p>
			<p class="index-2">source code  <a href="#_idIndexMarker194" aria-label="Footnote 56">56</a>-<a href="#_idIndexMarker198" aria-label="Footnote 58">58</a></p>
			<p class="index-2">starting  <a href="#_idIndexMarker190" aria-label="Footnote 54">54</a>, <a href="#_idIndexMarker193" aria-label="Footnote 56">56</a></p>
			<p>PITS study materials</p>
			<p class="index-2">indexing  <a href="#_idIndexMarker511" aria-label="Footnote 138">138</a>, <a href="#_idIndexMarker514" aria-label="Footnote 139">139</a></p>
			<p>Plug and Play convenience</p>
			<p class="index-2">with Llama Packs  <a href="#_idIndexMarker987" aria-label="Footnote 276">276</a>-<a href="#_idIndexMarker997" aria-label="Footnote 278">278</a></p>
			<p>PrevNextNodePostprocessor  <a href="#_idIndexMarker656" aria-label="Footnote 190">190</a>, <a href="#_idIndexMarker657" aria-label="Footnote 191">191</a></p>
			<p class="index-2">parameters  <a href="#_idIndexMarker658" aria-label="Footnote 191">191</a></p>
			<p>progressive disclosure of complexity  <a href="#_idIndexMarker047" aria-label="Footnote 18">18</a></p>
			<p class="index-2">advantages  <a href="#_idIndexMarker046" aria-label="Footnote 18">18</a></p>
			<p class="index-2">aspects  <a href="#_idIndexMarker049" aria-label="Footnote 19">19</a></p>
			<p>prompt chaining  <a href="#_idIndexMarker1147" aria-label="Footnote 318">318</a></p>
			<p>prompt engineering  <a href="#_idIndexMarker038" aria-label="Footnote 14">14</a>, <a href="#_idIndexMarker1076" aria-label="Footnote 302">302</a></p>
			<p class="index-2">LLM task, selecting  <a href="#_idIndexMarker1114" aria-label="Footnote 313">313</a></p>
			<p>prompt engineering rules  <a href="#_idIndexMarker1100" aria-label="Footnote 310">310</a></p>
			<p class="index-2">context quality  <a href="#_idIndexMarker1103" aria-label="Footnote 310">310</a></p>
			<p class="index-2">context quantity  <a href="#_idIndexMarker1104" aria-label="Footnote 311">311</a></p>
			<p class="index-2">directiveness  <a href="#_idIndexMarker1102" aria-label="Footnote 310">310</a></p>
			<p class="index-2">effective prompts methods, creating  <a href="#_idIndexMarker1132" aria-label="Footnote 316">316</a></p>
			<p class="index-2">expression accuracy and clarity  <a href="#_idIndexMarker1101" aria-label="Footnote 310">310</a></p>
			<p class="index-2">inference cost  <a href="#_idIndexMarker1110" aria-label="Footnote 312">312</a></p>
			<p class="index-2">required output format  <a href="#_idIndexMarker1109" aria-label="Footnote 312">312</a></p>
			<p class="index-2">system latency  <a href="#_idIndexMarker1112" aria-label="Footnote 312">312</a></p>
			<p>prompt engineering techniques  <a href="#_idIndexMarker1161" aria-label="Footnote 321">321</a>, <a href="#_idIndexMarker1162" aria-label="Footnote 322">322</a></p>
			<p>prompts</p>
			<p class="index-2">need for  <a href="#_idIndexMarker1072" aria-label="Footnote 300">300</a>-<a href="#_idIndexMarker1078" aria-label="Footnote 302">302</a></p>
			<p class="index-2">using, in LlamaIndex  <a href="#_idIndexMarker1080" aria-label="Footnote 303">303</a>-<a href="#_idIndexMarker1086" aria-label="Footnote 305">305</a></p>
			<p>PyCharm</p>
			<p class="index-2">reference link  <a href="#_idIndexMarker058" aria-label="Footnote 22">22</a></p>
			<p>Pydantic model  <a href="#_idIndexMarker311" aria-label="Footnote 86">86</a></p>
			<p>PydanticProgramExtractor  <a href="#_idIndexMarker310" aria-label="Footnote 86">86</a></p>
			<p>Pydantic programs  <a href="#_idIndexMarker722" aria-label="Footnote 206">206</a></p>
			<p class="index-2">used, for extracting structured outputs  <a href="#_idIndexMarker741" aria-label="Footnote 209">209</a></p>
			<p class="index-head">Q</p>
			<p>quantization level  <a href="#_idIndexMarker953" aria-label="Footnote 269">269</a></p>
			<p>QueryEngine  <a href="#_idIndexMarker164" aria-label="Footnote 46">46</a>-<a href="#_idIndexMarker171" aria-label="Footnote 48">48</a></p>
			<p>QueryEngine class  <a href="#_idIndexMarker783" aria-label="Footnote 226">226</a></p>
			<p>QueryEngine interface</p>
			<p class="index-2">advanced usage  <a href="#_idIndexMarker748" aria-label="Footnote 211">211</a>, <a href="#_idIndexMarker750" aria-label="Footnote 214">214</a></p>
			<p>query engines</p>
			<p class="index-2">building  <a href="#_idIndexMarker742" aria-label="Footnote 210">210</a></p>
			<p class="index-2">methods, exploring  <a href="#_idIndexMarker743" aria-label="Footnote 210">210</a>, <a href="#_idIndexMarker747" aria-label="Footnote 211">211</a></p>
			<p>QueryEngineTool  <a href="#_idIndexMarker833" aria-label="Footnote 237">237</a></p>
			<p>querying  <a href="#_idIndexMarker516" aria-label="Footnote 144">144</a></p>
			<p>query mechanics</p>
			<p class="index-2">overview  <a href="#_idIndexMarker517" aria-label="Footnote 144">144</a></p>
			<p class="index-2">postprocessing  <a href="#_idIndexMarker521" aria-label="Footnote 144">144</a></p>
			<p class="index-2">response synthesis  <a href="#_idIndexMarker520" aria-label="Footnote 144">144</a></p>
			<p class="index-2">retrieval  <a href="#_idIndexMarker519" aria-label="Footnote 144">144</a></p>
			<p>QuestionsAnsweredExtractor  <a href="#_idIndexMarker292" aria-label="Footnote 83">83</a>, <a href="#_idIndexMarker295" aria-label="Footnote 84">84</a></p>
			<p class="index-head">R</p>
			<p>RAG CLI</p>
			<p class="index-2">reference link  <a href="#_idIndexMarker1005" aria-label="Footnote 280">280</a></p>
			<p>RAG components</p>
			<p class="index-2">customizing  <a href="#_idIndexMarker931" aria-label="Footnote 266">266</a></p>
			<p>RAG system</p>
			<p class="index-2">evaluating  <a href="#_idIndexMarker1021" aria-label="Footnote 284">284</a></p>
			<p class="index-2">Phoenix framework evaluation features, using  <a href="#_idIndexMarker1022" aria-label="Footnote 285">285</a>-<a href="#_idIndexMarker1033" aria-label="Footnote 288">288</a></p>
			<p>RAG workflows</p>
			<p class="index-2">tracing, with Phoenix  <a href="#_idIndexMarker1011" aria-label="Footnote 281">281</a>-<a href="#_idIndexMarker1019" aria-label="Footnote 284">284</a></p>
			<p>RAKE extraction method  <a href="#_idIndexMarker466" aria-label="Footnote 124">124</a></p>
			<p>RankGPTRerank  <a href="#_idIndexMarker693" aria-label="Footnote 198">198</a></p>
			<p>ReActAgent  <a href="#_idIndexMarker867" aria-label="Footnote 246">246</a></p>
			<p class="index-2">parameters  <a href="#_idIndexMarker871" aria-label="Footnote 246">246</a>, <a href="#_idIndexMarker872" aria-label="Footnote 247">247</a></p>
			<p>ReAct loop  <a href="#_idIndexMarker868" aria-label="Footnote 246">246</a></p>
			<p>readers  <a href="#_idIndexMarker100" aria-label="Footnote 36">36</a></p>
			<p>read-eval-print loop (REPL)   <a href="#_idIndexMarker784" aria-label="Footnote 227">227</a>, <a href="#_idIndexMarker1001" aria-label="Footnote 279">279</a></p>
			<p>reasoning loop  <a href="#_idIndexMarker831" aria-label="Footnote 237">237</a>, <a href="#_idIndexMarker844" aria-label="Footnote 240">240</a>, <a href="#_idIndexMarker846" aria-label="Footnote 241">241</a></p>
			<p>Recall@k  <a href="#_idIndexMarker1038" aria-label="Footnote 289">289</a></p>
			<p>Recall-Oriented Understudy for Gisting Evaluation (ROUGE)  <a href="#_idIndexMarker1041" aria-label="Footnote 289">289</a></p>
			<p>RedisChatStore  <a href="#_idIndexMarker790" aria-label="Footnote 227">227</a></p>
			<p>relational parsers  <a href="#_idIndexMarker261" aria-label="Footnote 75">75</a></p>
			<p class="index-2">HierarchicalNodeParser  <a href="#_idIndexMarker263" aria-label="Footnote 76">76</a></p>
			<p class="index-2">UnstructuredElementNodeParser  <a href="#_idIndexMarker267" aria-label="Footnote 77">77</a></p>
			<p class="index-2">using  <a href="#_idIndexMarker262" aria-label="Footnote 75">75</a></p>
			<p>relationships</p>
			<p class="index-2">creating, between nodes  <a href="#_idIndexMarker127" aria-label="Footnote 41">41</a>, <a href="#_idIndexMarker133" aria-label="Footnote 42">42</a></p>
			<p class="index-2">significance  <a href="#_idIndexMarker135" aria-label="Footnote 43">43</a></p>
			<p>Reliable AI Markup Language (RAIL)  <a href="#_idIndexMarker729" aria-label="Footnote 206">206</a></p>
			<p>required output format  <a href="#_idIndexMarker1108" aria-label="Footnote 312">312</a></p>
			<p>re-ranking postprocessors  <a href="#_idIndexMarker684" aria-label="Footnote 197">197</a></p>
			<p class="index-2">CohereRerank  <a href="#_idIndexMarker689" aria-label="Footnote 198">198</a></p>
			<p class="index-2">LLMRerank  <a href="#_idIndexMarker685" aria-label="Footnote 198">198</a></p>
			<p class="index-2">LongLLMLinguaPostprocessor  <a href="#_idIndexMarker696" aria-label="Footnote 199">199</a></p>
			<p class="index-2">RankGPTRerank  <a href="#_idIndexMarker692" aria-label="Footnote 198">198</a></p>
			<p class="index-2">SentenceTransformerRerank  <a href="#_idIndexMarker691" aria-label="Footnote 198">198</a></p>
			<p>response_mode</p>
			<p class="index-2">parameter  <a href="#_idIndexMarker718" aria-label="Footnote 204">204</a></p>
			<p>response schema  <a href="#_idIndexMarker733" aria-label="Footnote 207">207</a></p>
			<p>response synthesizers  <a href="#_idIndexMarker162" aria-label="Footnote 45">45</a>, <a href="#_idIndexMarker714" aria-label="Footnote 201">201</a>-<a href="#_idIndexMarker720" aria-label="Footnote 204">204</a></p>
			<p>Retrieval-Augmented Generation Assessment (RAGAS)  <a href="#_idIndexMarker1036" aria-label="Footnote 289">289</a>, <a href="#_idIndexMarker1045" aria-label="Footnote 290">290</a></p>
			<p class="index-2">features  <a href="#_idIndexMarker1037" aria-label="Footnote 289">289</a></p>
			<p>retrieval-augmented generation (RAG)  <a href="#_idIndexMarker001" aria-label="Footnote 3">3</a>, <a href="#_idIndexMarker035" aria-label="Footnote 13">13</a>, <a href="#_idIndexMarker040" aria-label="Footnote 14">14</a>, <a href="#_idIndexMarker090" aria-label="Footnote 34">34</a></p>
			<p class="index-2">used, for augmenting LLMs  <a href="#_idIndexMarker028" aria-label="Footnote 11">11</a>, <a href="#_idIndexMarker032" aria-label="Footnote 12">12</a></p>
			<p>Retrieval-Augmented Generation (RAG)  <a href="#_idIndexMarker929" aria-label="Footnote 265">265</a></p>
			<p class="index-2">working, in command line  <a href="#_idIndexMarker999" aria-label="Footnote 279">279</a>, <a href="#_idIndexMarker1003" aria-label="Footnote 280">280</a></p>
			<p>Retrieval Dependency (RD)  <a href="#_idIndexMarker1042" aria-label="Footnote 289">289</a></p>
			<p>retrieval mechanisms  <a href="#_idIndexMarker523" aria-label="Footnote 144">144</a>, <a href="#_idIndexMarker526" aria-label="Footnote 145">145</a></p>
			<p>retrieval modes  <a href="#_idIndexMarker574" aria-label="Footnote 161">161</a></p>
			<p>Retrieval Relevance (RR)  <a href="#_idIndexMarker1043" aria-label="Footnote 289">289</a></p>
			<p>retrievers  <a href="#_idIndexMarker160" aria-label="Footnote 45">45</a>, <a href="#_idIndexMarker364" aria-label="Footnote 102">102</a>, <a href="#_idIndexMarker522" aria-label="Footnote 144">144</a>, <a href="#_idIndexMarker527" aria-label="Footnote 145">145</a></p>
			<p class="index-2">asynchronous operation  <a href="#_idIndexMarker579" aria-label="Footnote 163">163</a></p>
			<p class="index-2">characteristics  <a href="#_idIndexMarker578" aria-label="Footnote 162">162</a></p>
			<p class="index-2">DocumentSummaryIndex retrievers  <a href="#_idIndexMarker543" aria-label="Footnote 150">150</a></p>
			<p class="index-2">KnowledgeGraphIndex retrievers  <a href="#_idIndexMarker565" aria-label="Footnote 158">158</a></p>
			<p class="index-2">TreeIndex retrievers  <a href="#_idIndexMarker549" aria-label="Footnote 152">152</a></p>
			<p class="index-2">VectorStoreIndex retrievers  <a href="#_idIndexMarker528" aria-label="Footnote 145">145</a></p>
			<p class="index-2">versus dense retrieval  <a href="#_idIndexMarker633" aria-label="Footnote 179">179</a></p>
			<p>rogue agents  <a href="#_idIndexMarker859" aria-label="Footnote 244">244</a></p>
			<p>role-based access control (RBAC)  <a href="#_idIndexMarker1052" aria-label="Footnote 291">291</a></p>
			<p>RouterQueryEngine</p>
			<p class="index-2">used, for implementing advanced routing  <a href="#_idIndexMarker751" aria-label="Footnote 214">214</a>-<a href="#_idIndexMarker757" aria-label="Footnote 216">216</a></p>
			<p class="index-head">S</p>
			<p>scalar product  <a href="#_idIndexMarker393" aria-label="Footnote 109">109</a></p>
			<p>selectors  <a href="#_idIndexMarker596" aria-label="Footnote 167">167</a></p>
			<p>self-consistency  <a href="#_idIndexMarker1141" aria-label="Footnote 317">317</a></p>
			<p>SentenceEmbeddingOptimizer  <a href="#_idIndexMarker667" aria-label="Footnote 194">194</a>, <a href="#_idIndexMarker670" aria-label="Footnote 195">195</a></p>
			<p class="index-2">need for  <a href="#_idIndexMarker668" aria-label="Footnote 194">194</a></p>
			<p class="index-2">parameters  <a href="#_idIndexMarker669" aria-label="Footnote 195">195</a></p>
			<p>SentenceSplitter  <a href="#_idIndexMarker232" aria-label="Footnote 71">71</a>, <a href="#_idIndexMarker243" aria-label="Footnote 73">73</a></p>
			<p>SentenceTransformerRerank  <a href="#_idIndexMarker690" aria-label="Footnote 198">198</a></p>
			<p>SentenceWindowNodeParser  <a href="#_idIndexMarker245" aria-label="Footnote 73">73</a></p>
			<p class="index-2">parameters  <a href="#_idIndexMarker246" aria-label="Footnote 73">73</a></p>
			<p>similarity measure  <a href="#_idIndexMarker376" aria-label="Footnote 105">105</a></p>
			<p class="index-2">reference link  <a href="#_idIndexMarker403" aria-label="Footnote 111">111</a></p>
			<p>SimilarityPostprocessor  <a href="#_idIndexMarker645" aria-label="Footnote 186">186</a>-<a href="#_idIndexMarker647" aria-label="Footnote 188">188</a></p>
			<p>similarity search  <a href="#_idIndexMarker381" aria-label="Footnote 107">107</a>, <a href="#_idIndexMarker387" aria-label="Footnote 108">108</a></p>
			<p class="index-2">cosine similarity  <a href="#_idIndexMarker388" aria-label="Footnote 108">108</a>, <a href="#_idIndexMarker391" aria-label="Footnote 109">109</a></p>
			<p class="index-2">dot product  <a href="#_idIndexMarker392" aria-label="Footnote 109">109</a>, <a href="#_idIndexMarker395" aria-label="Footnote 110">110</a></p>
			<p class="index-2">Euclidean distance  <a href="#_idIndexMarker399" aria-label="Footnote 110">110</a>, <a href="#_idIndexMarker402" aria-label="Footnote 111">111</a></p>
			<p>similarity searches  <a href="#_idIndexMarker370" aria-label="Footnote 103">103</a></p>
			<p>SimpleChatEngine  <a href="#_idIndexMarker799" aria-label="Footnote 230">230</a></p>
			<p>SimpleChatStore  <a href="#_idIndexMarker789" aria-label="Footnote 227">227</a></p>
			<p>SimpleDirectoryReader  <a href="#_idIndexMarker218" aria-label="Footnote 67">67</a></p>
			<p class="index-2">using, to ingest multiple data formats  <a href="#_idIndexMarker216" aria-label="Footnote 67">67</a></p>
			<p>SimpleFileNodeParser  <a href="#_idIndexMarker251" aria-label="Footnote 74">74</a></p>
			<p>slack chat data connector  <a href="#_idIndexMarker1151" aria-label="Footnote 320">320</a></p>
			<p>spaCy library</p>
			<p class="index-2">reference link  <a href="#_idIndexMarker650" aria-label="Footnote 189">189</a></p>
			<p>span-marker package  <a href="#_idIndexMarker301" aria-label="Footnote 84">84</a></p>
			<p>sparse retrieval  <a href="#_idIndexMarker613" aria-label="Footnote 175">175</a>, <a href="#_idIndexMarker619" aria-label="Footnote 176">176</a></p>
			<p class="index-2">implementing, in LlamaIndex  <a href="#_idIndexMarker627" aria-label="Footnote 178">178</a>-<a href="#_idIndexMarker640" aria-label="Footnote 182">182</a></p>
			<p class="index-2">TF-IDF  <a href="#_idIndexMarker621" aria-label="Footnote 176">176</a>-<a href="#_idIndexMarker626" aria-label="Footnote 178">178</a></p>
			<p>sparse search  <a href="#_idIndexMarker533" aria-label="Footnote 147">147</a></p>
			<p>splitters</p>
			<p class="index-2">used, for extracting nodes from documents  <a href="#_idIndexMarker117" aria-label="Footnote 39">39</a>-<a href="#_idIndexMarker124" aria-label="Footnote 41">41</a></p>
			<p>SQLAlchemy Engine  <a href="#_idIndexMarker213" aria-label="Footnote 66">66</a></p>
			<p>SQLAlchemy library</p>
			<p class="index-2">reference link  <a href="#_idIndexMarker840" aria-label="Footnote 239">239</a></p>
			<p>stacking Indexes  <a href="#_idIndexMarker496" aria-label="Footnote 131">131</a></p>
			<p>state-of-the-art (SOTA)  <a href="#_idIndexMarker587" aria-label="Footnote 164">164</a></p>
			<p>StorageContext  <a href="#_idIndexMarker423" aria-label="Footnote 115">115</a>, <a href="#_idIndexMarker432" aria-label="Footnote 117">117</a></p>
			<p class="index-2">components  <a href="#_idIndexMarker424" aria-label="Footnote 115">115</a></p>
			<p>Streamlit  <a href="#_idIndexMarker077" aria-label="Footnote 26">26</a></p>
			<p class="index-2">deployment with  <a href="#_idIndexMarker1046" aria-label="Footnote 290">290</a></p>
			<p class="index-2">installing  <a href="#_idIndexMarker080" aria-label="Footnote 26">26</a></p>
			<p class="index-2">providing, web deployment solutions  <a href="#_idIndexMarker1047" aria-label="Footnote 291">291</a></p>
			<p class="index-2">session state  <a href="#_idIndexMarker1057" aria-label="Footnote 293">293</a></p>
			<p class="index-2">reference link  <a href="#_idIndexMarker1055" aria-label="Footnote 292">292</a></p>
			<p>Streamlit Community Cloud</p>
			<p class="index-2">PITS project, deploying  <a href="#_idIndexMarker1060" aria-label="Footnote 294">294</a>-<a href="#_idIndexMarker1070" aria-label="Footnote 297">297</a></p>
			<p>Streamlit Community Cloud, app deployment </p>
			<p class="index-2">reference link  <a href="#_idIndexMarker1068" aria-label="Footnote 297">297</a></p>
			<p>structured outputs</p>
			<p class="index-2">extracting, with output parsers  <a href="#_idIndexMarker725" aria-label="Footnote 206">206</a></p>
			<p class="index-2">extracting, with Pydantic programs  <a href="#_idIndexMarker740" aria-label="Footnote 209">209</a></p>
			<p>SubQuestionQueryEngine</p>
			<p class="index-2">used, for querying multiple documents  <a href="#_idIndexMarker762" aria-label="Footnote 216">216</a>-<a href="#_idIndexMarker766" aria-label="Footnote 218">218</a></p>
			<p>SummaryExtractor  <a href="#_idIndexMarker291" aria-label="Footnote 83">83</a></p>
			<p>SummaryIndex  <a href="#_idIndexMarker142" aria-label="Footnote 44">44</a>, <a href="#_idIndexMarker158" aria-label="Footnote 45">45</a>, <a href="#_idIndexMarker441" aria-label="Footnote 118">118</a>, <a href="#_idIndexMarker443" aria-label="Footnote 119">119</a></p>
			<p class="index-2">practical use case  <a href="#_idIndexMarker445" aria-label="Footnote 119">119</a></p>
			<p class="index-2">usage model  <a href="#_idIndexMarker447" aria-label="Footnote 119">119</a></p>
			<p class="index-2">workings  <a href="#_idIndexMarker449" aria-label="Footnote 120">120</a></p>
			<p>synchronous methods  <a href="#_idIndexMarker580" aria-label="Footnote 163">163</a></p>
			<p>system latency  <a href="#_idIndexMarker1113" aria-label="Footnote 312">312</a></p>
			<p class="index-head">T</p>
			<p>table nodes  <a href="#_idIndexMarker356" aria-label="Footnote 98">98</a></p>
			<p>tabular data documents</p>
			<p class="index-2">handling  <a href="#_idIndexMarker352" aria-label="Footnote 97">97</a></p>
			<p>Term Frequency - Inverse Document Frequency (TF-IDF)  <a href="#_idIndexMarker620" aria-label="Footnote 176">176</a></p>
			<p>Term Frequency (TF)  <a href="#_idIndexMarker622" aria-label="Footnote 176">176</a></p>
			<p>text data documents</p>
			<p class="index-2">handling  <a href="#_idIndexMarker351" aria-label="Footnote 97">97</a></p>
			<p>TextNode class</p>
			<p class="index-2">attributes  <a href="#_idIndexMarker111" aria-label="Footnote 38">38</a></p>
			<p>text nodes  <a href="#_idIndexMarker354" aria-label="Footnote 98">98</a></p>
			<p>text splitters  <a href="#_idIndexMarker228" aria-label="Footnote 70">70</a>, <a href="#_idIndexMarker230" aria-label="Footnote 71">71</a>, <a href="#_idIndexMarker270" aria-label="Footnote 77">77</a></p>
			<p class="index-2">CodeSplitter  <a href="#_idIndexMarker236" aria-label="Footnote 72">72</a></p>
			<p class="index-2">SentenceSplitter  <a href="#_idIndexMarker231" aria-label="Footnote 71">71</a></p>
			<p class="index-2">TokenTextSplitter  <a href="#_idIndexMarker233" aria-label="Footnote 71">71</a></p>
			<p>TF-IDF score  <a href="#_idIndexMarker624" aria-label="Footnote 177">177</a></p>
			<p>The Gartner Hype Cycle model  <a href="#_idIndexMarker019" aria-label="Footnote 6">6</a></p>
			<p>time-based postprocessors  <a href="#_idIndexMarker671" aria-label="Footnote 195">195</a></p>
			<p class="index-2">EmbeddingRecencyPostprocessor  <a href="#_idIndexMarker674" aria-label="Footnote 196">196</a></p>
			<p class="index-2">FixedRecencyPostprocessor  <a href="#_idIndexMarker672" aria-label="Footnote 196">196</a></p>
			<p class="index-2">TimeWeightedPostprocessor  <a href="#_idIndexMarker679" aria-label="Footnote 197">197</a></p>
			<p>time-decay function  <a href="#_idIndexMarker677" aria-label="Footnote 197">197</a></p>
			<p>TimeWeightedPostprocessor  <a href="#_idIndexMarker678" aria-label="Footnote 197">197</a></p>
			<p class="index-2">parameters  <a href="#_idIndexMarker680" aria-label="Footnote 197">197</a></p>
			<p>TitleExtractor  <a href="#_idIndexMarker296" aria-label="Footnote 84">84</a></p>
			<p class="index-2">parameters  <a href="#_idIndexMarker298" aria-label="Footnote 84">84</a></p>
			<p>TokenCountingHandler  <a href="#_idIndexMarker330" aria-label="Footnote 90">90</a></p>
			<p>tokenizer  <a href="#_idIndexMarker331" aria-label="Footnote 90">90</a></p>
			<p>TokenTextSplitter  <a href="#_idIndexMarker119" aria-label="Footnote 39">39</a>, <a href="#_idIndexMarker234" aria-label="Footnote 71">71</a></p>
			<p>ToolSpec class  <a href="#_idIndexMarker837" aria-label="Footnote 238">238</a>, <a href="#_idIndexMarker842" aria-label="Footnote 240">240</a></p>
			<p>top-k similarity search  <a href="#_idIndexMarker384" aria-label="Footnote 107">107</a></p>
			<p>training_UI.py module  <a href="#_idIndexMarker917" aria-label="Footnote 257">257</a></p>
			<p>transformation  <a href="#_idIndexMarker284" aria-label="Footnote 81">81</a>, <a href="#_idIndexMarker344" aria-label="Footnote 94">94</a></p>
			<p>transformer  <a href="#_idIndexMarker013" aria-label="Footnote 5">5</a></p>
			<p>TreeIndex  <a href="#_idIndexMarker148" aria-label="Footnote 44">44</a>, <a href="#_idIndexMarker469" aria-label="Footnote 124">124</a>, <a href="#_idIndexMarker472" aria-label="Footnote 125">125</a></p>
			<p class="index-2">customizable parameters  <a href="#_idIndexMarker473" aria-label="Footnote 125">125</a>, <a href="#_idIndexMarker474" aria-label="Footnote 126">126</a></p>
			<p class="index-2">drawbacks  <a href="#_idIndexMarker482" aria-label="Footnote 127">127</a>, <a href="#_idIndexMarker483" aria-label="Footnote 128">128</a></p>
			<p class="index-2">implementing, in organizations  <a href="#_idIndexMarker478" aria-label="Footnote 126">126</a></p>
			<p class="index-2">inner mechanics  <a href="#_idIndexMarker477" aria-label="Footnote 126">126</a></p>
			<p class="index-2">operating, ways  <a href="#_idIndexMarker481" aria-label="Footnote 127">127</a></p>
			<p class="index-2">supporting, retrieval modes  <a href="#_idIndexMarker479" aria-label="Footnote 127">127</a></p>
			<p class="index-2">usage model  <a href="#_idIndexMarker475" aria-label="Footnote 126">126</a></p>
			<p>TreeIndex retrievers  <a href="#_idIndexMarker550" aria-label="Footnote 152">152</a>, <a href="#_idIndexMarker551" aria-label="Footnote 153">153</a></p>
			<p class="index-2">KeywordTableGPTRetriever  <a href="#_idIndexMarker561" aria-label="Footnote 157">157</a></p>
			<p class="index-2">KeywordTableIndex retrievers  <a href="#_idIndexMarker560" aria-label="Footnote 156">156</a></p>
			<p class="index-2">KeywordTableRAKERetriever  <a href="#_idIndexMarker563" aria-label="Footnote 158">158</a></p>
			<p class="index-2">KeywordTableSimpleRetriever  <a href="#_idIndexMarker562" aria-label="Footnote 157">157</a></p>
			<p class="index-2">TreeAllLeafRetriever  <a href="#_idIndexMarker556" aria-label="Footnote 155">155</a></p>
			<p class="index-2">TreeRootRetriever  <a href="#_idIndexMarker557" aria-label="Footnote 155">155</a>, <a href="#_idIndexMarker559" aria-label="Footnote 156">156</a></p>
			<p class="index-2">TreeSelectLeafEmbeddingRetriever  <a href="#_idIndexMarker554" aria-label="Footnote 154">154</a></p>
			<p class="index-2">TreeSelectLeafRetriever  <a href="#_idIndexMarker552" aria-label="Footnote 153">153</a>, <a href="#_idIndexMarker553" aria-label="Footnote 154">154</a></p>
			<p>Tree of Thoughts (ToT) prompting  <a href="#_idIndexMarker1145" aria-label="Footnote 317">317</a></p>
			<p>triplets  <a href="#_idIndexMarker487" aria-label="Footnote 128">128</a>, <a href="#_idIndexMarker566" aria-label="Footnote 158">158</a></p>
			<p class="index-head">U</p>
			<p>UnstructuredElementNodeParser  <a href="#_idIndexMarker268" aria-label="Footnote 77">77</a></p>
			<p>utility tools, for agents  <a href="#_idIndexMarker877" aria-label="Footnote 248">248</a></p>
			<p class="index-2">LoadAndSearchToolSpec utility  <a href="#_idIndexMarker879" aria-label="Footnote 248">248</a>-<a href="#_idIndexMarker883" aria-label="Footnote 250">250</a></p>
			<p class="index-2">OnDemandLoaderTool  <a href="#_idIndexMarker885" aria-label="Footnote 250">250</a>, <a href="#_idIndexMarker889" aria-label="Footnote 251">251</a></p>
			<p class="index-head">V</p>
			<p>vector databases  <a href="#_idIndexMarker431" aria-label="Footnote 117">117</a></p>
			<p class="index-2">reference link  <a href="#_idIndexMarker439" aria-label="Footnote 118">118</a></p>
			<p>vector embeddings  <a href="#_idIndexMarker378" aria-label="Footnote 105">105</a>-<a href="#_idIndexMarker382" aria-label="Footnote 107">107</a></p>
			<p class="index-2">storing  <a href="#_idIndexMarker420" aria-label="Footnote 114">114</a></p>
			<p>VectorStoreIndex  <a href="#_idIndexMarker146" aria-label="Footnote 44">44</a>, <a href="#_idIndexMarker367" aria-label="Footnote 103">103</a></p>
			<p class="index-2">parameters  <a href="#_idIndexMarker373" aria-label="Footnote 104">104</a></p>
			<p class="index-2">usage example  <a href="#_idIndexMarker372" aria-label="Footnote 104">104</a>, <a href="#_idIndexMarker375" aria-label="Footnote 105">105</a></p>
			<p>VectorStoreIndex retrievers  <a href="#_idIndexMarker529" aria-label="Footnote 145">145</a></p>
			<p class="index-2">SummaryIndexEmbeddingRetriever  <a href="#_idIndexMarker539" aria-label="Footnote 148">148</a>, <a href="#_idIndexMarker540" aria-label="Footnote 149">149</a></p>
			<p class="index-2">SummaryIndexLLMRetriever  <a href="#_idIndexMarker541" aria-label="Footnote 149">149</a>, <a href="#_idIndexMarker542" aria-label="Footnote 150">150</a></p>
			<p class="index-2">SummaryIndexRetriever  <a href="#_idIndexMarker537" aria-label="Footnote 148">148</a></p>
			<p class="index-2">VectorIndexAutoRetriever  <a href="#_idIndexMarker535" aria-label="Footnote 147">147</a></p>
			<p class="index-2">VectorIndexRetriever  <a href="#_idIndexMarker530" aria-label="Footnote 145">145</a>-<a href="#_idIndexMarker534" aria-label="Footnote 147">147</a></p>
			<p>vector stores</p>
			<p class="index-2">versus vector databases  <a href="#_idIndexMarker434" aria-label="Footnote 117">117</a>, <a href="#_idIndexMarker438" aria-label="Footnote 118">118</a></p>
			<p>Vector Stores  <a href="#_idIndexMarker427" aria-label="Footnote 115">115</a></p>
			<p>VSCode</p>
			<p class="index-2">reference link  <a href="#_idIndexMarker057" aria-label="Footnote 22">22</a></p>
			<p class="index-head">W</p>
			<p>WikpediaReader  <a href="#_idIndexMarker104" aria-label="Footnote 37">37</a></p>
			<p class="index-head">Z</p>
			<p>Zephyr-7B</p>
			<p class="index-2">reference link  <a href="#_idIndexMarker945" aria-label="Footnote 268">268</a></p>
			<p>Zephyr Query Engine Pack  <a href="#_idIndexMarker991" aria-label="Footnote 277">277</a></p>
			<p class="index-2">reference link  <a href="#_idIndexMarker992" aria-label="Footnote 277">277</a></p>
			<p>zero-shot prompting  <a href="#_idIndexMarker1137" aria-label="Footnote 316">316</a></p>
		</div>
<div id="f_18__idContainer123" data-type="appendix" class="appendix" file="B21861_BM_xhtml" title2="Why subscribe?" no2="">
			<p><img src="image/Packt_Logo_New1.png" alt="Packt Logo" width="480" height="123"></p>
			<p><a href="http://packtpub.com" target="_blank" rel="noopener noreferrer"><span class="No-Break">packtpub.com</span></a></p>
			<p>Subscribe to our online digital library for full access to over 7,000 books and videos, as well as industry leading tools to help you plan your personal development and advance your career. For more information, please visit <span class="No-Break">our website.</span></p>
			<h1 id="f_18__idParaDest-244"><a id="_idTextAnchor243"></a>Why subscribe?</h1>
			<ul>
				<li>Spend less time learning and more time coding with practical eBooks and Videos from over 4,000 industry professionals</li>
				<li>Improve your learning with Skill Plans built especially for you</li>
				<li>Get a free eBook or video every month</li>
				<li>Fully searchable for easy access to vital information</li>
				<li>Copy and paste, print, and bookmark content</li>
			</ul>
			<p>Did you know that Packt offers eBook versions of every book published, with PDF and ePub files available? You can upgrade to the eBook version at <a href="http://packtpub.com" target="_blank" rel="noopener noreferrer">packtpub.com</a> and as a print book customer, you are entitled to a discount on the eBook copy. Get in touch with us at <a href="mailto:customercare@packtpub.com">customercare@packtpub.com</a> for <span class="No-Break">more details.</span></p>
			<p>At <a href="http://www.packtpub.com" target="_blank" rel="noopener noreferrer">www.packtpub.com</a>, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts and offers on Packt books <span class="No-Break">and eBooks.</span></p>
			<h2 id="f_18__idParaDest-245" data-type="sect1" class="sect1" title2="Other Books You May Enjoy" no2=""><a id="_idTextAnchor244"></a>Other Books You May Enjoy</h2>
			<p>If you enjoyed this book, you may be interested in these other books <span class="No-Break">by Packt:</span></p>
			<div>
				<a href="https://packt.link/1835083838" target="_blank" rel="noopener noreferrer">
					<div id="_idContainer119">
						<img src="image/9781835083833.jpg" alt="Other Books You May Enjoy - Unlocking the Secrets of Prompt Engineering

" width="657" height="810">
					</div>
				</a>
			</div>
			<p><strong class="bold">Unlocking the Secrets of </strong><span class="No-Break"><strong class="bold">Prompt Engineering</strong></span></p>
			<p><span class="No-Break">Gilbert Mizrahi</span></p>
			<p><span class="No-Break">ISBN: 978-1-83508-383-3</span></p>
			<ul>
				<li>Explore the different types of prompts, their strengths, and weaknesses</li>
				<li>Understand the AI agent’s knowledge and mental model</li>
				<li>Enhance your creative writing with AI insights for fiction and poetry</li>
				<li>Develop advanced skills in AI chatbot creation and deployment</li>
				<li>Discover how AI will transform industries such as education, legal, and others</li>
				<li>Integrate LLMs with various tools to boost productivity</li>
				<li>Understand AI ethics and best practices, and navigate limitations effectively</li>
				<li>Experiment and optimize AI techniques for best results</li>
			</ul>
			<div>
				<a href="https://packt.link/183508771X" target="_blank" rel="noopener noreferrer">
					<div id="_idContainer120">
						<img src="image/9781835087718.jpg" alt="Other Books You May Enjoy - Generating Creative Images With DALL-E 3
" width="657" height="810">
					</div>
				</a>
			</div>
			<p><strong class="bold">Generating Creative Images With </strong><span class="No-Break"><strong class="bold">DALL-E 3</strong></span></p>
			<p><span class="No-Break">Holly Picano</span></p>
			<p><span class="No-Break">ISBN: 978-1-83508-771-8</span></p>
			<ul>
				<li>Master DALL-E 3’s architecture and training methods</li>
				<li>Create fine prints and other AI-generated art with precision</li>
				<li>Seamlessly blend AI with traditional artistry</li>
				<li>Address ethical dilemmas in AI art</li>
				<li>Explore the future of digital creativity</li>
				<li>Implement practical optimization techniques for your artistic endeavors</li>
			</ul>
			<h2 id="f_18__idParaDest-246" data-type="sect1" class="sect1" title2="Packt is searching for authors like you" no2=""><a id="_idTextAnchor245"></a>Packt is searching for authors like you</h2>
			<p>If you’re interested in becoming an author for Packt, please visit <a href="http://authors.packtpub.com" target="_blank" rel="noopener noreferrer">authors.packtpub.com</a> and apply today. We have worked with thousands of developers and tech professionals, just like you, to help them share their insight with the global tech community. You can make a general application, apply for a specific hot topic that we are recruiting an author for, or submit your <span class="No-Break">own idea.</span></p>
			<h2 id="f_18__idParaDest-247" data-type="sect1" class="sect1" title2="Share Your Thoughts" no2=""><a id="_idTextAnchor246"></a>Share Your Thoughts</h2>
			<p>Now you’ve finished <em class="italic">Building Data-Driven Applications with LlamaIndex</em>, we’d love to hear your thoughts! If you purchased the book from Amazon, please <a href="https://packt.link/r/1-835-08950-X" target="_blank" rel="noopener noreferrer">click here to go straight to the Amazon review page</a> for this book and share your feedback or leave a review on the site that you purchased <span class="No-Break">it from.</span></p>
			<p>Your review is important to us and the tech community and will help us make sure we’re delivering excellent <span class="No-Break">quality content.</span></p>
			<h2 id="f_18__idParaDest-248" data-type="sect1" class="sect1" title2="Download a free PDF copy of this book" no2=""><a id="_idTextAnchor247"></a>Download a free PDF copy of this book</h2>
			<p>Thanks for purchasing <span class="No-Break">this book!</span></p>
			<p>Do you like to read on the go but are unable to carry your print <span class="No-Break">books everywhere?</span></p>
			<p>Is your eBook purchase not compatible with the device of <span class="No-Break">your choice?</span></p>
			<p>Don’t worry, now with every Packt book you get a DRM-free PDF version of that book at <span class="No-Break">no cost.</span></p>
			<p>Read anywhere, any place, on any device. Search, copy, and paste code from your favorite technical books directly into <span class="No-Break">your application.</span></p>
			<p>The perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free content in your <span class="No-Break">inbox daily</span></p>
			<p>Follow these simple steps to get <span class="No-Break">the benefits:</span></p>
			<ol>
				<li>Scan the QR code or visit the <span class="No-Break">link below</span></li>
			</ol>
			<div>
				<a href="https://packt.link/free-ebook/9781835089507" target="_blank" rel="noopener noreferrer">
					<div id="_idContainer122" class="IMG---Figure">
						<img src="image/B21861_QR_Free_PDF.jpg" alt="Download a free PDF copy of this book
" width="200" height="200" data-type="figure" id="untitled_figure_83" title2="https://packt.link/free-ebook/9781835089507" no2="">
					</div>
				</a>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a href="https://packt.link/free-ebook/9781835089507" target="_blank" rel="noopener noreferrer">https://packt.link/free-ebook/9781835089507</a></p>
			<ol>
				<li value="2">Submit your proof <span class="No-Break">of purchase</span></li>
				<li>That’s it! We’ll send your free PDF and other benefits to your <span class="No-Break">email directly</span></li>
			</ol>
		</div>
  
<nav id="fnPopDiv" style="display: none;"><div class="up-arrow"></div><div class="down-arrow"></div><div class="content"></div></nav><script>
        document.addEventListener("mouseover", getFn);

        function getFn(e) {
          const el = e.target;
          let fnPopDiv = document.getElementById('fnPopDiv');
          if(![...document.querySelectorAll('a[data-type="noteref"]')].includes(el) || 
            el.parentNode.parentNode.id.startsWith('menu') ||
            el.parentNode.parentNode.parentNode.id.startsWith('menu')) {
            if(fnPopDiv.style.display == 'block') fnPopDiv.style.display = 'none';

            return;
          }

          const fnPopDivUpArrow = fnPopDiv.querySelector('div.up-arrow'),
            fnPopDivDownArrow = fnPopDiv.querySelector('div.down-arrow'),
            fnPopDivBody = fnPopDiv.querySelector('div.content');
          fnPopDiv.style.display = 'block';

          if(el.hash) {
            let contentEl = document.getElementById(el.hash.slice(1));
            let content = contentEl.innerHTML;
            if(content == '' || contentEl.innerText == el.innerText) content = contentEl.parentNode.innerHTML;
            fnPopDivBody.innerHTML = content;
          }
          else  //not a good error-check
            fnPopDivBody.innerHTML = document.getElementById(el.parentNode.hash.slice(1)).parentNode.innerHTML;

          const rect = el.getBoundingClientRect();
          if(rect.top - (fnPopDiv.offsetHeight) - 6 < 0) {
            fnPopDiv.style.top = (rect.top + el.offsetHeight + 6 + 6) + 'px';
            fnPopDivUpArrow.style.top = (rect.top + el.offsetHeight + 6) + 'px';
            fnPopDivUpArrow.style.left = rect.left + 'px';
            fnPopDivUpArrow.style.display = 'block';
            fnPopDivDownArrow.style.display = 'none';
          }
          else {
            fnPopDiv.style.top = (rect.top - fnPopDiv.offsetHeight - 6) + 'px';
            fnPopDivDownArrow.style.top = (rect.top - 6) + 'px';
            fnPopDivDownArrow.style.left = rect.left + 'px';
            fnPopDivDownArrow.style.display = 'block';
            fnPopDivUpArrow.style.display = 'none';
          }

          fnPopDiv.style.left = rect.left + el.offsetWidth/2 - (fnPopDiv.offsetWidth/2);
          if(fnPopDiv.style.left < '1') fnPopDiv.style.left = '1';
        }
        </script><nav id="menuDiv" style="display: none;"></nav><nav id="menuText_hide" style="display: none;"></nav><nav id="menuText_TOC" style="display: none;"><hr><p><strong class="matter"><a href="#f_0__idContainer001">Building Data-Driven Applications with LlamaIndex</a></strong></p><p><a href="#f_0__idParaDest-2">_Contributors</a></p><p><a href="#f_0__idParaDest-4">_About the reviewers</a></p><hr><p><strong class="matter"><a href="#f_1__idContainer005">Preface</a></strong></p><p><a href="#f_1__idParaDest-6">_Who this book is for</a></p><p><a href="#f_1__idParaDest-7">_What this book covers</a></p><p><a href="#f_1__idParaDest-8">_To get the most out of this book</a></p><p><a href="#f_1__idParaDest-9">_Download the example code files</a></p><p><a href="#f_1__idParaDest-10">_Conventions used</a></p><p><a href="#f_1__idParaDest-11">_Get in touch</a></p><p><a href="#f_1__idParaDest-12">_Share Your Thoughts</a></p><p><a href="#f_1__idParaDest-13">_Download a free PDF copy of this book</a></p><hr><p><strong><a href="#f_2__idContainer006">Part 1. Introduction to  Generative AI and LlamaIndex</a></strong></p><hr><p><strong><a href="#f_3__idContainer016">1. Understanding Large Language Models</a></strong></p><p><a href="#f_3__idParaDest-17">_1.1. Introducing GenAI and LLMs</a></p><p><a href="#f_3__idParaDest-18">__1.1.1. What is GenAI?</a></p><p><a href="#f_3__idParaDest-19">__1.1.2. What is an LLM?</a></p><p><a href="#f_3__idParaDest-20">_1.2. Understanding the role of LLMs in modern technology</a></p><p><a href="#f_3__idParaDest-21">_1.3. Exploring challenges with LLMs</a></p><p><a href="#f_3__idParaDest-22">_1.4. Augmenting LLMs with RAG</a></p><p><a href="#f_3__idParaDest-23">_1.5. Summary</a></p><hr><p><strong><a href="#f_4__idContainer023">2. LlamaIndex: The Hidden Jewel - An Introduction to the LlamaIndex Ecosystem</a></strong></p><p><a href="#f_4__idParaDest-26">_2.1. Technical requirements</a></p><p><a href="#f_4__idParaDest-27">_2.2. Optimizing language models – the symbiosis of fine-tuning, RAG, and LlamaIndex</a></p><p><a href="#f_4__idParaDest-28">__2.2.1. Is RAG the only possible solution?</a></p><p><a href="#f_4__idParaDest-29">__2.2.2. What LlamaIndex does</a></p><p><a href="#f_4__idParaDest-30">_2.3. Discovering the advantages of progressively disclosing complexity</a></p><p><a href="#f_4__idParaDest-31">__2.3.1. An important aspect to consider</a></p><p><a href="#f_4__idParaDest-32">_2.4. Introducing PITS – our LlamaIndex hands-on project</a></p><p><a href="#f_4__idParaDest-33">__2.4.1. Here’s how it will work</a></p><p><a href="#f_4__idParaDest-34">_2.5. Preparing our coding environment</a></p><p><a href="#f_4__idParaDest-35">__2.5.1. Installing Python</a></p><p><a href="#f_4__idParaDest-36">__2.5.2. Installing Git</a></p><p><a href="#f_4__idParaDest-37">__2.5.3. Installing LlamaIndex</a></p><p><a href="#f_4__idParaDest-38">__2.5.4. Signing up for an OpenAI API key</a></p><p><a href="#f_4__idParaDest-39">__2.5.5. Discovering Streamlit – the perfect tool for rapid building and deployment!</a></p><p><a href="#f_4__idParaDest-40">__2.5.6. Installing Streamlit</a></p><p><a href="#f_4__idParaDest-41">__2.5.7. Finishing up</a></p><p><a href="#f_4__idParaDest-42">__2.5.8. One final check</a></p><p><a href="#f_4__idParaDest-43">_2.6. Familiarizing ourselves with the structure of the LlamaIndex code repository</a></p><p><a href="#f_4__idParaDest-44">_2.7. Summary</a></p><hr><p><strong><a href="#f_5__idContainer024">Part 2. Starting Your First LlamaIndex Project</a></strong></p><hr><p><strong><a href="#f_6__idContainer034">3. Kickstarting Your Journey with LlamaIndex</a></strong></p><p><a href="#f_6__idParaDest-48">_3.1. Technical requirements</a></p><p><a href="#f_6__idParaDest-49">_3.2. Uncovering the essential building blocks of LlamaIndex – documents, nodes, and indexes</a></p><p><a href="#f_6__idParaDest-50">__3.2.1. Documents</a></p><p><a href="#f_6__idParaDest-51">__3.2.2. Nodes</a></p><p><a href="#f_6__idParaDest-52">__3.2.3. Manually creating the Node objects</a></p><p><a href="#f_6__idParaDest-53">__3.2.4. Automatically extracting Nodes from Documents using splitters</a></p><p><a href="#f_6__idParaDest-54">__3.2.5. Nodes don’t like to be alone – they crave relationships</a></p><p><a href="#f_6__idParaDest-55">__3.2.6. Why are relationships important?</a></p><p><a href="#f_6__idParaDest-56">__3.2.7. Indexes</a></p><p><a href="#f_6__idParaDest-57">__3.2.8. Are we there yet?</a></p><p><a href="#f_6__idParaDest-58">__3.2.9. How does this actually work under the hood?</a></p><p><a href="#f_6__idParaDest-59">__3.2.10. A quick recap of the key concepts</a></p><p><a href="#f_6__idParaDest-60">_3.3. Building our first interactive, augmented LLM application</a></p><p><a href="#f_6__idParaDest-61">__3.3.1. Using the logging features of LlamaIndex to understand the logic and debug our applications</a></p><p><a href="#f_6__idParaDest-62">__3.3.2. Customizing the LLM used by LlamaIndex</a></p><p><a href="#f_6__idParaDest-63">__3.3.3. Easy as 1-2-3</a></p><p><a href="#f_6__idParaDest-64">__3.3.4. The temperature parameter</a></p><p><a href="#f_6__idParaDest-65">__3.3.5. Understanding how Settings can be used for customization</a></p><p><a href="#f_6__idParaDest-66">_3.4. Starting our PITS project – hands-on exercise</a></p><p><a href="#f_6__idParaDest-67">__3.4.1. Let’s have a look at the source code</a></p><p><a href="#f_6__idParaDest-68">_3.5. Summary</a></p><hr><p><strong><a href="#f_7__idContainer039">4. Ingesting Data into Our RAG Workflow</a></strong></p><p><a href="#f_7__idParaDest-71">_4.1. Technical requirements</a></p><p><a href="#f_7__idParaDest-72">_4.2. Ingesting data via LlamaHub</a></p><p><a href="#f_7__idParaDest-73">_4.3. An overview of LlamaHub</a></p><p><a href="#f_7__idParaDest-74">_4.4. Using the LlamaHub data loaders to ingest content</a></p><p><a href="#f_7__idParaDest-75">__4.4.1. Ingesting data from a web page</a></p><p><a href="#f_7__idParaDest-76">__4.4.2. Ingesting data from a database</a></p><p><a href="#f_7__idParaDest-77">__4.4.3. Bulk-ingesting data from sources with multiple file formats</a></p><p><a href="#f_7__idParaDest-78">_4.5. Parsing the documents into nodes</a></p><p><a href="#f_7__idParaDest-79">__4.5.1. Understanding the simple text splitters</a></p><p><a href="#f_7__idParaDest-80">__4.5.2. Using more advanced node parsers</a></p><p><a href="#f_7__idParaDest-81">__4.5.3. Using relational parsers</a></p><p><a href="#f_7__idParaDest-82">__4.5.4. Confused about node parsers and text splitters?</a></p><p><a href="#f_7__idParaDest-83">__4.5.5. Understanding chunk_size and chunk_overlap</a></p><p><a href="#f_7__idParaDest-84">__4.5.6. Including relationships with include_prev_next_rel</a></p><p><a href="#f_7__idParaDest-85">__4.5.7. Practical ways of using these node creation models</a></p><p><a href="#f_7__idParaDest-86">_4.6. Working with metadata to improve the context</a></p><p><a href="#f_7__idParaDest-87">__4.6.1. SummaryExtractor</a></p><p><a href="#f_7__idParaDest-88">__4.6.2. QuestionsAnsweredExtractor</a></p><p><a href="#f_7__idParaDest-89">__4.6.3. TitleExtractor</a></p><p><a href="#f_7__idParaDest-90">__4.6.4. EntityExtractor</a></p><p><a href="#f_7__idParaDest-91">__4.6.5. KeywordExtractor</a></p><p><a href="#f_7__idParaDest-92">__4.6.6. PydanticProgramExtractor</a></p><p><a href="#f_7__idParaDest-93">__4.6.7. MarvinMetadataExtractor</a></p><p><a href="#f_7__idParaDest-94">__4.6.8. Defining your custom extractor</a></p><p><a href="#f_7__idParaDest-95">__4.6.9. Is having all that metadata always a good thing?</a></p><p><a href="#f_7__idParaDest-96">_4.7. Estimating the potential cost of using metadata extractors</a></p><p><a href="#f_7__idParaDest-97">__4.7.1. Follow these simple best practices to minimize your costs</a></p><p><a href="#f_7__idParaDest-98">__4.7.2. Estimate your maximal costs before running the actual extractors</a></p><p><a href="#f_7__idParaDest-99">_4.8. Preserving privacy with metadata extractors, and not only</a></p><p><a href="#f_7__idParaDest-100">__4.8.1. Scrubbing personal data and other sensitive information</a></p><p><a href="#f_7__idParaDest-101">_4.9. Using the ingestion pipeline to increase efficiency</a></p><p><a href="#f_7__idParaDest-102">_4.10. Handling documents that contain a mix of text and tabular data</a></p><p><a href="#f_7__idParaDest-103">_4.11. Hands-on – ingesting study materials into our PITS</a></p><p><a href="#f_7__idParaDest-104">_4.12. Summary</a></p><hr><p><strong><a href="#f_8__idContainer052">5. Indexing with LlamaIndex</a></strong></p><p><a href="#f_8__idParaDest-107">_5.1. Technical requirements</a></p><p><a href="#f_8__idParaDest-108">_5.2. Indexing data – a bird’s-eye view</a></p><p><a href="#f_8__idParaDest-109">__5.2.1. Common features of all Index types</a></p><p><a href="#f_8__idParaDest-110">_5.3. Understanding the VectorStoreIndex</a></p><p><a href="#f_8__idParaDest-111">__5.3.1. A simple usage example for the VectorStoreIndex</a></p><p><a href="#f_8__idParaDest-112">__5.3.2. Understanding embeddings</a></p><p><a href="#f_8__idParaDest-113">__5.3.3. Understanding similarity search</a></p><p><a href="#f_8__idParaDest-114">__5.3.4. OK, but how does LlamaIndex generate these embeddings?</a></p><p><a href="#f_8__idParaDest-115">__5.3.5. How do I decide which embedding model I should use?</a></p><p><a href="#f_8__idParaDest-116">_5.4. Persisting and reusing Indexes</a></p><p><a href="#f_8__idParaDest-117">__5.4.1. Understanding the StorageContext</a></p><p><a href="#f_8__idParaDest-118">__5.4.2. The difference between vector stores and vector databases</a></p><p><a href="#f_8__idParaDest-119">_5.5. Exploring other index types in LlamaIndex</a></p><p><a href="#f_8__idParaDest-120">__5.5.1. The SummaryIndex</a></p><p><a href="#f_8__idParaDest-121">__5.5.2. The DocumentSummaryIndex</a></p><p><a href="#f_8__idParaDest-122">__5.5.3. The KeywordTableIndex</a></p><p><a href="#f_8__idParaDest-123">__5.5.4. The TreeIndex</a></p><p><a href="#f_8__idParaDest-124">__5.5.5. The KnowledgeGraphIndex</a></p><p><a href="#f_8__idParaDest-125">_5.6. Building Indexes on top of other Indexes with ComposableGraph</a></p><p><a href="#f_8__idParaDest-126">__5.6.1. How to use the ComposableGraph</a></p><p><a href="#f_8__idParaDest-127">__5.6.2. A more detailed description of this concept</a></p><p><a href="#f_8__idParaDest-128">_5.7. Estimating the potential cost of building and querying Indexes</a></p><p><a href="#f_8__idParaDest-129">_5.8. Indexing our PITS study materials – hands-on</a></p><p><a href="#f_8__idParaDest-130">_5.9. Summary</a></p><hr><p><strong><a href="#f_9__idContainer053">Part 3. Retrieving and Working with Indexed Data</a></strong></p><hr><p><strong><a href="#f_10__idContainer071">6. Querying Our Data, Part 1 – Context Retrieval</a></strong></p><p><a href="#f_10__idParaDest-134">_6.1. Technical requirements</a></p><p><a href="#f_10__idParaDest-135">_6.2. Learning about query mechanics – an overview</a></p><p><a href="#f_10__idParaDest-136">_6.3. Understanding the basic retrievers</a></p><p><a href="#f_10__idParaDest-137">__6.3.1. The VectorStoreIndex retrievers</a></p><p><a href="#f_10__idParaDest-138">__6.3.2. The DocumentSummaryIndex retrievers</a></p><p><a href="#f_10__idParaDest-139">__6.3.3. The TreeIndex retrievers</a></p><p><a href="#f_10__idParaDest-140">__6.3.4. The KnowledgeGraphIndex retrievers</a></p><p><a href="#f_10__idParaDest-141">__6.3.5. Common characteristics shared by all retrievers</a></p><p><a href="#f_10__idParaDest-142">__6.3.6. Efficient use of retrieval mechanisms – asynchronous operation</a></p><p><a href="#f_10__idParaDest-143">_6.4. Building more advanced retrieval mechanisms</a></p><p><a href="#f_10__idParaDest-144">__6.4.1. The naive retrieval method</a></p><p><a href="#f_10__idParaDest-145">__6.4.2. Implementing metadata filters</a></p><p><a href="#f_10__idParaDest-146">__6.4.3. Using selectors for more advanced decision logic</a></p><p><a href="#f_10__idParaDest-147">__6.4.4. Understanding tools</a></p><p><a href="#f_10__idParaDest-148">__6.4.5. Transforming and rewriting queries</a></p><p><a href="#f_10__idParaDest-149">__6.4.6. Creating more specific sub-queries</a></p><p><a href="#f_10__idParaDest-150">_6.5. Understanding the concepts of dense and sparse retrieval</a></p><p><a href="#f_10__idParaDest-151">__6.5.1. Dense retrieval</a></p><p><a href="#f_10__idParaDest-152">__6.5.2. Sparse retrieval</a></p><p><a href="#f_10__idParaDest-153">__6.5.3. Implementing sparse retrieval in LlamaIndex</a></p><p><a href="#f_10__idParaDest-154">__6.5.4. Discovering other advanced retrieval methods</a></p><p><a href="#f_10__idParaDest-155">_6.6. Summary</a></p><hr><p><strong><a href="#f_11__idContainer077">7. Querying Our Data, Part 2 – Postprocessing and Response Synthesis</a></strong></p><p><a href="#f_11__idParaDest-158">_7.1. Technical requirements</a></p><p><a href="#f_11__idParaDest-159">_7.2. Re-ranking, transforming, and filtering nodes using postprocessors</a></p><p><a href="#f_11__idParaDest-160">__7.2.1. Exploring how postprocessors filter, transform, and re-rank nodes</a></p><p><a href="#f_11__idParaDest-161">__7.2.2. SimilarityPostprocessor</a></p><p><a href="#f_11__idParaDest-162">__7.2.3. KeywordNodePostprocessor</a></p><p><a href="#f_11__idParaDest-163">__7.2.4. PrevNextNodePostprocessor</a></p><p><a href="#f_11__idParaDest-164">__7.2.5. LongContextReorder</a></p><p><a href="#f_11__idParaDest-165">__7.2.6. PIINodePostprocessor and NERPIINodePostprocessor</a></p><p><a href="#f_11__idParaDest-166">__7.2.7. MetadataReplacementPostprocessor</a></p><p><a href="#f_11__idParaDest-167">__7.2.8. SentenceEmbeddingOptimizer</a></p><p><a href="#f_11__idParaDest-168">__7.2.9. Time-based postprocessors</a></p><p><a href="#f_11__idParaDest-169">__7.2.10. Re-ranking postprocessors</a></p><p><a href="#f_11__idParaDest-170">__7.2.11. Final thoughts about node postprocessors</a></p><p><a href="#f_11__idParaDest-171">_7.3. Understanding response synthesizers</a></p><p><a href="#f_11__idParaDest-172">_7.4. Implementing output parsing techniques</a></p><p><a href="#f_11__idParaDest-173">__7.4.1. Extracting structured outputs using output parsers</a></p><p><a href="#f_11__idParaDest-174">__7.4.2. Extracting structured outputs using Pydantic programs</a></p><p><a href="#f_11__idParaDest-175">_7.5. Building and using query engines</a></p><p><a href="#f_11__idParaDest-176">__7.5.1. Exploring different methods of building query engines</a></p><p><a href="#f_11__idParaDest-177">__7.5.2. Advanced uses of the QueryEngine interface</a></p><p><a href="#f_11__idParaDest-178">_7.6. Hands-on – building quizzes in PITS</a></p><p><a href="#f_11__idParaDest-179">_7.7. Summary</a></p><hr><p><strong><a href="#f_12__idContainer094">8. Building Chatbots and Agents with LlamaIndex</a></strong></p><p><a href="#f_12__idParaDest-182">_8.1. Technical requirements</a></p><p><a href="#f_12__idParaDest-183">_8.2. Understanding chatbots and agents</a></p><p><a href="#f_12__idParaDest-184">__8.2.1. Discovering ChatEngine</a></p><p><a href="#f_12__idParaDest-185">__8.2.2. Understanding the different chat modes</a></p><p><a href="#f_12__idParaDest-186">_8.3. Implementing agentic strategies in our apps</a></p><p><a href="#f_12__idParaDest-187">__8.3.1. Building tools and ToolSpec classes for our agents</a></p><p><a href="#f_12__idParaDest-188">__8.3.2. Understanding reasoning loops</a></p><p><a href="#f_12__idParaDest-189">__8.3.3. OpenAIAgent</a></p><p><a href="#f_12__idParaDest-190">__8.3.4. ReActAgent</a></p><p><a href="#f_12__idParaDest-191">__8.3.5. How do we interact with agents?</a></p><p><a href="#f_12__idParaDest-192">__8.3.6. Enhancing our agents with the help of utility tools</a></p><p><a href="#f_12__idParaDest-193">__8.3.7. Using the LLMCompiler agent for more advanced scenarios</a></p><p><a href="#f_12__idParaDest-194">__8.3.8. Using the low-level Agent Protocol API</a></p><p><a href="#f_12__idParaDest-195">_8.4. Hands-on – implementing conversation tracking for PITS</a></p><p><a href="#f_12__idParaDest-196">_8.5. Summary</a></p><hr><p><strong><a href="#f_13__idContainer095">Part 4. Customization, Prompt Engineering, and Final Words</a></strong></p><hr><p><strong><a href="#f_14__idContainer110">9. Customizing and Deploying Our LlamaIndex Project</a></strong></p><p><a href="#f_14__idParaDest-200">_9.1. Technical requirements</a></p><p><a href="#f_14__idParaDest-201">_9.2. Customizing our RAG components</a></p><p><a href="#f_14__idParaDest-202">__9.2.1. How LLaMA and LLaMA 2 changed the open source landscape</a></p><p><a href="#f_14__idParaDest-203">__9.2.2. Running a local LLM using LM Studio</a></p><p><a href="#f_14__idParaDest-204">__9.2.3. Routing between LLMs using services such as Neutrino or OpenRouter</a></p><p><a href="#f_14__idParaDest-205">__9.2.4. What about customizing embedding models?</a></p><p><a href="#f_14__idParaDest-206">__9.2.5. Leveraging the Plug and Play convenience of using Llama Packs</a></p><p><a href="#f_14__idParaDest-207">__9.2.6. Using the Llama CLI</a></p><p><a href="#f_14__idParaDest-208">_9.3. Using advanced tracing and evaluation techniques</a></p><p><a href="#f_14__idParaDest-209">__9.3.1. Tracing our RAG workflows using Phoenix</a></p><p><a href="#f_14__idParaDest-210">__9.3.2. Evaluating our RAG system</a></p><p><a href="#f_14__idParaDest-211">_9.4. Introduction to deployment with Streamlit</a></p><p><a href="#f_14__idParaDest-212">_9.5. HANDS-ON – a step-by-step deployment guide</a></p><p><a href="#f_14__idParaDest-213">__9.5.1. Deploying our PITS project on Streamlit Community Cloud</a></p><p><a href="#f_14__idParaDest-214">_9.6. Summary</a></p><hr><p><strong><a href="#f_15__idContainer115">10. Prompt Engineering Guidelines and Best Practices</a></strong></p><p><a href="#f_15__idParaDest-217">_10.1. Technical requirements</a></p><p><a href="#f_15__idParaDest-218">_10.2. Why prompts are your secret weapon</a></p><p><a href="#f_15__idParaDest-219">_10.3. Understanding how LlamaIndex uses prompts</a></p><p><a href="#f_15__idParaDest-220">_10.4. Customizing default prompts</a></p><p><a href="#f_15__idParaDest-221">__10.4.1. Using advanced prompting techniques in LlamaIndex</a></p><p><a href="#f_15__idParaDest-222">_10.5. The golden rules of prompt engineering</a></p><p><a href="#f_15__idParaDest-223">__10.5.1. Accuracy and clarity in expression</a></p><p><a href="#f_15__idParaDest-224">__10.5.2. Directiveness</a></p><p><a href="#f_15__idParaDest-225">__10.5.3. Context quality</a></p><p><a href="#f_15__idParaDest-226">__10.5.4. Context quantity</a></p><p><a href="#f_15__idParaDest-227">__10.5.5. Required output format</a></p><p><a href="#f_15__idParaDest-228">__10.5.6. Inference cost</a></p><p><a href="#f_15__idParaDest-229">__10.5.7. Overall system latency</a></p><p><a href="#f_15__idParaDest-230">__10.5.8. Choosing the right LLM for the task</a></p><p><a href="#f_15__idParaDest-231">__10.5.9. Common methods used for creating effective prompts</a></p><p><a href="#f_15__idParaDest-232">_10.6. Summary</a></p><hr><p><strong><a href="#f_16__idContainer116">11. Conclusion and Additional Resources</a></strong></p><p><a href="#f_16__idParaDest-235">_11.1. Other projects and further learning</a></p><p><a href="#f_16__idParaDest-236">__11.1.1. The LlamaIndex examples collection</a></p><p><a href="#f_16__idParaDest-237">__11.1.2. Moving forward – Replit bounties</a></p><p><a href="#f_16__idParaDest-238">__11.1.3. The power of many – the LlamaIndex community</a></p><p><a href="#f_16__idParaDest-239">_11.2. Key takeaways, final words, and encouragement</a></p><p><a href="#f_16__idParaDest-240">__11.2.1. On the future of RAG in the larger context of generative AI</a></p><p><a href="#f_16__idParaDest-241">__11.2.2. A small philosophical nugget for you to consider</a></p><p><a href="#f_16__idParaDest-242">_11.3. Summary</a></p><hr><p><strong class="matter"><a href="#f_17__idContainer117">Index</a></strong></p><hr><p><strong class="matter"><a href="#f_18__idContainer123">Why subscribe?</a></strong></p><p><a href="#f_18__idParaDest-245">_Other Books You May Enjoy</a></p><p><a href="#f_18__idParaDest-246">_Packt is searching for authors like you</a></p><p><a href="#f_18__idParaDest-247">_Share Your Thoughts</a></p><p><a href="#f_18__idParaDest-248">_Download a free PDF copy of this book</a></p></nav><nav id="menuText_EQUATION" style="display: none;"></nav><nav id="menuText_inline_EQ" style="display: none;"></nav><nav id="menuText_EXAMPLE" style="display: none;"><p><a href="#untitled_example_1">(untitled_example_1)</a></p><p><a href="#untitled_example_2">(untitled_example_2)</a></p><p><a href="#untitled_example_3">(untitled_example_3)</a></p><p><a href="#untitled_example_4">(untitled_example_4)</a></p><p><a href="#untitled_terminal_1">(untitled_terminal_1)</a></p><p><a href="#untitled_terminal_2">(untitled_terminal_2)</a></p><p><a href="#untitled_terminal_3">(untitled_terminal_3)</a></p><p><a href="#untitled_example_5">(untitled_example_5)</a></p><p><a href="#untitled_terminal_4">(untitled_terminal_4)</a></p><p><a href="#untitled_terminal_5">(untitled_terminal_5)</a></p><p><a href="#untitled_terminal_6">(untitled_terminal_6)</a></p><p><a href="#untitled_terminal_7">(untitled_terminal_7)</a></p><p><a href="#untitled_example_6">(untitled_example_6)</a></p><p><a href="#untitled_example_7">(untitled_example_7)</a></p><p><a href="#untitled_example_8">(untitled_example_8)</a></p><p><a href="#untitled_example_9">(untitled_example_9)</a></p><p><a href="#untitled_example_10">(untitled_example_10)</a></p><p><a href="#untitled_example_11">(untitled_example_11)</a></p><p><a href="#untitled_example_12">(untitled_example_12)</a></p><p><a href="#untitled_example_13">(untitled_example_13)</a></p><p><a href="#untitled_example_14">(untitled_example_14)</a></p><p><a href="#untitled_example_15">(untitled_example_15)</a></p><p><a href="#untitled_example_16">(untitled_example_16)</a></p><p><a href="#untitled_example_17">(untitled_example_17)</a></p><p><a href="#untitled_example_18">(untitled_example_18)</a></p><p><a href="#untitled_example_19">(untitled_example_19)</a></p><p><a href="#untitled_example_20">(untitled_example_20)</a></p><p><a href="#untitled_example_21">(untitled_example_21)</a></p><p><a href="#untitled_example_22">(untitled_example_22)</a></p><p><a href="#untitled_example_23">(untitled_example_23)</a></p><p><a href="#untitled_example_24">(untitled_example_24)</a></p><p><a href="#untitled_example_25">(untitled_example_25)</a></p><p><a href="#untitled_example_26">(untitled_example_26)</a></p><p><a href="#untitled_example_27">(untitled_example_27)</a></p><p><a href="#untitled_example_28">(untitled_example_28)</a></p><p><a href="#untitled_example_29">(untitled_example_29)</a></p><p><a href="#untitled_example_30">(untitled_example_30)</a></p><p><a href="#untitled_example_31">(untitled_example_31)</a></p><p><a href="#untitled_example_32">(untitled_example_32)</a></p><p><a href="#untitled_example_33">(untitled_example_33)</a></p><p><a href="#untitled_example_34">(untitled_example_34)</a></p><p><a href="#untitled_example_35">(untitled_example_35)</a></p><p><a href="#untitled_example_36">(untitled_example_36)</a></p><p><a href="#untitled_example_37">(untitled_example_37)</a></p><p><a href="#untitled_example_38">(untitled_example_38)</a></p><p><a href="#untitled_example_39">(untitled_example_39)</a></p><p><a href="#untitled_example_40">(untitled_example_40)</a></p><p><a href="#untitled_example_41">(untitled_example_41)</a></p><p><a href="#untitled_example_42">(untitled_example_42)</a></p><p><a href="#untitled_example_43">(untitled_example_43)</a></p><p><a href="#untitled_example_44">(untitled_example_44)</a></p><p><a href="#untitled_example_45">(untitled_example_45)</a></p><p><a href="#untitled_example_46">(untitled_example_46)</a></p><p><a href="#untitled_example_47">(untitled_example_47)</a></p><p><a href="#untitled_example_48">(untitled_example_48)</a></p><p><a href="#untitled_example_49">(untitled_example_49)</a></p><p><a href="#untitled_example_50">(untitled_example_50)</a></p><p><a href="#untitled_example_51">(untitled_example_51)</a></p><p><a href="#untitled_example_52">(untitled_example_52)</a></p><p><a href="#untitled_example_53">(untitled_example_53)</a></p><p><a href="#untitled_example_54">(untitled_example_54)</a></p><p><a href="#untitled_example_55">(untitled_example_55)</a></p><p><a href="#untitled_example_56">(untitled_example_56)</a></p><p><a href="#untitled_example_57">(untitled_example_57)</a></p><p><a href="#untitled_example_58">(untitled_example_58)</a></p><p><a href="#untitled_example_59">(untitled_example_59)</a></p><p><a href="#untitled_example_60">(untitled_example_60)</a></p><p><a href="#untitled_example_61">(untitled_example_61)</a></p><p><a href="#untitled_example_62">(untitled_example_62)</a></p><p><a href="#untitled_example_63">(untitled_example_63)</a></p><p><a href="#untitled_example_64">(untitled_example_64)</a></p><p><a href="#untitled_example_65">(untitled_example_65)</a></p><p><a href="#untitled_example_66">(untitled_example_66)</a></p><p><a href="#untitled_example_67">(untitled_example_67)</a></p><p><a href="#untitled_example_68">(untitled_example_68)</a></p><p><a href="#untitled_example_69">(untitled_example_69)</a></p><p><a href="#untitled_example_70">(untitled_example_70)</a></p><p><a href="#untitled_example_71">(untitled_example_71)</a></p><p><a href="#untitled_example_72">(untitled_example_72)</a></p><p><a href="#untitled_example_73">(untitled_example_73)</a></p><p><a href="#untitled_example_74">(untitled_example_74)</a></p><p><a href="#untitled_example_75">(untitled_example_75)</a></p><p><a href="#untitled_example_76">(untitled_example_76)</a></p><p><a href="#untitled_example_77">(untitled_example_77)</a></p><p><a href="#untitled_example_78">(untitled_example_78)</a></p><p><a href="#untitled_example_79">(untitled_example_79)</a></p><p><a href="#untitled_example_80">(untitled_example_80)</a></p><p><a href="#untitled_example_81">(untitled_example_81)</a></p><p><a href="#untitled_example_82">(untitled_example_82)</a></p><p><a href="#untitled_example_83">(untitled_example_83)</a></p><p><a href="#untitled_example_84">(untitled_example_84)</a></p><p><a href="#untitled_example_85">(untitled_example_85)</a></p><p><a href="#untitled_example_86">(untitled_example_86)</a></p><p><a href="#untitled_example_87">(untitled_example_87)</a></p><p><a href="#untitled_example_88">(untitled_example_88)</a></p><p><a href="#untitled_example_89">(untitled_example_89)</a></p><p><a href="#untitled_example_90">(untitled_example_90)</a></p><p><a href="#untitled_example_91">(untitled_example_91)</a></p><p><a href="#untitled_example_92">(untitled_example_92)</a></p><p><a href="#untitled_example_93">(untitled_example_93)</a></p><p><a href="#untitled_example_94">(untitled_example_94)</a></p><p><a href="#untitled_example_95">(untitled_example_95)</a></p><p><a href="#untitled_example_96">(untitled_example_96)</a></p><p><a href="#untitled_example_97">(untitled_example_97)</a></p><p><a href="#untitled_example_98">(untitled_example_98)</a></p><p><a href="#untitled_example_99">(untitled_example_99)</a></p><p><a href="#untitled_example_100">(untitled_example_100)</a></p><p><a href="#untitled_example_101">(untitled_example_101)</a></p><p><a href="#untitled_example_102">(untitled_example_102)</a></p><p><a href="#untitled_example_103">(untitled_example_103)</a></p><p><a href="#untitled_example_104">(untitled_example_104)</a></p><p><a href="#untitled_example_105">(untitled_example_105)</a></p><p><a href="#untitled_example_106">(untitled_example_106)</a></p><p><a href="#untitled_example_107">(untitled_example_107)</a></p><p><a href="#untitled_example_108">(untitled_example_108)</a></p><p><a href="#untitled_example_109">(untitled_example_109)</a></p><p><a href="#untitled_example_110">(untitled_example_110)</a></p><p><a href="#untitled_example_111">(untitled_example_111)</a></p><p><a href="#untitled_example_112">(untitled_example_112)</a></p><p><a href="#untitled_example_113">(untitled_example_113)</a></p><p><a href="#untitled_example_114">(untitled_example_114)</a></p><p><a href="#untitled_example_115">(untitled_example_115)</a></p><p><a href="#untitled_example_116">(untitled_example_116)</a></p><p><a href="#untitled_example_117">(untitled_example_117)</a></p><p><a href="#untitled_example_118">(untitled_example_118)</a></p><p><a href="#untitled_example_119">(untitled_example_119)</a></p><p><a href="#untitled_example_120">(untitled_example_120)</a></p><p><a href="#untitled_example_121">(untitled_example_121)</a></p><p><a href="#untitled_example_122">(untitled_example_122)</a></p><p><a href="#untitled_example_123">(untitled_example_123)</a></p><p><a href="#untitled_example_124">(untitled_example_124)</a></p><p><a href="#untitled_example_125">(untitled_example_125)</a></p><p><a href="#untitled_example_126">(untitled_example_126)</a></p><p><a href="#untitled_example_127">(untitled_example_127)</a></p><p><a href="#untitled_example_128">(untitled_example_128)</a></p><p><a href="#untitled_example_129">(untitled_example_129)</a></p><p><a href="#untitled_example_130">(untitled_example_130)</a></p><p><a href="#untitled_example_131">(untitled_example_131)</a></p><p><a href="#untitled_example_132">(untitled_example_132)</a></p><p><a href="#untitled_example_133">(untitled_example_133)</a></p><p><a href="#untitled_example_134">(untitled_example_134)</a></p><p><a href="#untitled_example_135">(untitled_example_135)</a></p><p><a href="#untitled_example_136">(untitled_example_136)</a></p><p><a href="#untitled_example_137">(untitled_example_137)</a></p><p><a href="#untitled_example_138">(untitled_example_138)</a></p><p><a href="#untitled_example_139">(untitled_example_139)</a></p><p><a href="#untitled_example_140">(untitled_example_140)</a></p><p><a href="#untitled_example_141">(untitled_example_141)</a></p><p><a href="#untitled_example_142">(untitled_example_142)</a></p><p><a href="#untitled_example_143">(untitled_example_143)</a></p><p><a href="#untitled_example_144">(untitled_example_144)</a></p><p><a href="#untitled_example_145">(untitled_example_145)</a></p><p><a href="#untitled_example_146">(untitled_example_146)</a></p><p><a href="#untitled_example_147">(untitled_example_147)</a></p><p><a href="#untitled_example_148">(untitled_example_148)</a></p><p><a href="#untitled_example_149">(untitled_example_149)</a></p><p><a href="#untitled_example_150">(untitled_example_150)</a></p><p><a href="#untitled_example_151">(untitled_example_151)</a></p><p><a href="#untitled_example_152">(untitled_example_152)</a></p><p><a href="#untitled_example_153">(untitled_example_153)</a></p><p><a href="#untitled_example_154">(untitled_example_154)</a></p><p><a href="#untitled_example_155">(untitled_example_155)</a></p><p><a href="#untitled_example_156">(untitled_example_156)</a></p><p><a href="#untitled_example_157">(untitled_example_157)</a></p><p><a href="#untitled_example_158">(untitled_example_158)</a></p><p><a href="#untitled_example_159">(untitled_example_159)</a></p><p><a href="#untitled_example_160">(untitled_example_160)</a></p><p><a href="#untitled_example_161">(untitled_example_161)</a></p><p><a href="#untitled_example_162">(untitled_example_162)</a></p><p><a href="#untitled_example_163">(untitled_example_163)</a></p><p><a href="#untitled_example_164">(untitled_example_164)</a></p><p><a href="#untitled_example_165">(untitled_example_165)</a></p><p><a href="#untitled_example_166">(untitled_example_166)</a></p><p><a href="#untitled_example_167">(untitled_example_167)</a></p><p><a href="#untitled_example_168">(untitled_example_168)</a></p><p><a href="#untitled_example_169">(untitled_example_169)</a></p><p><a href="#untitled_example_170">(untitled_example_170)</a></p><p><a href="#untitled_example_171">(untitled_example_171)</a></p><p><a href="#untitled_example_172">(untitled_example_172)</a></p><p><a href="#untitled_example_173">(untitled_example_173)</a></p><p><a href="#untitled_example_174">(untitled_example_174)</a></p><p><a href="#untitled_example_175">(untitled_example_175)</a></p><p><a href="#untitled_example_176">(untitled_example_176)</a></p><p><a href="#untitled_example_177">(untitled_example_177)</a></p><p><a href="#untitled_example_178">(untitled_example_178)</a></p><p><a href="#untitled_example_179">(untitled_example_179)</a></p><p><a href="#untitled_example_180">(untitled_example_180)</a></p><p><a href="#untitled_example_181">(untitled_example_181)</a></p><p><a href="#untitled_example_182">(untitled_example_182)</a></p><p><a href="#untitled_example_183">(untitled_example_183)</a></p><p><a href="#untitled_example_184">(untitled_example_184)</a></p><p><a href="#untitled_example_185">(untitled_example_185)</a></p><p><a href="#untitled_example_186">(untitled_example_186)</a></p><p><a href="#untitled_example_187">(untitled_example_187)</a></p><p><a href="#untitled_example_188">(untitled_example_188)</a></p><p><a href="#untitled_example_189">(untitled_example_189)</a></p><p><a href="#untitled_example_190">(untitled_example_190)</a></p><p><a href="#untitled_example_191">(untitled_example_191)</a></p><p><a href="#untitled_example_192">(untitled_example_192)</a></p><p><a href="#untitled_example_193">(untitled_example_193)</a></p><p><a href="#untitled_example_194">(untitled_example_194)</a></p><p><a href="#untitled_example_195">(untitled_example_195)</a></p><p><a href="#untitled_example_196">(untitled_example_196)</a></p><p><a href="#untitled_example_197">(untitled_example_197)</a></p><p><a href="#untitled_example_198">(untitled_example_198)</a></p><p><a href="#untitled_example_199">(untitled_example_199)</a></p><p><a href="#untitled_example_200">(untitled_example_200)</a></p><p><a href="#untitled_example_201">(untitled_example_201)</a></p><p><a href="#untitled_example_202">(untitled_example_202)</a></p><p><a href="#untitled_example_203">(untitled_example_203)</a></p><p><a href="#untitled_example_204">(untitled_example_204)</a></p><p><a href="#untitled_example_205">(untitled_example_205)</a></p><p><a href="#untitled_example_206">(untitled_example_206)</a></p><p><a href="#untitled_example_207">(untitled_example_207)</a></p><p><a href="#untitled_example_208">(untitled_example_208)</a></p><p><a href="#untitled_example_209">(untitled_example_209)</a></p><p><a href="#untitled_example_210">(untitled_example_210)</a></p><p><a href="#untitled_example_211">(untitled_example_211)</a></p><p><a href="#untitled_example_212">(untitled_example_212)</a></p><p><a href="#untitled_example_213">(untitled_example_213)</a></p><p><a href="#untitled_example_214">(untitled_example_214)</a></p><p><a href="#untitled_example_215">(untitled_example_215)</a></p><p><a href="#untitled_example_216">(untitled_example_216)</a></p><p><a href="#untitled_example_217">(untitled_example_217)</a></p><p><a href="#untitled_example_218">(untitled_example_218)</a></p><p><a href="#untitled_example_219">(untitled_example_219)</a></p><p><a href="#untitled_example_220">(untitled_example_220)</a></p><p><a href="#untitled_example_221">(untitled_example_221)</a></p><p><a href="#untitled_example_222">(untitled_example_222)</a></p><p><a href="#untitled_example_223">(untitled_example_223)</a></p><p><a href="#untitled_example_224">(untitled_example_224)</a></p><p><a href="#untitled_example_225">(untitled_example_225)</a></p><p><a href="#untitled_example_226">(untitled_example_226)</a></p><p><a href="#untitled_example_227">(untitled_example_227)</a></p><p><a href="#untitled_example_228">(untitled_example_228)</a></p><p><a href="#untitled_example_229">(untitled_example_229)</a></p><p><a href="#untitled_example_230">(untitled_example_230)</a></p><p><a href="#untitled_example_231">(untitled_example_231)</a></p><p><a href="#untitled_example_232">(untitled_example_232)</a></p><p><a href="#untitled_example_233">(untitled_example_233)</a></p><p><a href="#untitled_example_234">(untitled_example_234)</a></p><p><a href="#untitled_example_235">(untitled_example_235)</a></p><p><a href="#untitled_example_236">(untitled_example_236)</a></p><p><a href="#untitled_example_237">(untitled_example_237)</a></p><p><a href="#untitled_example_238">(untitled_example_238)</a></p><p><a href="#untitled_example_239">(untitled_example_239)</a></p><p><a href="#untitled_example_240">(untitled_example_240)</a></p><p><a href="#untitled_example_241">(untitled_example_241)</a></p><p><a href="#untitled_example_242">(untitled_example_242)</a></p><p><a href="#untitled_example_243">(untitled_example_243)</a></p><p><a href="#untitled_example_244">(untitled_example_244)</a></p><p><a href="#untitled_example_245">(untitled_example_245)</a></p><p><a href="#untitled_example_246">(untitled_example_246)</a></p><p><a href="#untitled_example_247">(untitled_example_247)</a></p><p><a href="#untitled_example_248">(untitled_example_248)</a></p><p><a href="#untitled_example_249">(untitled_example_249)</a></p><p><a href="#untitled_example_250">(untitled_example_250)</a></p><p><a href="#untitled_example_251">(untitled_example_251)</a></p><p><a href="#untitled_example_252">(untitled_example_252)</a></p><p><a href="#untitled_example_253">(untitled_example_253)</a></p><p><a href="#untitled_example_254">(untitled_example_254)</a></p><p><a href="#untitled_example_255">(untitled_example_255)</a></p><p><a href="#untitled_example_256">(untitled_example_256)</a></p><p><a href="#untitled_example_257">(untitled_example_257)</a></p><p><a href="#untitled_example_258">(untitled_example_258)</a></p><p><a href="#untitled_example_259">(untitled_example_259)</a></p><p><a href="#untitled_example_260">(untitled_example_260)</a></p><p><a href="#untitled_example_261">(untitled_example_261)</a></p><p><a href="#untitled_example_262">(untitled_example_262)</a></p><p><a href="#untitled_example_263">(untitled_example_263)</a></p><p><a href="#untitled_example_264">(untitled_example_264)</a></p><p><a href="#untitled_example_265">(untitled_example_265)</a></p><p><a href="#untitled_example_266">(untitled_example_266)</a></p><p><a href="#untitled_example_267">(untitled_example_267)</a></p><p><a href="#untitled_example_268">(untitled_example_268)</a></p><p><a href="#untitled_example_269">(untitled_example_269)</a></p><p><a href="#untitled_example_270">(untitled_example_270)</a></p><p><a href="#untitled_terminal_8">(untitled_terminal_8)</a></p><p><a href="#untitled_terminal_9">(untitled_terminal_9)</a></p><p><a href="#untitled_example_271">(untitled_example_271)</a></p><p><a href="#untitled_example_272">(untitled_example_272)</a></p><p><a href="#untitled_example_273">(untitled_example_273)</a></p><p><a href="#untitled_terminal_10">(untitled_terminal_10)</a></p><p><a href="#untitled_terminal_11">(untitled_terminal_11)</a></p><p><a href="#untitled_terminal_12">(untitled_terminal_12)</a></p><p><a href="#untitled_example_274">(untitled_example_274)</a></p><p><a href="#untitled_example_275">(untitled_example_275)</a></p><p><a href="#untitled_example_276">(untitled_example_276)</a></p><p><a href="#untitled_example_277">(untitled_example_277)</a></p><p><a href="#untitled_example_278">(untitled_example_278)</a></p><p><a href="#untitled_example_279">(untitled_example_279)</a></p><p><a href="#untitled_example_280">(untitled_example_280)</a></p><p><a href="#untitled_example_281">(untitled_example_281)</a></p><p><a href="#untitled_example_282">(untitled_example_282)</a></p><p><a href="#untitled_example_283">(untitled_example_283)</a></p><p><a href="#untitled_example_284">(untitled_example_284)</a></p><p><a href="#untitled_example_285">(untitled_example_285)</a></p><p><a href="#untitled_example_286">(untitled_example_286)</a></p><p><a href="#untitled_example_287">(untitled_example_287)</a></p><p><a href="#untitled_example_288">(untitled_example_288)</a></p><p><a href="#untitled_example_289">(untitled_example_289)</a></p><p><a href="#untitled_example_290">(untitled_example_290)</a></p><p><a href="#untitled_example_291">(untitled_example_291)</a></p><p><a href="#untitled_terminal_13">(untitled_terminal_13)</a></p><p><a href="#untitled_terminal_14">(untitled_terminal_14)</a></p><p><a href="#untitled_example_292">(untitled_example_292)</a></p><p><a href="#untitled_terminal_15">(untitled_terminal_15)</a></p><p><a href="#untitled_example_293">(untitled_example_293)</a></p><p><a href="#untitled_example_294">(untitled_example_294)</a></p><p><a href="#untitled_example_295">(untitled_example_295)</a></p><p><a href="#untitled_example_296">(untitled_example_296)</a></p><p><a href="#untitled_example_297">(untitled_example_297)</a></p><p><a href="#untitled_example_298">(untitled_example_298)</a></p><p><a href="#untitled_example_299">(untitled_example_299)</a></p><p><a href="#untitled_example_300">(untitled_example_300)</a></p><p><a href="#untitled_example_301">(untitled_example_301)</a></p><p><a href="#untitled_example_302">(untitled_example_302)</a></p><p><a href="#untitled_example_303">(untitled_example_303)</a></p><p><a href="#untitled_example_304">(untitled_example_304)</a></p><p><a href="#untitled_example_305">(untitled_example_305)</a></p><p><a href="#untitled_example_306">(untitled_example_306)</a></p><p><a href="#untitled_example_307">(untitled_example_307)</a></p><p><a href="#untitled_example_308">(untitled_example_308)</a></p><p><a href="#untitled_example_309">(untitled_example_309)</a></p><p><a href="#untitled_example_310">(untitled_example_310)</a></p><p><a href="#untitled_example_311">(untitled_example_311)</a></p></nav><nav id="menuText_FIGURE" style="display: none;"><p><a href="#untitled_figure_1">https://packt.link/free-ebook/9781835089507</a></p><p><a href="#untitled_figure_2">Figure 1.1. – LLMs are a sub-branch of GenAI</a></p><p><a href="#untitled_figure_3">Figure 1.2. – The Gartner Hype Cycle</a></p><p><a href="#untitled_figure_4">Figure 1.3. – Screenshot from a GPT 3.5-turbo-instruct playground</a></p><p><a href="#untitled_figure_5">Figure 1.4. – Screenshot from a GPT-4 playground</a></p><p><a href="#untitled_figure_6">Figure 1.5. – A RAG model</a></p><p><a href="#untitled_figure_7">Figure 2.1. – An illustration of the LLM fine-tuning process</a></p><p><a href="#untitled_figure_8">Figure 2.2. – The relative costs of updating data in a pre-trained LLM</a></p><p><a href="#untitled_figure_9">Figure 2.3. – An overview of the PITS workflow</a></p><p><a href="#untitled_figure_10">Figure 2.4. – Editing Windows environment variables</a></p><p><a href="#untitled_figure_11">Figure 2.5. – Creating the OPENAI_API_KEY environment variable</a></p><p><a href="#untitled_figure_12">Figure 2.6. – The LlamaIndex GitHub repository code structure</a></p><p><a href="#untitled_figure_13">Figure 3.1. – Documents can come from multiple sources</a></p><p><a href="#untitled_figure_14">Figure 3.2. – The basic structure of a document</a></p><p><a href="#untitled_figure_15">Figure 3.3. – Relationships between Nodes extracted from a Document</a></p><p><a href="#untitled_figure_16">Figure 3.4. – Previous or next relationship between two Nodes</a></p><p><a href="#untitled_figure_17">Figure 3.5. – The complete RAG workflow with LlamaIndex</a></p><p><a href="#untitled_figure_18">Figure 3.6. – Effect of temperature on output variability</a></p><p><a href="#untitled_figure_19">Figure 4.1. – A sample PDF containing multiple articles, images, and tables</a></p><p><a href="#untitled_figure_20">Figure 4.2. – Hierarchical nodes of 2,048, 512, and 128 chunk sizes</a></p><p><a href="#untitled_figure_21">Figure 4.3. – chunk_size and chunk_overlap explained</a></p><p><a href="#untitled_figure_22">Figure 4.4. – An ingestion pipeline at work</a></p><p><a href="#untitled_figure_23">Figure 5.1. – The structure of a VectorStoreIndex</a></p><p><a href="#untitled_figure_24">Figure 5.2. – How an embedding model converts data into numerical representations</a></p><p><a href="#untitled_figure_25">Figure 5.3. – A comparison of the three embedded sentences in a 3D space</a></p><p><a href="#untitled_figure_26">Figure 5.4. – How a cosine similarity comparison would look</a></p><p><a href="#untitled_figure_27">Figure 5.5. – Calculating similarity using the dot product method</a></p><p><a href="#untitled_figure_28">Figure 5.6. – The Euclidean distance between two vectors</a></p><p><a href="#untitled_figure_29">Figure 5.7. – The structure of a SummaryIndex</a></p><p><a href="#untitled_figure_30">Figure 5.8. – The DocumentSummaryIndex</a></p><p><a href="#untitled_figure_31">Figure 5.9. – The structure of a KeywordTableIndex</a></p><p><a href="#untitled_figure_32">Figure 5.10. – The structure of a TreeIndex</a></p><p><a href="#untitled_figure_33">Figure 5.11. – The structure of a KnowledgeGraphIndex</a></p><p><a href="#untitled_figure_34">Figure 5.12. – The structure of a ComposableGraph</a></p><p><a href="#untitled_figure_35">Figure 6.1. – Node retrieval using VectorIndexRetriever</a></p><p><a href="#untitled_figure_36">Figure 6.2. – Retrieving nodes using SummaryIndexRetriever</a></p><p><a href="#untitled_figure_37">Figure 6.3. – Inner workings of SummaryIndexEmbeddingRetriever</a></p><p><a href="#untitled_figure_38">Figure 6.4. – SummaryIndexLLMRetriever in action</a></p><p><a href="#untitled_figure_39">Figure 6.5. – How DocumentSummaryIndexLLMRetriever works</a></p><p><a href="#untitled_figure_40">Figure 6.6. – DocumentSummaryIndexEmbeddingRetriever</a></p><p><a href="#untitled_figure_41">Figure 6.7. – TreeSelectLeafRetriever configured with a child_branch_factor argument value of 1</a></p><p><a href="#untitled_figure_42">Figure 6.8. – Retrieving all nodes by using TreeAllLeafRetriever</a></p><p><a href="#untitled_figure_43">Figure 6.9. – Retrieving from the root of the tree</a></p><p><a href="#untitled_figure_44">Figure 6.10. – KeywordTableIndex</a></p><p><a href="#untitled_figure_45">Figure 6.11. – The inner workings of KGTableRetriever</a></p><p><a href="#untitled_figure_46">Figure 6.12. – Visualizing LLMSingleSelector</a></p><p><a href="#untitled_figure_47">Figure 6.13. – QueryTransform improving the retrieval process</a></p><p><a href="#untitled_figure_48">Figure 7.1. – The role of node postprocessors in RAG</a></p><p><a href="#untitled_figure_49">Figure 7.2. – The refine response synthesizer</a></p><p><a href="#untitled_figure_50">Figure 7.3. – LLMs may produce unpredictable outputs</a></p><p><a href="#untitled_figure_51">Figure 7.4. – How RouterQueryEngine works</a></p><p><a href="#untitled_figure_52">Figure 7.5. – How SubQuestionQueryEngine works</a></p><p><a href="#untitled_figure_53">Figure 8.1. – The ELIZA chatbot interface</a></p><p><a href="#untitled_figure_54">Figure 8.2. – The ChatOps paradigm</a></p><p><a href="#untitled_figure_55">Figure 8.3. – SimpleChatEngine</a></p><p><a href="#untitled_figure_56">Figure 8.4. – ContextChatEngine</a></p><p><a href="#untitled_figure_57">Figure 8.5. – CondenseQuestionChatEngine</a></p><p><a href="#untitled_figure_58">Figure 8.6. – CondensePlusContextChatEngine</a></p><p><a href="#untitled_figure_59">Figure 8.7. – DatabaseToolSpec</a></p><p><a href="#untitled_figure_60">Figure 8.8. – The reasoning loop in an agent</a></p><p><a href="#untitled_figure_61">Figure 8.9. – The simplified workflow of OpenAIAgent</a></p><p><a href="#untitled_figure_62">Figure 8.10. – Sample output for the OpenAIAgent code example</a></p><p><a href="#untitled_figure_63">Figure 8.11. – Visualization of a direct API call versus interaction via LoadAndSearchToolSpec</a></p><p><a href="#untitled_figure_64">Figure 8.12. – Sample agent output when LoadAndSearchToolSpec is used</a></p><p><a href="#untitled_figure_65">Figure 8.13. – An overview of the LLMCompiler agent’s architecture</a></p><p><a href="#untitled_figure_66">Figure 8.14. – Sample output of the LLMCompiler agent</a></p><p><a href="#untitled_figure_67">Figure 8.15. – The AgentRunner and AgentWorker orchestration model</a></p><p><a href="#untitled_figure_68">Figure 8.16. – Screenshot from the PITS training UI</a></p><p><a href="#untitled_figure_69">Figure 9.1. – LM Studio screenshot displaying search results</a></p><p><a href="#untitled_figure_70">Figure 9.2. – LM Studio’s chat UI</a></p><p><a href="#untitled_figure_71">Figure 9.3. – The local Inference Server interface in LM Studio</a></p><p><a href="#untitled_figure_72">Figure 9.4. – A diagram of the Neutrino smart routing feature</a></p><p><a href="#untitled_figure_73">Figure 9.5. – A screenshot from the Phoenix server UI depicting our tracing output</a></p><p><a href="#untitled_figure_74">Figure 9.6. – Trace details visualized on the Phoenix server UI</a></p><p><a href="#untitled_figure_75">Figure 9.7. – Visualizing evaluation results in the Phoenix server UI</a></p><p><a href="#untitled_figure_76">Figure 9.8. – The contents of the C:\PITS_APP folder</a></p><p><a href="#untitled_figure_77">Figure 9.9. – Creating a new GitHub repository named PITS_ONLINE</a></p><p><a href="#untitled_figure_78">Figure 9.10. – Deploying an application into Streamlit Community Cloud</a></p><p><a href="#untitled_figure_79">Figure 10.1. – According to Moore’s law, the number of transistors roughly doubles every 2 years</a></p><p><a href="#untitled_figure_80">Figure 10.2. – How prompts are built by injecting variables into a prompt template</a></p><p><a href="#untitled_figure_81">Figure 10.3. – The two prompt templates used by the SummaryIndex query engine</a></p><p><a href="#untitled_figure_82">Figure 10.4. – The query output before and after updating the prompt templates</a></p><p><a href="#untitled_figure_83">https://packt.link/free-ebook/9781835089507</a></p></nav><nav id="menuText_TABLE" style="display: none;"><p><a href="#table001">(table001)</a></p><p><a href="#table001-1">Table 6.1. – A complete list of operators available for FilterOperator</a></p><p><a href="#table001-2">Table 7.1. – Different query engine modules available in LlamaIndex</a></p><p><a href="#table001-3">Table 8.1. – The sample Employees table from the Employees.db file</a></p><p><a href="#table001-4">Table 10.1. – An overview of the more advanced prompting techniques provided by LlamaIndex</a></p></nav><select id="selectList" onchange="updateMenuDiv()"><option value="hide">hide</option><option value="TOC">TOC</option><option value="EQUATION">EQUATION</option><option value="inline_EQ">inline_EQ</option><option value="EXAMPLE">EXAMPLE (326)</option><option value="FIGURE">FIGURE (83)</option><option value="TABLE">TABLE (5)</option></select><script>
        function updateMenuDiv() {
          let menuDiv = document.getElementById('menuDiv');
          let selectList = document.getElementById('selectList');
          switch(selectList.value) {
            case 'hide':
              menuDiv.style.display = 'none';
              break;
            default:
              menuDiv.innerHTML = document.getElementById('menuText_' + selectList.value).innerHTML;
              menuDiv.style.display = 'block';
          }
        }
        </script></body></html>